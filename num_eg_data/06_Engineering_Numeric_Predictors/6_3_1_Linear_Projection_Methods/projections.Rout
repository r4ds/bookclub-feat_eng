
R version 3.5.2 (2018-12-20) -- "Eggshell Igloo"
Copyright (C) 2018 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # ------------------------------------------------------------------------------
> # Feature Engineering and Selection: A Practical Approach for Predictive Models
> # by Max Kuhn and Kjell Johnson
> #
> # ------------------------------------------------------------------------------
> # 
> # Code for Section 6.3.1 at
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#linear-projection-methods
> #
> # ------------------------------------------------------------------------------
> # 
> # Code requires these packages: 
> 
> library(caret)
Loading required package: lattice
Loading required package: ggplot2
> library(NMF)
Loading required package: pkgmaker
Loading required package: registry

Attaching package: ‘pkgmaker’

The following object is masked from ‘package:base’:

    isFALSE

Loading required package: rngtools
Loading required package: cluster
NMF - BioConductor layer [OK] | Shared memory capabilities [NO: synchronicity] | Cores 19/20
  To enable shared memory capabilities, try: install.extras('
NMF
')
> library(fastICA)
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 0.0.2 ──
✔ broom     0.5.1     ✔ purrr     0.3.0
✔ dials     0.0.2     ✔ recipes   0.1.4
✔ dplyr     0.7.8     ✔ rsample   0.0.4
✔ infer     0.4.0     ✔ tibble    2.0.1
✔ parsnip   0.0.1     ✔ yardstick 0.0.2
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ dplyr::combine()         masks Biobase::combine(), BiocGenerics::combine()
✖ purrr::discard()         masks scales::discard()
✖ dplyr::filter()          masks stats::filter()
✖ parsnip::fit()           masks NMF::fit()
✖ dplyr::lag()             masks stats::lag()
✖ purrr::lift()            masks caret::lift()
✖ BiocGenerics::Position() masks ggplot2::Position(), base::Position()
✖ yardstick::precision()   masks caret::precision()
✖ yardstick::recall()      masks caret::recall()
✖ recipes::step()          masks stats::step()
> library(kernlab)

Attaching package: ‘kernlab’

The following object is masked from ‘package:purrr’:

    cross

The following object is masked from ‘package:scales’:

    alpha

The following object is masked from ‘package:ggplot2’:

    alpha

> library(pls)

Attaching package: ‘pls’

The following object is masked from ‘package:NMF’:

    loadings

The following object is masked from ‘package:caret’:

    R2

The following object is masked from ‘package:stats’:

    loadings

> library(RColorBrewer)
> library(leaflet)
> library(htmltools)
> library(dimRed)
Loading required package: DRR
Loading required package: CVST
Loading required package: Matrix

Attaching package: ‘Matrix’

The following object is masked from ‘package:tidyr’:

    expand


Attaching package: ‘dimRed’

The following object is masked from ‘package:BiocGenerics’:

    as.data.frame

The following object is masked from ‘package:stats’:

    embed

The following object is masked from ‘package:base’:

    as.data.frame

> library(heatmaply)
Loading required package: plotly

Attaching package: ‘plotly’

The following object is masked from ‘package:ggplot2’:

    last_plot

The following object is masked from ‘package:stats’:

    filter

The following object is masked from ‘package:graphics’:

    layout

Loading required package: viridis
Loading required package: viridisLite

Attaching package: ‘viridis’

The following object is masked from ‘package:scales’:

    viridis_pal


======================
Welcome to heatmaply version 0.15.2

Type citation('heatmaply') for how to cite the package.
Type ?heatmaply for the main documentation.

The github page is: https://github.com/talgalili/heatmaply/
Please submit your suggestions and bug-reports at: https://github.com/talgalili/heatmaply/issues
Or contact: <tal.galili@gmail.com>
======================


Attaching package: ‘heatmaply’

The following object is masked from ‘package:BiocGenerics’:

    normalize

> library(lobstr)
> 
> # The memory requires for this script are about 8GB although see the note below
> # regarding NNMF (which can be much higher)
> 
> mem_in_gb <- function() {
+   res <- as.numeric(mem_used()) * 1e-9
+   cat(round(res, 1), "GB\n")
+   invisible(NULL)
+ }
> 
> # ------------------------------------------------------------------------------
> 
> # For plots
> source("chicago_maps.R")

Attaching package: ‘rlang’

The following objects are masked from ‘package:purrr’:

    %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int,
    flatten_lgl, flatten_raw, invoke, list_along, modify, prepend,
    splice

The following object is masked from ‘package:Biobase’:

    exprs

> 
> # ------------------------------------------------------------------------------
> 
> load("../../Data_Sets/Chicago_trains/chicago.RData")
> 
> # ------------------------------------------------------------------------------
> 
> weekends <- 
+   training %>% 
+   dplyr::filter(dow %in% c("Sat", "Sun")) %>%
+   dplyr::select(matches("l14_[0-9]"), s_40380)
> 
> stations <- 
+   stations %>% 
+   mutate(terms = gsub("s_", "l14_", station_id))
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> # Emulate the rolling origin forecast resampling but for weekends
> 
> weekend_days <- train_days[training$dow %in% c("Sat", "Sun")]
> 
> wend_slices <- createTimeSlices(weekend_days, initialWindow = 1600, horizon = 4, fixedWindow = FALSE)
> wend_ctrl <- ctrl
> wend_ctrl$index <- wend_slices$train
> wend_ctrl$indexOut <- wend_slices$test
> wend_ctrl$verboseIter <- FALSE
> wend_ctrl$allowParallel <- FALSE
> 
> # ------------------------------------------------------------------------------
> 
> simple_rec <- recipe(s_40380 ~ ., data = weekends)
> 
> set.seed(7599)
> simple_mod <- 
+   train(simple_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "Original Predictors")
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> pca_rec <- 
+   simple_rec %>%
+   step_center(matches("l14_[0-9]")) %>%
+   step_scale(matches("l14_[0-9]")) %>%
+   step_pca(matches("l14_[0-9]"), num_comp = 20)
> 
> pca_rec_tr <- 
+   pca_rec %>%
+   prep(training = weekends, verbose = TRUE)
oper 1 step center [training] 
oper 2 step scale [training] 
oper 3 step pca [training] 
The retained training set is ~ 0.26 Mb  in memory.

> 
> pca_features <- juice(pca_rec_tr, matches("^PC"))
> 
> pca_cor <- apply(pca_features, 2, cor, y = weekends$s_40380, method = "spearman")
> pca_cor <- pca_cor[order(-abs(pca_cor))]
> pca_cor
        PC01         PC02         PC06         PC07         PC20         PC11 
 0.788363004 -0.340091805  0.160229143  0.140769244  0.128354958  0.107559442 
        PC19         PC04         PC16         PC03         PC18         PC10 
-0.099391044  0.093115558 -0.087066549  0.053463985  0.050741445 -0.049221526 
        PC09         PC14         PC15         PC12         PC08         PC17 
 0.037859109  0.034199452 -0.024362732  0.018376755  0.016049202  0.014475841 
        PC13         PC05 
 0.007675887  0.001579482 
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> # Compute the % variation for each component
> sds <- pca_rec_tr$steps[[3]]$res$sdev
> pct_var <- (sds^2)/sum(sds^2)*100
> cum_var <- cumsum(pct_var)
> 
> # Get component values
> pca_coefs <- 
+   tidy(pca_rec_tr, number = 3) %>%
+   dplyr::select(-id) %>% 
+   spread(component, value) %>% 
+   dplyr::select(terms, PC1, PC2, PC3, PC4, PC5) %>%  
+   full_join(stations, by = "terms")
> 
> # Get a different version used for the heatmap
> five_pca_coefs <- 
+   pca_coefs %>% 
+   as.data.frame() %>% 
+   na.omit()
> 
> rownames(five_pca_coefs) <- five_pca_coefs$description
> 
> five_pca_coefs <- 
+   five_pca_coefs %>%  
+   dplyr::select(starts_with("PC"))
> 
> # ------------------------------------------------------------------------------
> 
> five_comps_range <-
+   tidy(pca_rec, number = 3) %>% 
+   mutate(value = abs(value)) %>% 
+   pull(value) %>% 
+   max(na.rm = TRUE)
Warning message:
In max(., na.rm = TRUE) : no non-missing arguments to max; returning -Inf
> 
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#fig:numeric-heatmap-html
> # heatmaply(
> #   five_pca_coefs,
> #   cexRow = .5,
> #   Colv = FALSE,
> #   branches_lwd = .5,
> #   colors = RdBu(50),
> #   label_format_fun = function(...) round(..., digits = 3),
> #   limits = c(-five_comps_range, five_comps_range),
> #   margins = c(50,215,10,150)
> # )
> 
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#fig:numeric-pc2-map-html
> # chicago_map(pca_coefs, plot_var = "PC2")
> 
> # ------------------------------------------------------------------------------
> 
> set.seed(7599)
> pca_mod <- 
+   train(pca_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "PCA")
> 
> rm(pca_rec, pca_rec_tr)
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> # Determine a reasonable value for the radial basis function parameter sigma
> sig_range <- 
+   simple_rec %>%
+   step_center(matches("l14_[0-9]")) %>%
+   step_scale(matches("l14_[0-9]")) %>%
+   prep(training = weekends, verbose = TRUE) %>%
+   juice(matches("l14_[0-9]")) %>%
+   as.matrix() %>%
+   sigest(frac = 1) 
oper 1 step center [training] 
oper 2 step scale [training] 
The retained training set is ~ 1.58 Mb  in memory.

> 
> kpca_rec <- 
+   simple_rec %>%
+   step_center(matches("l14_[0-9]")) %>%
+   step_scale(matches("l14_[0-9]")) %>%
+   step_kpca(
+     matches("l14_[0-9]"), 
+     num_comp = 20, 
+     options = list(kernel = "rbfdot", kpar = list(sigma = sig_range[2]))
+   ) 
> 
> kpca_rec_tr <- 
+   kpca_rec %>%
+   prep(training = weekends, retain = TRUE, verbose = TRUE)
oper 1 step center [training] 
oper 2 step scale [training] 
oper 3 step kpca [training] 
2019-02-19 15:52:42: Calculating kernel PCA
2019-02-19 15:52:49: Trying to calculate reverse
2019-02-19 15:52:50: DONE
The retained training set is ~ 0.26 Mb  in memory.

> 
> kpca_features <- juice(kpca_rec_tr, matches("PC"))
> 
> kpca_cor <- apply(kpca_features, 2, cor, y = weekends$s_40380, method = "spearman")
> kpca_cor <- kpca_cor[order(-abs(kpca_cor))]
> kpca_cor
       kPC01        kPC03        kPC04        kPC05        kPC02        kPC17 
 0.767281690 -0.319556278  0.225248705  0.125555055 -0.094370801 -0.075681929 
       kPC13        kPC15        kPC11        kPC07        kPC09        kPC08 
-0.073586814  0.058904663  0.058716708  0.049590286  0.043634165  0.034709347 
       kPC12        kPC06        kPC19        kPC14        kPC10        kPC16 
 0.031428436 -0.031097233 -0.026206895  0.026178743  0.023340771 -0.007396164 
       kPC18        kPC20 
-0.005702554 -0.002925519 
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> set.seed(7599)
> kpca_mod <- 
+   train(kpca_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "kPCA")
2019-02-19 15:52:53: Calculating kernel PCA
2019-02-19 15:52:59: Trying to calculate reverse
2019-02-19 15:53:01: DONE
2019-02-19 15:53:03: Calculating kernel PCA
2019-02-19 15:53:09: Trying to calculate reverse
2019-02-19 15:53:10: DONE
2019-02-19 15:53:13: Calculating kernel PCA
2019-02-19 15:53:19: Trying to calculate reverse
2019-02-19 15:53:21: DONE
2019-02-19 15:53:23: Calculating kernel PCA
2019-02-19 15:53:29: Trying to calculate reverse
2019-02-19 15:53:31: DONE
2019-02-19 15:53:33: Calculating kernel PCA
2019-02-19 15:53:39: Trying to calculate reverse
2019-02-19 15:53:41: DONE
2019-02-19 15:53:43: Calculating kernel PCA
2019-02-19 15:53:49: Trying to calculate reverse
2019-02-19 15:53:51: DONE
2019-02-19 15:53:53: Calculating kernel PCA
2019-02-19 15:54:00: Trying to calculate reverse
2019-02-19 15:54:02: DONE
2019-02-19 15:54:04: Calculating kernel PCA
2019-02-19 15:54:10: Trying to calculate reverse
2019-02-19 15:54:12: DONE
2019-02-19 15:54:14: Calculating kernel PCA
2019-02-19 15:54:20: Trying to calculate reverse
2019-02-19 15:54:22: DONE
2019-02-19 15:54:24: Calculating kernel PCA
2019-02-19 15:54:30: Trying to calculate reverse
2019-02-19 15:54:33: DONE
2019-02-19 15:54:37: Calculating kernel PCA
2019-02-19 15:54:48: Trying to calculate reverse
2019-02-19 15:54:51: DONE
2019-02-19 15:54:55: Calculating kernel PCA
2019-02-19 15:55:03: Trying to calculate reverse
2019-02-19 15:55:05: DONE
2019-02-19 15:55:08: Calculating kernel PCA
2019-02-19 15:55:18: Trying to calculate reverse
2019-02-19 15:55:21: DONE
2019-02-19 15:55:24: Calculating kernel PCA
2019-02-19 15:55:34: Trying to calculate reverse
2019-02-19 15:55:37: DONE
2019-02-19 15:55:41: Calculating kernel PCA
2019-02-19 15:55:50: Trying to calculate reverse
2019-02-19 15:55:53: DONE
2019-02-19 15:55:55: Calculating kernel PCA
2019-02-19 15:56:05: Trying to calculate reverse
2019-02-19 15:56:08: DONE
2019-02-19 15:56:12: Calculating kernel PCA
2019-02-19 15:56:20: Trying to calculate reverse
2019-02-19 15:56:22: DONE
2019-02-19 15:56:26: Calculating kernel PCA
2019-02-19 15:56:36: Trying to calculate reverse
2019-02-19 15:56:40: DONE
2019-02-19 15:56:43: Calculating kernel PCA
2019-02-19 15:56:54: Trying to calculate reverse
2019-02-19 15:56:57: DONE
2019-02-19 15:57:00: Calculating kernel PCA
2019-02-19 15:57:07: Trying to calculate reverse
2019-02-19 15:57:09: DONE
2019-02-19 15:57:12: Calculating kernel PCA
2019-02-19 15:57:23: Trying to calculate reverse
2019-02-19 15:57:26: DONE
2019-02-19 15:57:30: Calculating kernel PCA
2019-02-19 15:57:40: Trying to calculate reverse
2019-02-19 15:57:43: DONE
2019-02-19 15:57:46: Calculating kernel PCA
2019-02-19 15:57:54: Trying to calculate reverse
2019-02-19 15:57:57: DONE
2019-02-19 15:58:01: Calculating kernel PCA
2019-02-19 15:58:11: Trying to calculate reverse
2019-02-19 15:58:14: DONE
2019-02-19 15:58:18: Calculating kernel PCA
2019-02-19 15:58:29: Trying to calculate reverse
2019-02-19 15:58:32: DONE
2019-02-19 15:58:36: Calculating kernel PCA
2019-02-19 15:58:46: Trying to calculate reverse
2019-02-19 15:58:49: DONE
2019-02-19 15:58:52: Calculating kernel PCA
2019-02-19 15:59:00: Trying to calculate reverse
2019-02-19 15:59:03: DONE
> 
> rm(kpca_rec, kpca_rec_tr)
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> # Create some reproducible random numbers for starting values
> set.seed(1257)
> ica_start <- matrix(rnorm(20^2), nrow = 20)
> 
> ica_rec <- 
+   simple_rec %>%
+   step_ica(
+     matches("l14_[0-9]"),
+     num_comp = 20,
+     options = list(
+       maxit = 1000,
+       tol = 1e-10,
+       alg.type = "deflation",
+       w.init = ica_start
+     )
+   ) 
> 
> ica_rec_tr <- 
+   ica_rec %>%
+   prep(training = weekends, verbose = TRUE)
oper 1 step ica [training] 
The retained training set is ~ 0.26 Mb  in memory.

> 
> ica_features <- juice(ica_rec_tr, matches("IC"))
> 
> ica_cor <- apply(ica_features, 2, cor, y = weekends$s_40380, method = "spearman")
> ica_cor <- ica_cor[order(-abs(ica_cor))]
> ica_cor
        IC07         IC12         IC16         IC11         IC05         IC09 
 0.413728801 -0.400549046 -0.385795161 -0.209446873 -0.197866781 -0.195436330 
        IC13         IC10         IC01         IC02         IC14         IC19 
-0.186660410  0.149341120  0.148894354  0.141208087  0.129444557 -0.119243519 
        IC20         IC15         IC04         IC18         IC06         IC17 
 0.077188134 -0.064558249  0.059083186  0.042021285 -0.039488352 -0.027166845 
        IC03         IC08 
-0.014412620  0.002797806 
> 
> ica_coefs <- 
+   tidy(ica_rec_tr, number = 1) %>%
+   dplyr::select(-id) %>% 
+   spread(component, value) %>% 
+   dplyr::select(terms, IC01, IC02, IC03) %>%  
+   full_join(stations, by = "terms")
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> set.seed(7599)
> ica_mod <- 
+   train(ica_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "ICA")
> 
> rm(ica_rec, ica_rec_tr)
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> # By default, the next section computes the NNMF results in parallel and *each 
> # worker* uses about 1GB of memory (it will automatically try to use as many
> # cores as `num_run`). See the details in ?NMF::nmf. This can be turned off by
> # the `options` argument to `step_nnmf()`. 
> 
> nnmf_rec <- 
+   simple_rec %>%
+   step_scale(matches("l14_[0-9]")) %>%
+   step_nnmf(matches("l14_[0-9]"), num_comp = 20, seed = 17202, num_run = 20) 
> 
> nnmf_rec_tr <- 
+   nnmf_rec %>%
+   prep(training = weekends, verbose = TRUE)
oper 1 step scale [training] 
oper 2 step nnmf [training] 
The retained training set is ~ 0.26 Mb  in memory.

> 
> nnmf_features <- juice(nnmf_rec_tr)
> 
> nnmf_cor <- apply(nnmf_features[, -1], 2, cor, y = nnmf_features$s_40380, method = "spearman")
> nnmf_cor <- nnmf_cor[order(-abs(nnmf_cor))]
> nnmf_cor
     NNMF11      NNMF02      NNMF20      NNMF17      NNMF15      NNMF19 
 0.79106416  0.58042020  0.57439578  0.56444937  0.48349924  0.39180518 
     NNMF01      NNMF12      NNMF13      NNMF10      NNMF04      NNMF09 
 0.36559112 -0.36190701 -0.34914770  0.33855854  0.33370640  0.33290962 
     NNMF14      NNMF03      NNMF18      NNMF07      NNMF05      NNMF08 
-0.32077886 -0.25222052  0.20714398  0.12307111  0.09663890  0.08029172 
     NNMF16      NNMF06 
 0.05152268  0.01836211 
> 
> # As of this writing, the `tidy` method isn't working for `step_nnmf()`
> nnmf_coefs <- 
+   nnmf_rec_tr$steps[[2]]$res@other.data$w %>%
+   as.data.frame() %>%
+   rownames_to_column(var = "terms") %>%  
+   full_join(stations, by = "terms")
> 
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#fig:numeric-nnmf3-map-html
> # chicago_map(nnmf_coefs, plot_var = names(nnmf_cor)[1])
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> set.seed(7599)
> nnmf_mod <- 
+   train(nnmf_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "NNMF")
> 
> rm(nnmf_rec, nnmf_rec_tr)
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> pls_rec <- 
+   simple_rec %>%
+   step_center(matches("l14_[0-9]")) %>%
+   step_scale(matches("l14_[0-9]")) %>%
+   step_pls(matches("l14_[0-9]"), outcome = "s_40380", num_comp = 20)
> 
> pls_rec_tr <- 
+   pls_rec %>%
+   prep(training = weekends, verbose = TRUE)
oper 1 step center [training] 
oper 2 step scale [training] 
oper 3 step pls [training] 
The retained training set is ~ 0.26 Mb  in memory.

> 
> pls_features <- juice(pls_rec_tr, matches("PLS"), s_40380)
> 
> pls_cor <- apply(pls_features, 2, cor, y = weekends$s_40380, method = "spearman")
> pls_cor <- pls_cor[order(-abs(pls_cor))]
> pls_cor
     s_40380        PLS01        PLS02        PLS03        PLS04        PLS07 
 1.000000000  0.804549224  0.363582970  0.163265735  0.159053915  0.154461457 
       PLS17        PLS05        PLS08        PLS06        PLS11        PLS13 
 0.100240185  0.087482788  0.085146271  0.073490255  0.049025947  0.048950716 
       PLS19        PLS12        PLS10        PLS15        PLS16        PLS09 
 0.047683245  0.042737759  0.034438015  0.031065974  0.027324275 -0.014115651 
       PLS14        PLS18        PLS20 
-0.013138491  0.011810647 -0.001263635 
> 
> # ------------------------------------------------------------------------------
> 
> set.seed(7599)
> pls_mod <- 
+   train(pls_rec, data = weekends, method = "lm", trControl = wend_ctrl) %>% 
+   pluck("resample") %>% 
+   mutate(model = "PLS")
> 
> rm(pls_rec, pls_rec_tr)
> 
> mem_in_gb()
0.4 GB
> 
> # ------------------------------------------------------------------------------
> 
> # Merge the first five features from each method for plotting
> comps <- 
+   pca_features %>%
+   dplyr::select(num_range("PC0", 1:5)) %>% 
+   bind_cols(ica_features %>% dplyr::select(num_range("IC0", 1:5))) %>%
+   bind_cols(kpca_features %>% dplyr::select(num_range("kPC0", 1:5))) %>%
+   bind_cols(nnmf_features %>% dplyr::select(num_range("NNMF0", 1:5))) %>%
+   bind_cols(pls_features %>% dplyr::select(num_range("PLS0", 1:5), s_40380)) %>% 
+   gather(name, value, -s_40380) %>% 
+   mutate(
+     Type = gsub("[[:digit:]]", "", name),
+     Type = factor(Type, levels = c("PC", "kPC", "IC", "NNMF", "PLS")),
+     Comp = paste("Component", gsub("([[:alpha:]])|(0)", "", name))
+   )
> 
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#fig:numeric-weekend-scores
> # ggplot(comps, aes(x = value, y = s_40380)) + 
> #   geom_point(cex = .3, alpha = .5) + 
> #   facet_grid(Comp ~ Type, scale = "free_x")
> 
> # ------------------------------------------------------------------------------
> 
> # Collate the resampling results together for interval estimates
> model_res <- 
+   simple_mod  %>% 
+   bind_rows(pca_mod) %>% 
+   bind_rows(kpca_mod) %>% 
+   bind_rows(ica_mod) %>% 
+   bind_rows(nnmf_mod) %>% 
+   bind_rows(pls_mod)
> 
> # https://bookdown.org/max/FES/engineering-numeric-predictors.html#fig:numeric-weekend-ci
> # model_res %>% 
> #   group_by(model) %>% 
> #   do(broom::tidy(t.test(.$RMSE))) %>% 
> #   ggplot() + 
> #   aes(x = reorder(model, estimate), y = estimate) + 
> #   geom_point() + 
> #   geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = .3) + 
> #   coord_flip() + 
> #   xlab("") + 
> #   ylab("RMSE (resampled)")
> 
> 
> # ------------------------------------------------------------------------------
> 
> save(weekends, file = "weekends.RData")
> save(model_res, comps, sig_range, ica_coefs, ica_cor, nnmf_cor, file = "lin_proj_res.RData")
> save(five_pca_coefs, pca_coefs, cum_var, file = "pca_res.RData")
> save(nnmf_coefs, file = "nnmf_res.RData")
> 
> # ------------------------------------------------------------------------------
> 
> sessionInfo()
R version 3.5.2 (2018-12-20)
Platform: x86_64-apple-darwin15.6.0 (64-bit)
Running under: macOS High Sierra 10.13.6

Matrix products: default
BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] parallel  stats     graphics  grDevices utils     datasets  methods  
[8] base     

other attached packages:
 [1] doParallel_1.0.14   iterators_1.0.10    foreach_1.4.4      
 [4] bindrcpp_0.2.2      rlang_0.3.1         tidyselect_0.2.5   
 [7] lobstr_1.0.0        heatmaply_0.15.2    viridis_0.5.1      
[10] viridisLite_0.3.0   plotly_4.8.0        dimRed_0.2.2       
[13] DRR_0.0.3           CVST_0.2-2          Matrix_1.2-15      
[16] htmltools_0.3.6     leaflet_2.0.2       RColorBrewer_1.1-2 
[19] pls_2.7-0           kernlab_0.9-27      yardstick_0.0.2    
[22] tibble_2.0.1        rsample_0.0.4       tidyr_0.8.2        
[25] recipes_0.1.4       purrr_0.3.0         parsnip_0.0.1      
[28] infer_0.4.0         dplyr_0.7.8         dials_0.0.2        
[31] scales_1.0.0        broom_0.5.1         tidymodels_0.0.2   
[34] fastICA_1.2-1       NMF_0.21.0          bigmemory_4.5.33   
[37] Biobase_2.40.0      BiocGenerics_0.26.0 cluster_2.0.7-1    
[40] rngtools_1.3.1      pkgmaker_0.27       registry_0.5       
[43] caret_6.0-82        ggplot2_3.1.0       lattice_0.20-38    

loaded via a namespace (and not attached):
  [1] backports_1.1.3          tidytext_0.2.0           plyr_1.8.4              
  [4] igraph_1.2.2             lazyeval_0.2.1           splines_3.5.2           
  [7] crosstalk_1.0.0          SnowballC_0.5.1          gridBase_0.4-7          
 [10] rstantools_1.5.1         inline_0.3.15            digest_0.6.18           
 [13] rsconnect_0.8.11         gdata_2.18.0             magrittr_1.5            
 [16] gclus_1.3.1              gower_0.1.2              matrixStats_0.54.0      
 [19] xts_0.11-2               prettyunits_1.0.2        colorspace_1.3-2        
 [22] jsonlite_1.6             callr_3.1.1              crayon_1.3.4            
 [25] bigmemory.sri_0.1.3      lme4_1.1-19              bindr_0.1.1             
 [28] survival_2.43-3          zoo_1.8-4                glue_1.3.0              
 [31] gtable_0.2.0             ipred_0.9-8              webshot_0.5.1           
 [34] pkgbuild_1.0.2           rstan_2.18.2             DEoptimR_1.0-8          
 [37] prabclus_2.2-6           mvtnorm_1.0-8            bibtex_0.4.2            
 [40] miniUI_0.1.1.1           Rcpp_1.0.0               xtable_1.8-3            
 [43] mclust_5.4.1             stats4_3.5.2             lava_1.6.4              
 [46] StanHeaders_2.18.0       prodlim_2018.04.18       DT_0.5                  
 [49] httr_1.4.0               htmlwidgets_1.3          threejs_0.3.1           
 [52] gplots_3.0.1             fpc_2.1-11.1             modeltools_0.2-22       
 [55] flexmix_2.3-14           pkgconfig_2.0.2          loo_2.0.0               
 [58] nnet_7.3-12              reshape2_1.4.3           later_0.7.5             
 [61] munsell_0.5.0            tools_3.5.2              cli_1.0.1               
 [64] generics_0.0.2           ggridges_0.5.1           stringr_1.3.1           
 [67] ModelMetrics_1.2.2       processx_3.2.1           knitr_1.20              
 [70] robustbase_0.93-3        caTools_1.17.1.1         dendextend_1.9.0        
 [73] nlme_3.1-137             whisker_0.3-2            mime_0.6                
 [76] rstanarm_2.18.2          tokenizers_0.2.1         compiler_3.5.2          
 [79] bayesplot_1.6.0          shinythemes_1.1.2        rstudioapi_0.8          
 [82] tidyposterior_0.0.2.9000 stringi_1.3.1            ps_1.2.1                
 [85] trimcluster_0.1-2.1      nloptr_1.2.1             markdown_0.8            
 [88] shinyjs_1.0              pillar_1.3.1             bitops_1.0-6            
 [91] data.table_1.11.8        seriation_1.2-3          httpuv_1.4.5            
 [94] R6_2.3.0                 TSP_1.1-6                promises_1.0.1          
 [97] KernSmooth_2.23-15       gridExtra_2.3            janeaustenr_0.1.5       
[100] codetools_0.2-15         colourpicker_1.0         MASS_7.3-51.1           
[103] gtools_3.8.1             assertthat_0.2.0         withr_2.1.2             
[106] shinystan_2.5.0          diptest_0.75-7           grid_3.5.2              
[109] rpart_4.1-13             timeDate_3043.102        class_7.3-14            
[112] minqa_1.2.4              pROC_1.13.0              tidypredict_0.3.0       
[115] shiny_1.2.0              lubridate_1.7.4          base64enc_0.1-3         
[118] dygraphs_1.1.1.6        
> 
> q("no")
> proc.time()
     user    system   elapsed 
58001.874 15027.287  6352.247 
