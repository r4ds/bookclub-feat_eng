[["index.html", "Feature Engineering and Selection Book Club Welcome", " Feature Engineering and Selection Book Club The R4DS Online Learning Community 2022-10-01 Welcome Welcome to the bookclub! This is a companion for the book Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson (Chapman and Hall/CRC, copyright August 2, 2019, 9781138079229). This companion is available at r4ds.io/feat_eng. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize the structure of the book Establish base lines for good practice Define feature engineering "],["structure-of-the-book.html", "1.1 Structure of the book", " 1.1 Structure of the book The book is divided into two main parts: Feature engineering (techniques for augmenting predictors - chapters 2-9) Predicting risk of Ischemic Review of the PMP (predictive modeling process) Exploratory visualization Encoding categorical predictors Engineering numeric predictors Detecting interaction effects Handling missing data Working with profile data (time series analysis) Feature selection (methods for filtering the enhanced predictors - chapters 10-12) Overview Greedy search methods (simple filters and eliminations) Golbal search methods (predictor space investigations) "],["good-practice-guidelines.html", "1.2 Good Practice guidelines", " 1.2 Good Practice guidelines There are some vital steps to take to modeling: knowledge of the process to model collect appropriate data understand variation in the response select relevant predictors utilize a range of models All of these are not enough when model lacks on performance. The answer might be the in the way the predictors are presented to the model. 1.2.1 What is feature engineering “…best re-representation of the predictors to improve model performance.” (ct. Preface) What are the possible ways to acheive a better performance? transform the predictors with special functions (log/exp) add an interaction term (prod/ratio) add a functional transformation (splines/poly) add a re-representation of the predictors (mean/med/standardz) imputing missing values (knn/bagging) Disclaimer: Risk of Overfitting! 1.2.2 Nature of modeling The estimation of uncertainty/noise is another very important step to take. “If a model is only 50% accurate should it be used to make inferences or predictions?” The trade-off between accuracy and interpretability is important, a neural network model might be less explicable but can provide a higher level of accuracy. Feature engineering is a matter of choice in finding the most suitable variable transformation for the best performance. More considerations about bad model reactions to: multicollinarity or correlation between predictors missing values irrelevant predictors "],["a-model-with-two-predictors.html", "1.3 A model with two predictors", " 1.3 A model with two predictors data(segmentationData) This example uses segmentationData. Data originates from an experiment from Hill et al. (2007), a study on “Impact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells.” BMC Bioinformatics. The data set includes a Case vector containing Train and Test variables, with a total of 61 different vectors, about cellular structures and morphology. Selected for this first example are two predictors: EqSphereAreaCh1 and PerimCh1. The objective is to predict shape parameters of poorly-segmented (PS) and well-segmented (WS) cells from the Class variable. This is the full list of variables in the set. ## [1] &quot;Cell&quot; &quot;Case&quot; ## [3] &quot;Class&quot; &quot;AngleCh1&quot; ## [5] &quot;AreaCh1&quot; &quot;AvgIntenCh1&quot; ## [7] &quot;AvgIntenCh2&quot; &quot;AvgIntenCh3&quot; ## [9] &quot;AvgIntenCh4&quot; &quot;ConvexHullAreaRatioCh1&quot; ## [11] &quot;ConvexHullPerimRatioCh1&quot; &quot;DiffIntenDensityCh1&quot; ## [13] &quot;DiffIntenDensityCh3&quot; &quot;DiffIntenDensityCh4&quot; ## [15] &quot;EntropyIntenCh1&quot; &quot;EntropyIntenCh3&quot; ## [17] &quot;EntropyIntenCh4&quot; &quot;EqCircDiamCh1&quot; ## [19] &quot;EqEllipseLWRCh1&quot; &quot;EqEllipseOblateVolCh1&quot; ## [21] &quot;EqEllipseProlateVolCh1&quot; &quot;EqSphereAreaCh1&quot; ## [23] &quot;EqSphereVolCh1&quot; &quot;FiberAlign2Ch3&quot; ## [25] &quot;FiberAlign2Ch4&quot; &quot;FiberLengthCh1&quot; ## [27] &quot;FiberWidthCh1&quot; &quot;IntenCoocASMCh3&quot; ## [29] &quot;IntenCoocASMCh4&quot; &quot;IntenCoocContrastCh3&quot; ## [31] &quot;IntenCoocContrastCh4&quot; &quot;IntenCoocEntropyCh3&quot; ## [33] &quot;IntenCoocEntropyCh4&quot; &quot;IntenCoocMaxCh3&quot; ## [35] &quot;IntenCoocMaxCh4&quot; &quot;KurtIntenCh1&quot; ## [37] &quot;KurtIntenCh3&quot; &quot;KurtIntenCh4&quot; ## [39] &quot;LengthCh1&quot; &quot;NeighborAvgDistCh1&quot; ## [41] &quot;NeighborMinDistCh1&quot; &quot;NeighborVarDistCh1&quot; ## [43] &quot;PerimCh1&quot; &quot;ShapeBFRCh1&quot; ## [45] &quot;ShapeLWRCh1&quot; &quot;ShapeP2ACh1&quot; ## [47] &quot;SkewIntenCh1&quot; &quot;SkewIntenCh3&quot; ## [49] &quot;SkewIntenCh4&quot; &quot;SpotFiberCountCh3&quot; ## [51] &quot;SpotFiberCountCh4&quot; &quot;TotalIntenCh1&quot; ## [53] &quot;TotalIntenCh2&quot; &quot;TotalIntenCh3&quot; ## [55] &quot;TotalIntenCh4&quot; &quot;VarIntenCh1&quot; ## [57] &quot;VarIntenCh3&quot; &quot;VarIntenCh4&quot; ## [59] &quot;WidthCh1&quot; &quot;XCentroid&quot; ## [61] &quot;YCentroid&quot; ## [1] 2019 61 Parsimony: ## Class Area Perimeter ## 1 PS 3278.726 154.89876 ## 2 WS 1727.410 84.56460 ## 3 PS 1194.932 101.09107 ## 4 WS 1027.222 68.71062 ## 5 PS 1035.608 73.40559 ## 6 PS 1433.918 79.47569 The dataset is already split between training and test sets, all that is to be added is cross-validation on the training set. set.seed(2222) folds &lt;- vfold_cv(train, v = 10) A first visualization of the relationship between the two predictors. Check for Class imbalance of the response variable This would be the first level transformation of the response, this type of transformation is considered a structural transformation, we will see more about it later in the book. PS WS tb_class 636.00 373.00 pr_class 0.63 0.37 up_samp_ws &lt;- pr_class[2] Recipes library(themis) log_rec_natural_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) log_rec_inverse_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) %&gt;% step_BoxCox(all_numeric()) Workflow logistic_reg_glm_spec &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) log_wfl_natural_units &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(log_rec_natural_units) log_fit_natural_units &lt;- log_wfl_natural_units %&gt;% fit(train) log_fit_natural_units %&gt;% extract_fit_parsnip() %&gt;% tidy() # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 1.58 0.248 6.36 1.99e-10 2 Area 0.00301 0.000281 10.7 8.95e-27 3 Perimeter -0.0682 0.00604 -11.3 1.47e-29 Prediction with_pred_natural_units &lt;- log_fit_natural_units %&gt;% augment(test) with_pred_natural_units %&gt;% head # A tibble: 6 × 6 Class Area Perimeter .pred_class .pred_PS .pred_WS &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 PS 742. 68.8 PS 0.705 0.295 2 PS 1140. 86.5 PS 0.707 0.293 3 WS 692. 49.5 WS 0.429 0.571 4 WS 709. 50.4 WS 0.431 0.569 5 PS 1006. 89.9 PS 0.820 0.180 6 WS 1983. 112. PS 0.516 0.484 Confusion Matrics Roc Curve with_pred_natural_units %&gt;% roc_curve(Class,.pred_PS) %&gt;% mutate(Format = &quot;Natural Units&quot;) %&gt;% ggplot(aes(1 - specificity, sensitivity))+ geom_line(aes(color = .threshold), size = 1)+ geom_abline(linetype = &quot;dashed&quot;, size = 1, color = &quot;gray&quot;) + scale_colour_continuous()+ theme_fivethirtyeight() + theme(axis.title = element_text()) Workflow set Let’s compare the two transformations with a workflow_set(): full_workflow &lt;- workflow_set( models = list(logitstic = logistic_reg_glm_spec), preproc = list(natural_units = log_rec_natural_units, inverse_units = log_rec_inverse_units)) system.time( grid_results &lt;- full_workflow %&gt;% workflow_map( seed = 1503, resamples = folds, grid = 25, control = control_grid( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE), verbose = TRUE) ) user system elapsed 6.631 0.008 6.639 grid_results # A workflow set/tibble: 2 × 4 wflow_id info option result &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 natural_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; 2 inverse_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; Roc curves for two different recipes roc &lt;- grid_results %&gt;% unnest(result) %&gt;% unnest(.predictions) %&gt;% select(wflow_id, .pred_PS, .pred_WS, .pred_class, Class) %&gt;% group_by(wflow_id) %&gt;% roc_curve(Class, .pred_PS) roc_curves &lt;- roc %&gt;% ggplot( aes(x = 1 - specificity, y = sensitivity, group = wflow_id, color = wflow_id) ) + geom_line(size = 0.5) + geom_abline(lty = 2, alpha = 0.5, color = &quot;gray50&quot;, size = 0.8)+ scale_color_tableau()+ theme_fivethirtyeight()+ theme(axis.title = element_text()) roc_curves "],["important-concepts.html", "1.4 Important concepts", " 1.4 Important concepts Overfitting Supervised and unsupervised Model bias and variance Experience and empirically driven modeling Generalizing the main boundaries, the risk of overfitting the model is always challenged by anomalous patterns new data can hide. 1.4.1 Acknowledge vulnerabilities To consider: small number of observations compared to the number of predictors low bias models can have a higher likelihood of overfitting supervised analysis can be used to detect predictors significance No free lunch therem (Wolpert, 1996) - knowledge is an important part of modeling variance-bias trade-off Low variance: linear/logistic regression and PLS High variance: trees, nearest neighbor, neural networks Bias: level of ability to closer estimation irrilevant predictors can causing excess model variation be data-driven rather than experience-driven big data does not mean better data unlabeled data can improve autoencoders modeling compensatory effect there may not be a unique set of predictors. Finally, one more important consideration is to consider Strategies for Supervised and Unsupervised feature selections. Supervised selection method can be divided into: wrapper methods, such as backwards and stepwise selection embedded methods, such as decision tree variable selection Unsupervised selection method variable encoding, such as dummy or indicator variables 1.4.2 The Modeling process Few steps summary: EDA summary and correlation model methods evaluation model tuning summary measures and EDA residual analysis/ check for systematic issues more feature engineering model selection final bake off prediction "],["predicting-ridership-on-chicago.html", "1.5 Predicting ridership on Chicago", " 1.5 Predicting ridership on Chicago This set will be widely used in the book to predict the number of people entering a train station daily. library(modeldata) modeldata::Chicago %&gt;% head ## # A tibble: 6 × 50 ## rider…¹ Austin Quinc…² Belmont Arche…³ Oak_P…⁴ Western Clark…⁵ Clinton Merch…⁶ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.7 1.46 8.37 4.60 2.01 1.42 3.32 15.6 2.40 6.48 ## 2 15.8 1.50 8.35 4.72 2.09 1.43 3.34 15.7 2.40 6.48 ## 3 15.9 1.52 8.36 4.68 2.11 1.49 3.36 15.6 2.37 6.40 ## 4 15.9 1.49 7.85 4.77 2.17 1.44 3.36 15.7 2.42 6.49 ## 5 15.4 1.50 7.62 4.72 2.06 1.42 3.27 15.6 2.42 5.80 ## 6 2.42 0.693 0.911 2.27 0.624 0.426 1.11 2.41 0.814 0.858 ## # … with 40 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, ## # Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;, Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, ## # Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;, California &lt;dbl&gt;, ## # temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;, ## # humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, ## # wind_max &lt;dbl&gt;, gust &lt;dbl&gt;, gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, ## # weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;, weather_cloud &lt;dbl&gt;, … 1.5.1 Extra Resources Cooking Your Data with Recipes Here is a nice example on how to Compute a sliding mean by Julia Silge caret-vs-tidymodels tidymodels-or-caret-how-they-compare "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log LOG "],["illustrative-example-predicting-risk-of-ischemic-stroke.html", "Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke", " Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke Learning objectives: Understanding the computing part with two examples "],["introduction-1.html", "2.1 Introduction", " 2.1 Introduction Here we see how to make things in practice with two case studies. "],["example-1.html", "2.2 Example 1", " 2.2 Example 1 Code for Ischemic Stroke case study. Code requires these packages, across all of Chapter 2: library(corrplot) library(utils) library(pROC) library(plotly) library(caret) library(patchwork) library(tidymodels) theme_set(theme_bw()) Load the stroke_data.R.data from the Ischemic_Stroke folder here: https://github.com/topepo/FES/tree/master/Data_Sets load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Ischemic_Stroke/stroke_data.RData?raw=true&quot;)) load(url(&quot;https://github.com/topepo/FES/blob/master/02_Predicting_Risk_of_Ischemic_Stroke/stroke_rfe.RData?raw=true&quot;)) #pre_split data how many in each class by set stroke_train %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Training&quot;) %&gt;% bind_rows( stroke_test %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Testing&quot;) ) %&gt;% spread(Stroke, n) ## Data N Y ## 1 Testing 18 19 ## 2 Training 44 45 #sampling using tidymodels/rsample package all_stroke &lt;- bind_rows(stroke_train, stroke_test) #put all the data back together tidy_sample &lt;- #sample, but fixing proportion of Stroke between test and train initial_split(all_stroke, prop = 0.71, strata = Stroke) tidy_testing &lt;- testing(tidy_sample) #extract testing tidy_training &lt;- training(tidy_sample) #extract training tidy_training %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Training&quot;) %&gt;% bind_rows( tidy_testing %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Testing&quot;) ) %&gt;% spread(Stroke, n) ## Data N Y ## 1 Testing 18 19 ## 2 Training 44 45 # distribution of training and testing is either exactly the same or one off depending on seed 2.2.1 Predictor Quality The first thing is to just look at your data: what’s missing what’s normal(ish) what’s the range what’s the data type dplyr’s glimpse is a good starting point. dplyr::glimpse(tidy_training) ## Rows: 89 ## Columns: 29 ## $ Stroke &lt;fct&gt; N, N, N, N, N, N, N, N, N, N, N, N, N, N, … ## $ NASCET &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ CALCVol &lt;dbl&gt; 235.25260, 31.43360, 360.73754, 433.34638,… ## $ CALCVolProp &lt;dbl&gt; 0.070442702, 0.016164769, 0.073350960, 0.1… ## $ MATXVol &lt;dbl&gt; 3156.835, 3032.861, 4444.045, 3106.593, 24… ## $ MATXVolProp &lt;dbl&gt; 0.7599582, 0.8133063, 0.7839963, 0.7520699… ## $ LRNCVol &lt;dbl&gt; 224.87171, 368.56066, 277.26275, 36.50728,… ## $ LRNCVolProp &lt;dbl&gt; 0.09108513, 0.13398944, 0.06073955, 0.0146… ## $ MaxCALCArea &lt;dbl&gt; 12.350494, 7.130660, 21.297476, 27.488064,… ## $ MaxCALCAreaProp &lt;dbl&gt; 0.3657684, 0.2112469, 0.3862498, 0.4901225… ## $ MaxDilationByArea &lt;dbl&gt; 520.98259, 91.72005, 60.42487, 240.78404, … ## $ MaxMATXArea &lt;dbl&gt; 71.24743, 27.21084, 43.43857, 38.78870, 36… ## $ MaxMATXAreaProp &lt;dbl&gt; 0.9523705, 0.9455539, 0.9526153, 0.9439308… ## $ MaxLRNCArea &lt;dbl&gt; 21.686815, 6.434661, 9.403324, 3.242581, 3… ## $ MaxLRNCAreaProp &lt;dbl&gt; 0.42957812, 0.28151013, 0.35606305, 0.0725… ## $ MaxMaxWallThickness &lt;dbl&gt; 2.409943, 2.540334, 3.411158, 3.695132, 4.… ## $ MaxRemodelingRatio &lt;dbl&gt; 5.697931, 1.739927, 2.059370, 2.909791, 2.… ## $ MaxStenosisByArea &lt;dbl&gt; 18.99554, 30.23761, 41.56107, 42.17789, 43… ## $ MaxWallArea &lt;dbl&gt; 106.20676, 33.36714, 62.27910, 59.76256, 5… ## $ WallVol &lt;dbl&gt; 4192.170, 3917.040, 5814.552, 2489.344, 29… ## $ MaxStenosisByDiameter &lt;dbl&gt; 10.54411, 18.64620, 36.52606, 31.82890, 33… ## $ age &lt;int&gt; 72, 76, 82, 83, 85, 56, 60, 70, 63, 77, 90… ## $ sex &lt;int&gt; 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, … ## $ SmokingHistory &lt;int&gt; 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, … ## $ AtrialFibrillation &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … ## $ CoronaryArteryDisease &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, … ## $ DiabetesHistory &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ HypercholesterolemiaHistory &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, … ## $ HypertensionHistory &lt;int&gt; 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, … # skimr is better (by a lot) # https://github.com/ropensci/skimr skimr::skim(all_stroke) Table 2.1: Data summary Name all_stroke Number of rows 126 Number of columns 29 _______________________ Column type frequency: factor 1 numeric 28 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Stroke 0 1 FALSE 2 Y: 64, N: 62 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist NASCET 0 1 0.35 0.48 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▅ CALCVol 0 1 199.45 213.49 4.02 77.12 146.76 253.95 1517.54 ▇▂▁▁▁ CALCVolProp 0 1 0.07 0.06 0.00 0.04 0.06 0.10 0.44 ▇▂▁▁▁ MATXVol 0 1 3074.68 738.98 939.34 2525.41 3070.49 3596.27 4821.52 ▁▅▇▇▂ MATXVolProp 0 1 0.77 0.06 0.48 0.75 0.78 0.81 0.87 ▁▁▂▇▇ LRNCVol 0 1 387.00 307.26 15.66 156.85 317.70 508.37 1612.40 ▇▅▂▁▁ LRNCVolProp 0 1 0.11 0.07 0.01 0.06 0.10 0.15 0.49 ▇▅▁▁▁ MaxCALCArea 0 1 20.40 13.13 1.75 10.86 16.54 26.77 63.35 ▇▇▃▁▁ MaxCALCAreaProp 0 1 0.35 0.16 0.01 0.24 0.33 0.42 0.96 ▂▇▅▁▁ MaxDilationByArea 0 1 451.18 870.06 15.53 78.83 150.26 370.28 5920.63 ▇▁▁▁▁ MaxMATXArea 0 1 65.49 58.05 17.19 36.63 45.73 67.48 341.12 ▇▁▁▁▁ MaxMATXAreaProp 0 1 0.95 0.09 0.83 0.94 0.95 0.96 1.94 ▇▁▁▁▁ MaxLRNCArea 0 1 17.62 17.83 1.26 6.82 12.87 20.51 134.34 ▇▂▁▁▁ MaxLRNCAreaProp 0 1 0.33 0.15 0.03 0.20 0.32 0.44 0.74 ▃▇▆▅▁ MaxMaxWallThickness 0 1 5.86 9.23 1.31 3.62 4.72 5.51 83.56 ▇▁▁▁▁ MaxRemodelingRatio 0 1 4.75 4.48 1.21 2.31 3.22 5.35 27.88 ▇▁▁▁▁ MaxStenosisByArea 0 1 74.80 19.67 19.00 61.81 79.34 90.00 100.00 ▁▃▃▇▇ MaxWallArea 0 1 89.94 74.97 19.97 53.21 66.67 92.80 454.17 ▇▂▁▁▁ WallVol 0 1 4156.53 1083.81 1416.75 3464.17 4056.41 5066.84 6306.81 ▁▅▇▆▃ MaxStenosisByDiameter 0 1 63.76 22.64 10.54 47.63 63.42 78.97 100.00 ▁▆▇▅▆ age 0 1 72.10 10.83 39.00 65.00 72.00 80.00 90.00 ▁▂▇▇▇ sex 0 1 0.56 0.50 0.00 0.00 1.00 1.00 1.00 ▆▁▁▁▇ SmokingHistory 0 1 0.58 0.50 0.00 0.00 1.00 1.00 1.00 ▆▁▁▁▇ AtrialFibrillation 0 1 0.12 0.33 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▁ CoronaryArteryDisease 0 1 0.28 0.45 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▃ DiabetesHistory 0 1 0.22 0.42 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ HypercholesterolemiaHistory 0 1 0.53 0.50 0.00 0.00 1.00 1.00 1.00 ▇▁▁▁▇ HypertensionHistory 0 1 0.77 0.42 0.00 1.00 1.00 1.00 1.00 ▂▁▁▁▇ The book says original dataset has 4 missing data points, but already been median imputed via recipes::step_impute_median() imputation methhods discussed more in Chapter 8, some models hate missing data if you have missing data… use naniar/visdat to visualize datasets and understand where you’re missing data getting-started-w-naniar visdat::vis_dat(tidy_training) # https://bookdown.org/max/FES/numeric-one-to-one.html#numeric-one-to-one # this is a plot of the distribution of MaxLRNCArea, very right skewed fig_2_2_a &lt;- all_stroke %&gt;% ggplot(aes(x = MaxLRNCArea)) + geom_histogram(bins = 15, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;MaxLRNCArea&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(a)&quot;) + theme_bw() fig_2_2_a # same plot but after yeojohnson transformmation, become normal-like fig_2_2_b &lt;- recipe(Stroke ~ ., data = all_stroke) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% prep(.) %&gt;% bake(., new_data = NULL) %&gt;% ggplot(aes(x = MaxLRNCArea)) + geom_histogram(bins = 15, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;Transformed MaxLRNCArea&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(b)&quot;) + theme_bw() #sidebyside with patchwork fig_2_2_a + fig_2_2_b 2.2.2 understanding interactions and multicollinearity some models hate correlated traits this only looks at imaging traits, (why is it called risk?) risk_train &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_of(VC_preds)) %&gt;% # center the data step_scale(all_of(VC_preds)) %&gt;% # scale the data step_YeoJohnson(all_of(VC_preds)) %&gt;% # YeoJohnson transform https://recipes.tidymodels.org/reference/step_YeoJohnson.html prep(.) %&gt;% bake(., new_data = NULL) %&gt;% # juice is superseded by bake select(-one_of(c(&quot;Stroke&quot;, &quot;NASCET&quot;, risk_preds))) #select everything but these risk_corr &lt;- cor(risk_train) #make a correlation matrix corrplot(risk_corr, addgrid.col = rgb(0, 0, 0, .05), order = &quot;hclust&quot;) #plot that # you can remove these with step_corr(all_predictors(), threshold = 0.75) %&gt;% risk_train_step_corr &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_of(VC_preds)) %&gt;% # center the data step_scale(all_of(VC_preds)) %&gt;% # scale the data step_YeoJohnson(all_of(VC_preds)) %&gt;% # YeoJohnson transform https://recipes.tidymodels.org/reference/step_YeoJohnson.html step_corr(all_predictors(), threshold = 0.75) %&gt;% # remove &quot;extra&quot; predictors with correlations higher than 0.75 prep(.) %&gt;% bake(., new_data = NULL) %&gt;% # juice is superseded by bake select(-one_of(c(&quot;Stroke&quot;, &quot;NASCET&quot;, risk_preds))) #select everything but these risk_corr_step_corr &lt;- cor(risk_train_step_corr) #make a correlation matrix corrplot(risk_corr_step_corr, addgrid.col = rgb(0, 0, 0, .05), order = &quot;hclust&quot;) #plot that #BUT WE&#39;RE NOT DOING THAT YET! Chapter 3 shows more methods on this. "],["example-2.html", "2.3 Example 2", " 2.3 Example 2 Code for Section 2.4 at https://bookdown.org/max/FES/stroke-tour.html#stroke-exploration Code to compare 2-way interaction models to their main effects model a and b are two models from train() compare_models_1way &lt;- function(a, b, metric = a$metric[1], ...) { mods &lt;- list(a, b) rs &lt;- resamples(mods) diffs &lt;- diff(rs, metric = metric[1], ...) diffs$statistics[[1]][[1]] } risk_preds is contained in the original data file and has the predictor names for the risk related variables 2.3.1 Create a “null model” with no predictors to get baseline performance Compare the models with single predictors to the risk model. These data make https://bookdown.org/max/FES/stroke-tour.html#tab:stroke-strokeRiskAssociations VC_preds and risk_preds contain the predictor names for different sets. one_predictor_res &lt;- data.frame(Predictor = c(VC_preds, risk_preds), Improvement = NA, Pvalue = NA, ROC = NA, stringsAsFactors = FALSE) for (i in 1:nrow(one_predictor_res)) { set.seed(63331) var_mod &lt;- train(Stroke ~ ., data = stroke_train[, c(&quot;Stroke&quot;, one_predictor_res$Predictor[i])], method = &quot;glm&quot;, metric = &quot;ROC&quot;, trControl = ctrl) tmp_diff &lt;- compare_models_1way(var_mod, null_mod, alternative = &quot;greater&quot;) one_predictor_res$ROC[i] &lt;- getTrainPerf(var_mod)[1, &quot;TrainROC&quot;] one_predictor_res$Improvement[i] &lt;- tmp_diff$estimate one_predictor_res$Pvalue[i] &lt;- tmp_diff$p.value } 2.3.2 With Tidymodels attempting this with tidymodels, not exactly the same need to make some sort of resample object to feed workflow_map model_folds &lt;- vfold_cv(tidy_training, v = 10, repeats = 5) # model_boots &lt;- bootstraps(tidy_training, times = 50) #want to bootstrap this? #defining the null model null_class_model &lt;- null_model() %&gt;% set_engine(&quot;parsnip&quot;) %&gt;% set_mode(&quot;classification&quot;) null_model &lt;- workflow_set(preproc = c(Stroke ~ .), models = list(null_mod = null_class_model)) # defining the model lm_model &lt;- logistic_reg( mode = &quot;classification&quot;, # outcome is a classification Stroke Y/N engine = &quot;glm&quot;, #using glm like the example penalty = NULL, mixture = NULL ) # make named list of single variable formulae single_var_formulae &lt;- c(VC_preds, risk_preds) %&gt;% paste0(&quot;Stroke ~ &quot;, .) %&gt;% set_names(., c(VC_preds, risk_preds)) %&gt;% as.list() %&gt;% map(., as.formula) Create the workflow set, all of our models use the same type of model and input data single_var_models &lt;- workflow_set(preproc = single_var_formulae, models = list(lm = lm_model)) all_models &lt;- bind_rows(null_model, single_var_models) # control grid/resamples allow processing of resampled data and parallel processing # here we are asking only to save the predictions control &lt;- control_resamples(save_pred = TRUE) doParallel::registerDoParallel() all_models1 &lt;- all_models %&gt;% #map over each model and its resamples, use the control parameters, and be noisy workflow_map(., resamples = model_folds, control = control, verbose = TRUE) # save(all_models1, file = &quot;data/all_models1.RData&quot;, compress = &quot;xz&quot;) # per model what are the predictions, summmarize over resamples collect_predictions(all_models1, summarize = TRUE) ## # A tibble: 2,492 × 9 ## wflow_id .config preproc model .row Stroke .pred_N .pred_Y .pred…¹ ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 formula_null_mod Preproce… formula null… 1 N 0.495 0.505 Y ## 2 formula_null_mod Preproce… formula null… 2 N 0.478 0.522 Y ## 3 formula_null_mod Preproce… formula null… 3 N 0.495 0.505 Y ## 4 formula_null_mod Preproce… formula null… 4 N 0.475 0.525 Y ## 5 formula_null_mod Preproce… formula null… 5 N 0.485 0.515 Y ## 6 formula_null_mod Preproce… formula null… 6 N 0.498 0.502 Y ## 7 formula_null_mod Preproce… formula null… 7 N 0.488 0.512 Y ## 8 formula_null_mod Preproce… formula null… 8 N 0.485 0.515 Y ## 9 formula_null_mod Preproce… formula null… 9 N 0.491 0.509 Y ## 10 formula_null_mod Preproce… formula null… 10 N 0.482 0.518 Y ## # … with 2,482 more rows, and abbreviated variable name ¹​.pred_class # per model what are the outputs in terms of fit (auc_roc) and accuracy collect_metrics(all_models1) ## # A tibble: 56 × 9 ## wflow_id .config preproc model .metric .esti…¹ mean n std_err ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 formula_null_mod Preproces… formula null… accura… binary 0.378 50 0.0131 ## 2 formula_null_mod Preproces… formula null… roc_auc binary 0.5 50 0 ## 3 CALCVol_lm Preproces… formula logi… accura… binary 0.388 50 0.0183 ## 4 CALCVol_lm Preproces… formula logi… roc_auc binary 0.457 50 0.0291 ## 5 CALCVolProp_lm Preproces… formula logi… accura… binary 0.441 50 0.0196 ## 6 CALCVolProp_lm Preproces… formula logi… roc_auc binary 0.502 50 0.0260 ## 7 MATXVol_lm Preproces… formula logi… accura… binary 0.346 50 0.0177 ## 8 MATXVol_lm Preproces… formula logi… roc_auc binary 0.380 50 0.0277 ## 9 MATXVolProp_lm Preproces… formula logi… accura… binary 0.415 50 0.0222 ## 10 MATXVolProp_lm Preproces… formula logi… roc_auc binary 0.516 50 0.0285 ## # … with 46 more rows, and abbreviated variable name ¹​.estimator # plot the output to show which individual parameters have the most impact autoplot( all_models1, rank_metric = &quot;roc_auc&quot;, # &lt;- how to order models metric = &quot;roc_auc&quot;, # &lt;- which metric to visualize select_best = FALSE # &lt;- one point per workflow ) + geom_text(aes(y = mean - 1/10*mean, label = wflow_id), angle = 90, hjust = 1) + theme(legend.position = &quot;none&quot;) # per model, sort by auc_roc all_models1 %&gt;% rank_results() %&gt;% filter(.metric == &quot;roc_auc&quot;) ## # A tibble: 28 × 9 ## wflow_id .config .metric mean std_err n prepr…¹ model rank ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 MaxRemodelingRatio_lm Prepro… roc_auc 0.682 0.0300 50 formula logi… 1 ## 2 MaxMaxWallThickness_… Prepro… roc_auc 0.669 0.0276 50 formula logi… 2 ## 3 MaxStenosisByArea_lm Prepro… roc_auc 0.651 0.0256 50 formula logi… 3 ## 4 MaxStenosisByDiamete… Prepro… roc_auc 0.640 0.0281 50 formula logi… 4 ## 5 MaxDilationByArea_lm Prepro… roc_auc 0.625 0.0302 50 formula logi… 5 ## 6 MaxLRNCArea_lm Prepro… roc_auc 0.622 0.0281 50 formula logi… 6 ## 7 LRNCVolProp_lm Prepro… roc_auc 0.598 0.0260 50 formula logi… 7 ## 8 MaxMATXArea_lm Prepro… roc_auc 0.595 0.0298 50 formula logi… 8 ## 9 MaxWallArea_lm Prepro… roc_auc 0.591 0.0300 50 formula logi… 9 ## 10 CoronaryArteryDiseas… Prepro… roc_auc 0.574 0.0208 50 formula logi… 10 ## # … with 18 more rows, and abbreviated variable name ¹​preprocessor # Data in table 2.3 # https://bookdown.org/max/FES/stroke-tour.html#tab:stroke-strokeRiskAssociations one_predictor_res %&gt;% dplyr::filter(Predictor %in% risk_preds) %&gt;% arrange(Pvalue) ## Predictor Improvement Pvalue ROC ## 1 CoronaryArteryDisease 0.079000 0.0002957741 0.579000 ## 2 DiabetesHistory 0.066500 0.0003019908 0.566500 ## 3 HypertensionHistory 0.065000 0.0004269919 0.565000 ## 4 age 0.083375 0.0010729715 0.583375 ## 5 AtrialFibrillation 0.044000 0.0013131334 0.544000 ## 6 SmokingHistory -0.009500 0.6520765973 0.490500 ## 7 sex -0.034500 0.9287682162 0.465500 ## 8 HypercholesterolemiaHistory -0.101500 0.9999999044 0.398500 # Figure 2.4 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-vascuCAPAssocations vc_pred &lt;- recipe(Stroke ~ ., data = stroke_train %&gt;% dplyr::select(Stroke, !!!VC_preds)) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% prep(stroke_train %&gt;% dplyr::select(Stroke, !!!VC_preds)) %&gt;% bake(., new_data = NULL) %&gt;% gather(Predictor, value, -Stroke) vc_pred%&gt;%head ## # A tibble: 6 × 3 ## Stroke Predictor value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N CALCVol 9.51 ## 2 N CALCVol 4.90 ## 3 N CALCVol 7.63 ## 4 Y CALCVol 13.2 ## 5 N CALCVol 6.93 ## 6 N CALCVol 2.27 pred_max &lt;- vc_pred %&gt;% group_by(Predictor) %&gt;% summarize(max_val = max(value)) %&gt;% inner_join(one_predictor_res %&gt;% dplyr::select(Pvalue, Predictor)) %&gt;% mutate( x = 1.5, value = 1.25 * max_val, label = paste0(&quot;p-value: &quot;, format.pval(Pvalue, digits = 2, sci = FALSE, eps = .0001)) ) ## Joining, by = &quot;Predictor&quot; pred_max%&gt;%head ## # A tibble: 6 × 6 ## Predictor max_val Pvalue x value label ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CALCVol 15.7 0.000872 1.5 19.6 p-value: 0.00087 ## 2 CALCVolProp 0.440 0.000856 1.5 0.550 p-value: 0.00086 ## 3 LRNCVol 23.2 1.00 1.5 29.0 p-value: 1.00000 ## 4 LRNCVolProp 0.495 0.207 1.5 0.618 p-value: 0.20723 ## 5 MATXVol 41681. 1.00 1.5 52101. p-value: 1.00000 ## 6 MATXVolProp 0.870 1.00 1.5 1.09 p-value: 1.00000 new_order &lt;- pred_max$Predictor[order(pred_max$Pvalue)] vc_pred &lt;- vc_pred %&gt;% mutate(Predictor = factor(Predictor, levels = new_order)) pred_max &lt;- pred_max %&gt;% mutate(Predictor = factor(Predictor, levels = new_order)) fig_2_4 &lt;- ggplot(vc_pred, aes(x = Stroke, y = value)) + geom_boxplot() + geom_point(alpha = 0.3, cex = .5) + geom_text(data = pred_max, aes(x = x, label = label), size = 3) + facet_wrap(~Predictor, scales = &quot;free_y&quot;) + ylab(&quot;&quot;) fig_2_4 # Figure 2.5 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-maxRemodelingRatioROC fig_2_5 &lt;- roc_curve(stroke_train, Stroke, MaxRemodelingRatio) %&gt;% # used opposite values ggplot(aes(x = specificity, y = 1-sensitivity)) + geom_abline(alpha = .5, lty = 2) + geom_path() fig_2_5 2.3.3 Interaction exploration Here they create all the pairs of all of the image analysis components there are 171 interactions pairs &lt;- combn(VC_preds, 2) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) %&gt;% mutate( Improvement = NA, Pvalue = NA, ROC = NA ) Run comparisons with caret retained_pairs &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5 &amp; Pvalue &lt;= 0.2) retained_pairs%&gt;%head ## V1 V2 Improvement Pvalue ROC ## 1 MATXVol MaxMaxWallThickness 0.07635 0.001903838 0.65435 ## 2 MATXVol MaxRemodelingRatio 0.11085 0.001345363 0.55150 ## 3 MATXVol MaxStenosisByArea 0.02325 0.086210254 0.60960 ## 4 MATXVol MaxStenosisByDiameter 0.01930 0.197228736 0.61550 ## 5 LRNCVol MaxMATXArea 0.02590 0.152003535 0.55690 ## 6 LRNCVol MaxRemodelingRatio 0.03345 0.119774508 0.59510 # Figure 2.6 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-interactionScreening vol_plot &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5) %&gt;% mutate(Term = paste(V1, &quot;by&quot;, V2, &quot;\\nROC:&quot;, round(ROC, 2))) %&gt;% ggplot(aes(x = Improvement, y = -log10(Pvalue))) + xlab(&quot;Improvement&quot;) + geom_point(alpha = .2, aes(size = ROC, text = Term)) vol_plot &lt;- ggplotly(vol_plot, tooltip = &quot;Term&quot;) vol_plot Create interaction formula of things that matter most int_form &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5 &amp; Pvalue &lt;= 0.2 &amp; Improvement &gt; 0) %&gt;% mutate(form = paste0(V1, &quot;:&quot;, V2)) %&gt;% pull(form) %&gt;% paste(collapse = &quot;+&quot;) int_form &lt;- paste(&quot;~&quot;, int_form) int_form &lt;- as.formula(int_form) int_form%&gt;%head ## ~MATXVol:MaxMaxWallThickness + MATXVol:MaxRemodelingRatio + MATXVol:MaxStenosisByArea + ## MATXVol:MaxStenosisByDiameter + LRNCVol:MaxMATXArea + LRNCVol:MaxRemodelingRatio + ## MaxCALCAreaProp:MaxMATXAreaProp + MaxCALCAreaProp:MaxRemodelingRatio + ## MaxDilationByArea:MaxMaxWallThickness + MaxDilationByArea:MaxRemodelingRatio + ## MaxMATXAreaProp:MaxLRNCArea + MaxMATXAreaProp:MaxRemodelingRatio + ## MaxMATXAreaProp:MaxWallArea + MaxMaxWallThickness:MaxStenosisByArea + ## MaxMaxWallThickness:WallVol + MaxRemodelingRatio:MaxWallArea + ## MaxRemodelingRatio:WallVol + MaxStenosisByArea:WallVol This part of the script is to work through all of of the potential models: original risk set alone imaging predictors alone risk and imaging predictors together imaging predictors and interactions of imaging predictors, and risk, imaging predictors, and interactions of imaging predictors All the models are run below. risk_train &lt;- stroke_train %&gt;% dplyr::select(one_of(risk_preds), Stroke) risk_train%&gt;%head ## age sex SmokingHistory AtrialFibrillation CoronaryArteryDisease ## 1 72 1 1 0 0 ## 2 76 1 1 0 0 ## 4 72 0 0 0 0 ## 5 61 1 1 0 0 ## 7 65 1 0 0 0 ## 8 64 1 1 0 1 ## DiabetesHistory HypercholesterolemiaHistory HypertensionHistory Stroke ## 1 0 0 1 N ## 2 1 1 1 N ## 4 0 0 0 N ## 5 1 1 1 Y ## 7 0 0 1 N ## 8 0 1 1 N image_train &lt;- stroke_train %&gt;% dplyr::select(one_of(VC_preds), Stroke) image_train%&gt;%head ## CALCVol CALCVolProp MATXVol MATXVolProp LRNCVol LRNCVolProp MaxCALCArea ## 1 235.252599 0.070442702 3156.835 0.7599582 224.87171 0.09108513 12.350494 ## 2 31.433595 0.016164769 3032.861 0.8133063 368.56066 0.13398944 7.130660 ## 4 113.404823 0.038081488 3835.220 0.7825256 321.15893 0.08303659 16.286916 ## 5 780.823789 0.213432061 3518.877 0.7610895 140.51735 0.03206536 63.350869 ## 7 84.055774 0.041383842 2990.273 0.7498691 293.26992 0.07539753 17.583561 ## 8 5.644322 0.002824946 3359.323 0.8492801 55.76888 0.01983567 2.841252 ## MaxCALCAreaProp MaxDilationByArea MaxMATXArea MaxMATXAreaProp MaxLRNCArea ## 1 0.36576842 520.98259 71.24743 0.9523705 21.686815 ## 2 0.21124686 91.72005 27.21084 0.9455539 6.434661 ## 4 0.40881121 270.96930 38.12211 0.9459098 5.705054 ## 5 0.57620574 2270.45120 341.12089 0.9691989 6.046787 ## 7 0.32150685 95.15505 56.57457 0.9213197 7.213451 ## 8 0.07734609 298.42121 33.92709 0.9612049 4.595196 ## MaxLRNCAreaProp MaxMaxWallThickness MaxRemodelingRatio MaxStenosisByArea ## 1 0.4295781 2.409943 5.697931 18.99554 ## 2 0.2815101 2.540334 1.739927 30.23761 ## 4 0.1547786 3.708515 2.831636 33.93906 ## 5 0.1870965 6.115838 15.647750 34.30985 ## 7 0.2169263 3.975168 1.912069 36.59954 ## 8 0.2053274 2.581908 2.181675 40.31766 ## MaxWallArea WallVol MaxStenosisByDiameter Stroke ## 1 106.20676 4192.170 10.54411 N ## 2 33.36714 3917.040 18.64620 N ## 4 55.34671 4935.327 19.73511 N ## 5 426.47858 4909.504 20.28832 Y ## 7 59.82696 4045.053 49.29705 N ## 8 34.79742 3960.832 30.50857 N fiveStats &lt;- function(...) c(twoClassSummary(...), defaultSummary(...)) internal_ctrl = trainControl(method = &quot;none&quot;, classProbs = TRUE, allowParallel = FALSE) lrFuncsNew &lt;- caretFuncs lrFuncsNew$summary &lt;- fiveStats rfeCtrl &lt;- rfeControl(functions = lrFuncsNew, method = &quot;repeatedcv&quot;, repeats = 5, rerank = FALSE, returnResamp = &quot;all&quot;, saveDetails = TRUE, verbose = FALSE) RFE procedure using risk predictors All pair-wise interactions. risk_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_interact(~ all_predictors():all_predictors()) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_int_filtered_rfe &lt;- rfe( risk_int_filtered_recipe, data = risk_train, sizes = 1:36, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) # Main effects risk_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_main_filtered_rfe &lt;- rfe( risk_main_filtered_recipe, data = risk_train, sizes = 1:8, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) RFE procedure using imaging predictors. img_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_interact(int_form) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_int_filtered_rfe &lt;- rfe( img_int_filtered_recipe, data = image_train, sizes = 1:35, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_main_filtered_rfe &lt;- rfe( img_main_filtered_recipe, data = image_train, sizes = 1:19, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_interact(int_form) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_int_filtered_rfe &lt;- rfe( both_int_filtered_recipe, data = stroke_train, sizes = 1:44, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_main_filtered_rfe &lt;- rfe( both_main_filtered_recipe, data = stroke_train, sizes = 1:28, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) risk_int_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_interact(~ all_predictors():all_predictors()) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_int_rfe &lt;- rfe( risk_int_recipe, data = risk_train, sizes = 1:36, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) risk_main_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_main_rfe &lt;- rfe( risk_main_recipe, data = risk_train, sizes = 1:8, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_int_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_interact(int_form) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_int_rfe &lt;- rfe( img_int_recipe, data = image_train, sizes = 1:35, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_main_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_main_rfe &lt;- rfe( img_main_recipe, data = image_train, sizes = 1:19, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_int_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_interact(int_form) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_int_rfe &lt;- rfe( both_int_recipe, data = stroke_train, sizes = 1:44, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_main_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_main_rfe &lt;- rfe( both_main_recipe, data = stroke_train, sizes = 1:28, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) format_data &lt;- function(x, lab, int = FALSE) { dat &lt;- x %&gt;% pluck(&quot;results&quot;) %&gt;% mutate(Predictors = !!lab) %&gt;% dplyr::select(ROC, Variables, Predictors, Variables, Num_Resamples) %&gt;% mutate(Model = &quot;Main Effects&quot;) if (int) dat$Model &lt;- &quot;Interactions&quot; dat } filtered_dat &lt;- bind_rows( format_data(risk_main_filtered_rfe, lab = &quot;Risk Predictors&quot;), format_data(risk_int_filtered_rfe, lab = &quot;Risk Predictors&quot;, TRUE), format_data(img_main_filtered_rfe, lab = &quot;Imaging Predictors&quot;), format_data(img_int_filtered_rfe, lab = &quot;Imaging Predictors&quot;, TRUE), format_data(both_main_filtered_rfe, lab = &quot;All Predictors&quot;), format_data(both_int_filtered_rfe, lab = &quot;All Predictors&quot;, TRUE) ) %&gt;% mutate( Predictors = factor( Predictors, levels = c(&quot;Risk Predictors&quot;, &quot;Imaging Predictors&quot;, &quot;All Predictors&quot;) ), Model = factor(Model, levels = c(&quot;Main Effects&quot;, &quot;Interactions&quot;)), Filtering = &quot;Correlation Filter&quot; ) unfiltered_dat &lt;- bind_rows( format_data(risk_main_rfe, lab = &quot;Risk Predictors&quot;), format_data(risk_int_rfe, lab = &quot;Risk Predictors&quot;, TRUE), format_data(img_main_rfe, lab = &quot;Imaging Predictors&quot;), format_data(img_int_rfe, lab = &quot;Imaging Predictors&quot;, TRUE), format_data(both_main_rfe, lab = &quot;All Predictors&quot;), format_data(both_int_rfe, lab = &quot;All Predictors&quot;, TRUE) ) %&gt;% mutate( Predictors = factor( Predictors, levels = c(&quot;Risk Predictors&quot;, &quot;Imaging Predictors&quot;, &quot;All Predictors&quot;) ), Model = factor(Model, levels = c(&quot;Main Effects&quot;, &quot;Interactions&quot;)), Filtering = &quot;No Filter&quot; ) rfe_data &lt;- bind_rows(filtered_dat, unfiltered_dat) %&gt;% mutate( Filtering = factor(Filtering, levels = c(&quot;No Filter&quot;, &quot;Correlation Filter&quot;)) ) # https://bookdown.org/max/FES/predictive-modeling-across-sets.html#fig:stroke-rfe-res ggplot(rfe_data, aes(x = Variables, y = ROC, col = Model)) + geom_point(size = 0.75) + geom_line() + facet_grid(Filtering ~ Predictors) + scale_color_manual(values = c(&quot;#6A3D9A&quot;, &quot;#CAB2D6&quot;)) # https://bookdown.org/max/FES/predictive-modeling-across-sets.html#tab:stroke-rfe-tab rfe_tab &lt;- img_main_filtered_rfe %&gt;% pluck(&quot;variables&quot;) %&gt;% filter(Variables == img_main_filtered_rfe$optsize) %&gt;% group_by(var) %&gt;% count() %&gt;% arrange(desc(n)) %&gt;% mutate(final = ifelse(var %in% img_main_filtered_rfe$optVariables, &quot;Yes&quot;, &quot;No&quot;)) %&gt;% ungroup() Meeting Videos 2.3.4 Cohort 1 Meeting chat log LOG "],["a-review-of-the-predictive-modeling-process.html", "Chapter 3 A Review of the Predictive Modeling Process", " Chapter 3 A Review of the Predictive Modeling Process Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-1.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log LOG "],["exploratory-visualizations.html", "Chapter 4 Exploratory Visualizations", " Chapter 4 Exploratory Visualizations Learning objectives: Perform exploratory data visualization for the Chicago train ridership and OKCupid datasets. Perform univariate and bivariate visualizations for numerical variables. Perform visualizations for categorical variables. Perform post-modeling visualizations. "],["data-visualization-chart.html", "4.1 Data Visualization Chart", " 4.1 Data Visualization Chart Source: Exploratory Data Analysis for Feature Selection in Machine Learning - Google Cloud Another reference for data visualization using R Data Visualization with R "],["introduction-to-the-chicago-train-ridership-dataset.html", "4.2 Introduction to the Chicago Train Ridership Dataset", " 4.2 Introduction to the Chicago Train Ridership Dataset Source: Wikimedia Commons, Creative Commons license Our interest is predicting the ridership at the Clark/Lake in the Chicago Loop. Source: Google Maps "],["chicago-train-ridership-dataset.html", "4.3 Chicago Train Ridership dataset", " 4.3 Chicago Train Ridership dataset library(tidyverse) library(lubridate) library(plotly) library(patchwork) library(here) ## here() starts at /home/runner/work/bookclub-feat_eng/bookclub-feat_eng library(heatmaply) ## Loading required package: viridis ## Loading required package: viridisLite ## ## Attaching package: &#39;viridis&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## viridis_pal ## ## ====================== ## Welcome to heatmaply version 1.3.0 ## ## Type citation(&#39;heatmaply&#39;) for how to cite the package. ## Type ?heatmaply for the main documentation. ## ## The github page is: https://github.com/talgalili/heatmaply/ ## Please submit your suggestions and bug-reports at: https://github.com/talgalili/heatmaply/issues ## You may ask questions at stackoverflow, use the r and heatmaply tags: ## https://stackoverflow.com/questions/tagged/heatmaply ## ====================== library(RColorBrewer) library(skimr) library(vcd) ## Loading required package: grid library(colorspace) library(FactoMineR) library(caret) load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Chicago_trains/chicago.RData?raw=true&quot;)) load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Chicago_trains/stations.RData?raw=true&quot;)) Create train_plot_data train_plot_data &lt;- training %&gt;% mutate(date = train_days) %&gt;% relocate(date, .before = everything()) train_plot_data ## # A tibble: 5,698 × 1,092 ## date s_40380 dow doy week month year Advent1st Advent2nd Advent…¹ ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2001-01-22 15.7 Mon 22 4 Jan 2001 0 0 0 ## 2 2001-01-23 15.8 Tue 23 4 Jan 2001 0 0 0 ## 3 2001-01-24 15.9 Wed 24 4 Jan 2001 0 0 0 ## 4 2001-01-25 15.9 Thu 25 4 Jan 2001 0 0 0 ## 5 2001-01-26 15.4 Fri 26 4 Jan 2001 0 0 0 ## 6 2001-01-27 2.42 Sat 27 4 Jan 2001 0 0 0 ## 7 2001-01-28 1.47 Sun 28 4 Jan 2001 0 0 0 ## 8 2001-01-29 15.5 Mon 29 5 Jan 2001 0 0 0 ## 9 2001-01-30 15.9 Tue 30 5 Jan 2001 0 0 0 ## 10 2001-01-31 15.9 Wed 31 5 Jan 2001 0 0 0 ## # … with 5,688 more rows, 1,082 more variables: Advent4th &lt;dbl&gt;, ## # AllSaints &lt;dbl&gt;, AllSouls &lt;dbl&gt;, Annunciation &lt;dbl&gt;, Ascension &lt;dbl&gt;, ## # AshWednesday &lt;dbl&gt;, AssumptionOfMary &lt;dbl&gt;, BirthOfVirginMary &lt;dbl&gt;, ## # BoxingDay &lt;dbl&gt;, CaRemembranceDay &lt;dbl&gt;, CelebrationOfHolyCross &lt;dbl&gt;, ## # ChristmasEve &lt;dbl&gt;, ChristTheKing &lt;dbl&gt;, CorpusChristi &lt;dbl&gt;, Easter &lt;dbl&gt;, ## # EasterMonday &lt;dbl&gt;, EasterSunday &lt;dbl&gt;, Epiphany &lt;dbl&gt;, ## # MassOfArchangels &lt;dbl&gt;, PalmSunday &lt;dbl&gt;, Pentecost &lt;dbl&gt;, … "],["preliminary-exploratory-visualizations.html", "4.4 Preliminary exploratory visualizations", " 4.4 Preliminary exploratory visualizations Ridership line plot by month g1 &lt;- train_plot_data %&gt;% select(date, rides = s_40380) %&gt;% mutate(date = floor_date(date, &quot;month&quot;)) %&gt;% arrange(date) %&gt;% group_by(date) %&gt;% summarise(rides = sum(rides), .groups = &#39;drop&#39;) %&gt;% ggplot(aes(date, rides)) + geom_line(size = 1) + geom_smooth(method = &#39;loess&#39;, se = FALSE, color = &#39;steelblue&#39;) + scale_x_date(date_labels = &quot;%b-%Y&quot;, date_breaks =&quot;2 year&quot;)+ labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Monthly Ridership Volume (Jan 2001 - Aug 2016)&#39; ) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) ggplotly(g1) ## `geom_smooth()` using formula &#39;y ~ x&#39; Boxplot rides by day of the week g2 &lt;- train_plot_data %&gt;% select(dow, rides = s_40380) %&gt;% ggplot(aes(dow, rides, fill = dow)) + geom_boxplot() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Ridership by Day of the Week&#39;) + theme(legend.position = &#39;none&#39;) ggplotly(g2) Violinplot rides by day of the week train_plot_data %&gt;% select(dow, rides = s_40380) %&gt;% ggplot(aes(dow, rides, fill = dow)) + geom_violin() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Ridership by Day of the Week&#39;) + theme(legend.position = &#39;none&#39;) Boxplot rides by month g3 &lt;- train_plot_data %&gt;% select(month, rides = s_40380) %&gt;% ggplot(aes(month, rides, fill = month)) + geom_boxplot() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Monthly Station Ridership&#39;) + theme(legend.position = &#39;none&#39;) ggplotly(g3) "],["visualizations-for-numeric-data.html", "4.5 Visualizations for Numeric Data", " 4.5 Visualizations for Numeric Data 4.5.1 Box Plots, Violin Plots, and Histograms Understanding the distribution of the response g4 &lt;- train_plot_data %&gt;% ggplot(aes(x = &quot;&quot;, y = s_40380)) + geom_boxplot(fill = &quot;blue&quot;, alpha = 0.5) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + theme( axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2, 29) ggplotly(g4) y_hist &lt;- ggplot(train_plot_data, aes(s_40380)) + geom_histogram(binwidth = .7, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;Clark/Lake Rides (x1000)&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(a)&quot;) + xlim(-2,29) + theme( axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) y_box &lt;- ggplot(train_plot_data, aes(x = &quot;&quot;, y = s_40380)) + geom_boxplot(alpha = 0.2) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + ggtitle(&quot;(b)&quot;) + theme( axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2,29) y_violin &lt;- ggplot(train_plot_data, aes(x = &quot;&quot;, y = s_40380)) + geom_violin(alpha = 0.2) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + ggtitle(&quot;(c)&quot;) + theme( axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2,29) y_hist / y_box / y_violin ## Warning: Removed 2 rows containing missing values (geom_bar). The histogram (a) allows us to see that there are two peaks or modes in ridership distribution indicating that there may be two mechanisms affecting ridership. The box plot (b) does not have the ability to see multiple peaks in the data. However the violin plot (c) provides a compact visualization that identifies the distributional nuance. 4.5.2 Augmenting Visualizations through Faceting, Colors, and Shapes Distribution of daily ridership at the Clark/Lake stop from 2001 to 2016 colored and faceted by weekday and weekend. l10_breaks &lt;- scales::trans_breaks(&quot;log10&quot;, function(x) 10^x) l10_labels &lt;- scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)) all_pred %&gt;% mutate(pow = as.factor(ifelse(dow %in% c(&quot;Sat&quot;,&quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;))) %&gt;% ggplot(aes(s_40380 * 1000, fill = pow, col = pow)) + facet_wrap( ~ pow, nrow = 2, scales = &quot;free_y&quot;) + geom_histogram(binwidth = .03, alpha = .5) + scale_fill_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + scale_x_log10(breaks = l10_breaks, labels = l10_labels) + xlab(&quot;Clark/Lakes Rides&quot;) + ylab(&quot;Frequency&quot;) + theme(legend.position=&quot;none&quot;) 4.5.3 Scatterplots Scatterplots are numeric-to-numeric visualizations. Example: A scatter plot of the 14-day lag ridership at the Clark/Lake station and the current-day ridership at the same station. # create &#39;pow&#39; (weekend/weekday flag) train_plot_data &lt;- train_plot_data %&gt;% mutate( pow = ifelse(dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;), pow = factor(pow) ) train_plot_data %&gt;% select(l14_40380, s_40380, pow) %&gt;% ggplot(aes(l14_40380,s_40380, col = pow)) + geom_point(alpha=0.5) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + xlab(&quot;Two-week Lag in Ridership (x1000)&quot;) + ylab(&quot;Current Day Ridership (x1000)&quot;) + theme(legend.title=element_blank()) + coord_equal() In general, there is a strong linear relationship between the 14-day lag and current-day ridership. However, there are many 14-day lag/current day pairs of days that lie far off from the overall scatter of points. 4.5.4 Scatterplots - Exclude U.S. holidays Let’s filter major U.S. holidays from the train_plot_data. commonHolidays &lt;- c(&quot;USNewYearsDay&quot;, &quot;Jan02_Mon_Fri&quot;, &quot;USMLKingsBirthday&quot;, &quot;USPresidentsDay&quot;, &quot;USMemorialDay&quot;, &quot;USIndependenceDay&quot;, &quot;Jul03_Mon_Fri&quot;, &quot;Jul05_Mon_Fri&quot;, &quot;USLaborDay&quot;, &quot;USThanksgivingDay&quot;, &quot;Day_after_Thx&quot;, &quot;ChristmasEve&quot;, &quot;USChristmasDay&quot;, &quot;Dec26_wkday&quot;, &quot;Dec31_Mon_Fri&quot;) any_holiday &lt;- train_plot_data %&gt;% dplyr::select(date, !!commonHolidays) %&gt;% gather(holiday, value, -date) %&gt;% group_by(date) %&gt;% summarize(common_holiday = max(value)) %&gt;% ungroup() %&gt;% mutate(common_holiday = ifelse(common_holiday == 1, &quot;Holiday&quot;, &quot;Non-holiday&quot;)) %&gt;% inner_join(train_plot_data, by = &quot;date&quot;) holiday_values &lt;- any_holiday %&gt;% dplyr::select(date, common_holiday) make_lag &lt;- function(x, lag = 14) { x$date &lt;- x$date + days(lag) prefix &lt;- ifelse(lag &lt; 10, paste0(&quot;0&quot;, lag), lag) prefix &lt;- paste0(&quot;l&quot;, prefix, &quot;_holiday&quot;) names(x) &lt;- gsub(&quot;common_holiday&quot;, prefix, names(x)) x } lag_hol &lt;- make_lag(holiday_values, lag = 14) holiday_data &lt;- any_holiday %&gt;% left_join(lag_hol, by = &quot;date&quot;) %&gt;% mutate( year = factor(year), l14_holiday = ifelse(is.na(l14_holiday), &quot;Non-holiday&quot;, l14_holiday) ) no_holiday_plot &lt;- holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot; &amp; l14_holiday == &quot;Non-holiday&quot;) %&gt;% ggplot(aes(l14_40380, s_40380, col = pow)) + geom_point(alpha=0.5) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + xlab(&quot;14-Day Lag&quot;) + ylab(&quot;Current Day&quot;) + theme(legend.title=element_blank())+ coord_equal() no_holiday_plot Filtering the holidays, the weekday scatterplot compactness improves (less outliers scattered on both sides of the cluster). 4.5.5 Heatmaps For the ridership data, we will create a month and day predictor, a year predictor, and an indicator of weekday ridership less than 10,000 rides. heatmap_data &lt;- all_pred %&gt;% mutate(pow = as.factor(ifelse(dow %in% c(&quot;Sat&quot;,&quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;))) %&gt;% dplyr::select(date, s_40380, pow) %&gt;% mutate( mmdd = format(as.Date(date), &quot;%m-%d&quot;), yyyy = format(as.Date(date), &quot;%Y&quot;), lt10 = ifelse(s_40380 &lt; 10 &amp; pow==&quot;Weekday&quot;, 1, 0) ) # U.S. holidays break_vals &lt;- c(&quot;01-01&quot;,&quot;01-15&quot;,&quot;02-01&quot;,&quot;02-15&quot;,&quot;03-01&quot;,&quot;03-15&quot;,&quot;04-01&quot;, &quot;04-15&quot;,&quot;05-01&quot;,&quot;05-15&quot;,&quot;06-01&quot;,&quot;06-15&quot;, &quot;07-01&quot;,&quot;07-15&quot;, &quot;08-01&quot;, &quot;08-15&quot;,&quot;09-01&quot;,&quot;09-15&quot;,&quot;10-01&quot;,&quot;10-15&quot;,&quot;11-01&quot;, &quot;11-15&quot;,&quot;12-01&quot;,&quot;12-15&quot;) heatmap_data %&gt;% ggplot(aes(yyyy, mmdd)) + geom_tile(aes(fill = lt10), colour = &quot;white&quot;) + scale_fill_gradient(low = &quot;transparent&quot;, high = &quot;red&quot;) + scale_y_discrete( breaks = break_vals ) + xlab(&quot;Year&quot;) + ylab(&quot;Month &amp; Day&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) This visualization indicates that the distinct patterns of low ridership on weekdays occur on and around major US holidays. 4.5.6 Correlation Matrix Plots Visualization of the correlation matrix of the 14-day lag ridership station predictors for non-holiday, weekdays in 2016. cor_mat &lt;- holiday_data %&gt;% dplyr::filter(year == &quot;2016&quot;) %&gt;% dplyr::select(matches(&quot;l14_[0-9]&quot;), pow, common_holiday) %&gt;% dplyr::filter(pow == &quot;Weekday&quot; &amp; common_holiday == &quot;Non-holiday&quot;) %&gt;% dplyr::select(-pow, -common_holiday) %&gt;% cor() cor_map &lt;- heatmaply_cor( cor_mat, symm = TRUE, cexRow = .0001, cexCol = .0001, branches_lwd = .1 ) cor_map Ridership across stations is positively correlated (red) for nearly all pairs of stations. This means that low ridership at one station corresponds to relatively low ridership at another station, and high ridership at one station corresponds to relatively high ridership at another station. For feature selection, the high degree of correlation is a clear indicator that the information present across the stations is redundant and could be eliminated or reduced. 4.5.7 Line plots Monthly average ridership per year by weekday (excluding holidays) or weekend. year_cols &lt;- colorRampPalette(colors = brewer.pal(n = 9, &quot;YlOrRd&quot;)[-1])(16) holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot;) %&gt;% dplyr::mutate(year = factor(year)) %&gt;% group_by( month = lubridate::month(date, label = TRUE, abbr = TRUE), year, pow ) %&gt;% dplyr::summarize(average_ridership = mean(s_40380, na.rm = TRUE)) %&gt;% ggplot(aes(month, average_ridership)) + facet_wrap( ~ pow, ncol = 2) + geom_line(aes(group = year, col = year), size = 1.1) + xlab(&quot;&quot;) + ylab(&quot;Geometric Mean Ridership (x1000)&quot;) + scale_color_manual(values = year_cols) + guides( col = guide_legend( title = &quot;&quot;, nrow = 2, byrow = TRUE ) ) + theme(legend.position = &quot;top&quot;) ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the ## `.groups` argument. Weekend ridership also shows annual trends but exhibits more variation within the trends for some years. The Weekend line plots have the highest variation during 2008, with much higher ridership in the summer. Monthly average gas price per gallon (USD) per year. holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot;) %&gt;% mutate(year = factor(year)) %&gt;% group_by( month = lubridate::month(date, label = TRUE, abbr = TRUE), year ) %&gt;% dplyr::summarize(average_l14_gas_price = mean(l14_gas_price, na.rm = TRUE)) %&gt;% ggplot(aes(x = month, y = average_l14_gas_price)) + geom_line(aes(group = year, col = year), size = 1.3) + xlab(&quot;&quot;) + ylab(&quot;Average Gas Price/Gallon ($)&quot;) + scale_color_manual(values = year_cols) + guides( col = guide_legend( title = &quot;&quot;, nrow = 2, byrow = TRUE ) ) + theme(legend.position = &quot;top&quot;) ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. Prices spike in the summer of 2008, which is at the same time that weekend ridership spikes. 4.5.8 Principal Component Analysis (PCA) Principal component analysis of the 14-day station lag ridership. lag_14_data &lt;- holiday_data %&gt;% dplyr::select(matches(&quot;l14_[0-9]&quot;)) PCA_station &lt;- prcomp(lag_14_data) var_explained &lt;- c(0, PCA_station$sdev ^ 2) cumulative_var &lt;- cumsum(var_explained) pct_var_explained &lt;- 100 * cumulative_var / max(cumulative_var) var_df &lt;- tibble( Component = seq_along(pct_var_explained) - 1, pct_var_explained = pct_var_explained ) score_data &lt;- tibble( y = holiday_data$s_40380, year = factor(holiday_data$year), pow = holiday_data$pow, PC1 = PCA_station$x[, 1], PC2 = PCA_station$x[, 2], dow = holiday_data$dow ) pca_rng &lt;- extendrange(c(score_data$PC1, score_data$PC2)) var_plot &lt;- var_df %&gt;% dplyr::filter(Component &lt;= 50) %&gt;% ggplot(aes(x = Component, y = pct_var_explained)) + geom_line(size = 1.3) + ylim(0, 100) + xlab(&quot;Component&quot;) + ylab(&quot;Percent Variance Explained&quot;) + ggtitle(&quot;(a)&quot;) score_plot12 &lt;- ggplot(score_data, aes(PC1,PC2)) + geom_point(size = 1, alpha = 0.25) + xlab(&quot;Component 1&quot;) + ylab(&quot;Component 2&quot;) + xlim(pca_rng) + ylim(pca_rng) + ggtitle(&quot;(b)&quot;) score1_vs_day &lt;- ggplot(score_data, aes(x = dow, y = PC1)) + geom_violin(adjust = 1.5) + ylab(&quot;Component 1&quot;) + xlab(&quot;&quot;) + ylim(pca_rng) + ggtitle(&quot;(c)&quot;) score2_vs_year &lt;- ggplot(score_data, aes(x = year, y = PC2, col = year)) + geom_violin(adjust = 1.5) + ylab(&quot;Component 2&quot;) + xlab(&quot;&quot;) + # ylim(pca_rng) + scale_color_manual(values = year_cols)+ theme(legend.position = &quot;none&quot;) + ggtitle(&quot;(d)&quot;) (var_plot + score_plot12) / score1_vs_day / score2_vs_year The cumulative variability summarized across the first 10 components (a). A scatter plot of the first two principal components. The first component focuses on variation due to part of the week while the second component focuses on variation due to time (year) (b). The relationship between the first principal component and ridership for each day of the week at the Clark/Lake station (c). The relationship between second principal component and ridership for each year at the Clark/Lake station (d). "],["visualizations-for-categorical-data-exploring-the-okcupid-dataset.html", "4.6 Visualizations for Categorical Data: Exploring the OKCupid dataset", " 4.6 Visualizations for Categorical Data: Exploring the OKCupid dataset OkCupid is an online dating site that serves international users. Kim and Escobedo-Land (2015) describe a data set where over 50,000 profiles from the San Francisco area. load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/OkCupid/okc.RData?raw=true&quot;)) First look at the dataset # bind &#39;okc_test&#39; okc &lt;- okc_train %&gt;% bind_rows(okc_test) Skim okc skimr::skim(okc) %&gt;% knitr::kable() skim_type skim_variable n_missing complete_rate factor.ordered factor.n_unique factor.top_counts numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist factor diet 0 1 FALSE 19 die: 19691, mos: 15229, any: 5440, str: 4583 NA NA NA NA NA NA NA NA factor drinks 0 1 FALSE 7 soc: 36756, rar: 5328, oft: 4531, not: 2868 NA NA NA NA NA NA NA NA factor drugs 0 1 FALSE 4 nev: 32805, dru: 11758, som: 6818, oft: 366 NA NA NA NA NA NA NA NA factor education 0 1 FALSE 33 gra: 21423, gra: 8151, wor: 5193, ed_: 3572 NA NA NA NA NA NA NA NA factor income 0 1 FALSE 13 mis: 40584, inc: 2876, inc: 1572, inc: 1080 NA NA NA NA NA NA NA NA factor offspring 0 1 FALSE 16 kid: 29360, doe: 6744, doe: 3653, doe: 3322 NA NA NA NA NA NA NA NA factor pets 0 1 FALSE 16 pet: 15181, lik: 13642, lik: 6534, lik: 3988 NA NA NA NA NA NA NA NA factor religion 0 1 FALSE 10 rel: 15333, agn: 8043, oth: 7231, ath: 6316 NA NA NA NA NA NA NA NA factor sign 0 1 FALSE 13 sig: 7818, leo: 3922, gem: 3911, can: 3784 NA NA NA NA NA NA NA NA factor smokes 0 1 FALSE 6 no: 38941, smo: 3619, som: 3268, whe: 2688 NA NA NA NA NA NA NA NA factor status 0 1 FALSE 5 sin: 48032, see: 1814, ava: 1624, mar: 269 NA NA NA NA NA NA NA NA factor where_state 0 1 FALSE 36 cal: 51672, new: 15, ill: 6, mas: 4 NA NA NA NA NA NA NA NA factor where_town 0 1 FALSE 51 san: 26683, oak: 6214, ber: 3616, san: 1168 NA NA NA NA NA NA NA NA factor religion_modifer 0 1 FALSE 5 rel: 25718, but: 11469, and: 8309, and: 4221 NA NA NA NA NA NA NA NA factor sign_modifer 0 1 FALSE 4 sig: 17909, and: 17709, but: 15511, and: 618 NA NA NA NA NA NA NA NA factor Class 0 1 FALSE 2 oth: 42190, ste: 9557 NA NA NA NA NA NA NA NA numeric age 0 1 NA NA NA 3.255509e+01 9.518919e+00 18 26.000000 30.000000 37.000000 109.000000 ▇▂▁▁▁ numeric height 0 1 NA NA NA 6.833121e+01 3.979818e+00 1 66.000000 68.000000 71.000000 95.000000 ▁▁▁▇▁ numeric last_online 0 1 NA NA NA 3.916536e+01 7.625808e+01 0 1.000000 4.000000 30.000000 370.000000 ▇▁▁▁▁ numeric cpp 0 1 NA NA NA 2.898700e-03 5.376220e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_fluently 0 1 NA NA NA 1.252250e-02 1.112020e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_okay 0 1 NA NA NA 9.855600e-03 9.878610e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_poorly 0 1 NA NA NA 7.382100e-03 8.560210e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp 0 1 NA NA NA 5.991000e-04 2.446880e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_fluently 0 1 NA NA NA 1.488000e-03 3.854640e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_okay 0 1 NA NA NA 2.319000e-03 4.810030e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_poorly 0 1 NA NA NA 2.280300e-03 4.769870e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric asian 0 1 NA NA NA 1.371094e-01 3.439661e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric black 0 1 NA NA NA 5.637040e-02 2.306379e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric hispanic_latin 0 1 NA NA NA 8.937720e-02 2.852901e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric indian 0 1 NA NA NA 2.467780e-02 1.551426e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric middle_eastern 0 1 NA NA NA 1.571110e-02 1.243564e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric native_american 0 1 NA NA NA 2.125730e-02 1.442422e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric other 0 1 NA NA NA 6.083440e-02 2.390287e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric pacific_islander 0 1 NA NA NA 2.475510e-02 1.553793e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric white 0 1 NA NA NA 6.474578e-01 4.777663e-01 0 0.000000 1.000000 1.000000 1.000000 ▅▁▁▁▇ numeric essay_length 0 1 NA NA NA 3.132866e+00 6.777757e-01 0 3.009026 3.269513 3.480438 4.983486 ▁▁▂▇▁ numeric profile 0 1 NA NA NA 2.587400e+04 1.493822e+04 1 12937.500000 25874.000000 38810.500000 51747.000000 ▇▇▇▇▇ Plot Class okc %&gt;% ggplot(aes(Class, fill = Class)) + geom_bar() + theme(legend.position = &#39;none&#39;) 4.6.1 Visualizing Relationships between Outcomes and Predictors 4.6.1.1 Outcome and Categorical Predictor Let’s plot the frequency of the stated religion, partitioned and color by the outcome (Class). binom_stats &lt;- function(x, ...) { x &lt;- x$Class[!is.na(x$Class)] res &lt;- prop.test(x = sum(x == &quot;stem&quot;), n = length(x), ...) data.frame(Proportion = unname(res$estimate), Lower = res$conf.int[1], Upper = res$conf.int[2]) } stem_rate &lt;- mean(okc_train$Class == &quot;stem&quot;) religion_rates &lt;- okc_train %&gt;% group_by(religion) %&gt;% do(binom_stats(.)) %&gt;% arrange(Proportion) %&gt;% ungroup() %&gt;% mutate(religion = gsub(&quot;religion_&quot;, &quot;&quot;, religion), religion = reorder(factor(religion), Proportion)) okc_train &lt;- okc_train %&gt;% mutate( religion2 = gsub(&quot;religion_&quot;, &quot;&quot;, as.character(religion)), religion2 = factor(religion2, levels = as.character(religion_rates$religion)) ) bars &lt;- ggplot(okc_train, aes(x = religion2, fill = Class)) + geom_bar(position = position_dodge()) + scale_fill_brewer(palette = &quot;Paired&quot;) + xlab(&quot;&quot;) + theme(legend.position = &quot;top&quot;, axis.text = element_text(size = 8)) + ggtitle(&quot;(a)&quot;) stacked_vars &lt;- ggplot(okc_train, aes(x = religion2, fill = Class)) + geom_bar(position = &quot;fill&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion&quot;) + theme(legend.position = &quot;none&quot;, axis.text = element_text(size = 8)) + ggtitle(&quot;(b)&quot;) ci_plots &lt;- ggplot(religion_rates, aes(x = religion, y = Proportion)) + geom_hline(yintercept = stem_rate, col = &quot;red&quot;, alpha = .35, lty = 2) + geom_point() + geom_errorbar(aes(ymin = Lower, ymax = Upper), width = .1) + theme(axis.text = element_text(size = 8)) + xlab(&quot;&quot;) + ggtitle(&quot;(c)&quot;) bars / stacked_vars / ci_plots Does religion appear to be related to the outcome? Since there is a gradation of rates of STEM professions between the groups, it would appear so. 4.6.1.2 Outcome and Numerical Predictor Now, let’s visualize the relationship between a categorical outcome (Class) and a numerical predictor (essay_length). l10_breaks &lt;- scales::trans_breaks(&quot;log10&quot;, function(x) 10^x) l10_labels &lt;- scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)) gam_dat &lt;- okc_train %&gt;% dplyr::select(essay_length, Class) %&gt;% arrange(essay_length) gam_small &lt;- gam_dat %&gt;% distinct(essay_length) gam_mod &lt;- mgcv::gam(Class ~ s(essay_length), data = gam_dat, family = binomial()) gam_small &lt;- gam_small %&gt;% mutate( link = -predict(gam_mod, gam_small, type = &quot;link&quot;), se = predict(gam_mod, gam_small, type = &quot;link&quot;, se.fit = TRUE)$se.fit, upper = link + qnorm(.975) * se, lower = link - qnorm(.975) * se, lower = binomial()$linkinv(lower), upper = binomial()$linkinv(upper), probability = binomial()$linkinv(link) ) brks &lt;- l10_breaks(exp(okc_train$essay_length)) essay_hist &lt;- ggplot(okc_train, aes(x = exp(essay_length))) + geom_histogram(binwidth = .1, col = &quot;#FEB24C&quot;, fill = &quot;#FED976&quot;) + facet_wrap(~ Class, ncol = 1) + scale_x_log10(breaks = brks, labels = l10_labels) + xlab(&quot;Essay Character Length&quot;) + theme_bw() + theme(plot.margin = unit(c(0,1,0,1.2), &quot;cm&quot;)) + ggtitle(&quot;(a)&quot;) essay_gam &lt;- ggplot(gam_small, aes(x = exp(essay_length))) + geom_line(aes(y = probability)) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;grey&quot;, alpha = .5) + geom_hline(yintercept = stem_rate, col = &quot;red&quot;, alpha = .35, lty = 2) + scale_x_log10(breaks = brks, labels = l10_labels) + theme_bw() + xlab(&quot;&quot;) + theme(plot.margin = unit(c(0,1,0,1.2), &quot;cm&quot;))+ ggtitle(&quot;(b)&quot;) essay_hist / essay_gam The black line represents the class probability of the logistic regression model and the bands denote 95% confidence intervals around the fit. The horizontal red line indicates the baseline probability of STEM profiles from the training set. This predictor might be worth including in a model but is unlikely to show a strong effect on its own. 4.6.2 Exploring Relationships Between Categorical Predictors When considering relationships between categorical data, there are several options. Once a cross-tabulation between variables is created, mosaic plots can once again be used to understand the relationship between variables. okc_train &lt;- okc_train %&gt;% mutate( drugs = factor(as.character(drugs), levels = c(&quot;drugs_missing&quot;, &quot;never&quot;, &quot;sometimes&quot;, &quot;often&quot;)), drinks = factor(as.character(drinks), levels = c(&quot;drinks_missing&quot;, &quot;not_at_all&quot;, &quot;rarely&quot;, &quot;socially&quot;, &quot;often&quot;, &quot;very_often&quot;, &quot;desperately&quot;)) ) dd_tab &lt;- table(okc_train$drugs, okc_train$drinks, dnn = c(&quot;Drugs&quot;, &quot;Alcohol&quot;)) # Formatting for slightly better printing plot_tab &lt;- dd_tab dimnames(plot_tab)[[1]][1] &lt;- &quot;missing&quot; dimnames(plot_tab)[[2]] &lt;- gsub(&quot;_&quot;, &quot; &quot;, dimnames(plot_tab)[[2]]) dimnames(plot_tab)[[2]][1] &lt;- &quot;missing&quot; dimnames(plot_tab)[[2]][6] &lt;- &quot;often\\n&quot; dimnames(plot_tab)[[2]][6] &lt;- &quot;very often&quot; dimnames(plot_tab)[[2]][7] &lt;- &quot;\\ndesperately&quot; mosaic( t(plot_tab), highlighting = TRUE, highlighting_fill = rainbow_hcl, margins = unit(c(6, 1, 1, 8), &quot;lines&quot;), labeling = labeling_border( rot_labels = c(90, 0, 0, 0), just_labels = c(&quot;left&quot;, &quot;right&quot;, &quot;center&quot;, &quot;right&quot;), offset_varnames = unit(c(3, 1, 1, 4), &quot;lines&quot;) ), keep_aspect_ratio = FALSE ) In the cross-tabulation between alcohol and drug use, the χ2 statistic is very large (4043.8) for its degrees of freedom (18) and is associated with a very small p-value (0). This indicates that there is a strong association between these two variables. ca_obj &lt;- CA(dd_tab, graph = FALSE) ca_drugs &lt;- as.data.frame(ca_obj$row$coord) ca_drugs$label &lt;- gsub(&quot;_&quot;, &quot; &quot;, rownames(ca_drugs)) ca_drugs$Variable &lt;- &quot;Drugs&quot; ca_drinks &lt;- as.data.frame(ca_obj$col$coord) ca_drinks$label &lt;- gsub(&quot;_&quot;, &quot; &quot;, rownames(ca_drinks)) ca_drinks$Variable &lt;- &quot;Alcohol&quot; ca_rng &lt;- extendrange(c(ca_drinks$`Dim 1`, ca_drinks$`Dim 2`)) ca_x &lt;- paste0(&quot;Dimension #1 (&quot;, round(ca_obj$eig[&quot;dim 1&quot;, &quot;percentage of variance&quot;], 0), &quot;%)&quot;) ca_y &lt;- paste0(&quot;Dimension #2 (&quot;, round(ca_obj$eig[&quot;dim 2&quot;, &quot;percentage of variance&quot;], 0), &quot;%)&quot;) ca_coord &lt;- rbind(ca_drugs, ca_drinks) ca_plot &lt;- ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_text(aes(label = label)) + xlim(ca_rng) + ylim(ca_rng) + xlab(ca_x) + ylab(ca_y) + coord_equal() ca_plot ## Warning: Removed 1 rows containing missing values (geom_text). The correspondence analysis principal coordinates for the drug and alcohol data in the OkCupid data. "],["post-modeling-exploratory-visualizations.html", "4.7 Post Modeling Exploratory Visualizations", " 4.7 Post Modeling Exploratory Visualizations Multiple linear regression has a rich set of diagnostics based on model residuals that aid in understanding the model fit and in identifying relationships that may be useful to include in the model. One tool from regression diagnosis that is helpful for identifying useful predictors is the partial regression plot (Neter et al. 1996). This plot utilizes residuals from two distinct linear regression models to unearth the potential usefulness of a predictor in a model. (refer to textbook for math formulas) For the Chicago data, a rolling forecast origin scheme (Section 3.4.4) was used for resampling. Figure 4.18 "],["residual-diagnostic-plots.html", "4.8 Residual Diagnostic Plots", " 4.8 Residual Diagnostic Plots The response for the regression model is the ridership at the Clark/Lake station, and our initial model will contain the predictors of week, month and year. As we saw earlier in this chapter, the distribution has two peaks, which we found were due to the part of the week (weekday versus weekend). To investigate the importance of part of the week we then regress the base predictors on part of the week and compute the hold-out residuals from this model. We can see that including part of the week in the model further reduces the residual distribution as illustrated in the histogram labeled Base + Part of Week. Next, let’s explore the importance of the 14-day lag of ridership at the Clark/Lake station. holidays &lt;- c(&quot;USNewYearsDay&quot;, &quot;Jan02_Mon_Fri&quot;, &quot;USMLKingsBirthday&quot;, &quot;USPresidentsDay&quot;, &quot;USMemorialDay&quot;, &quot;USIndependenceDay&quot;, &quot;Jul03_Mon_Fri&quot;, &quot;Jul05_Mon_Fri&quot;, &quot;USLaborDay&quot;, &quot;USThanksgivingDay&quot;, &quot;Day_after_Thx&quot;, &quot;ChristmasEve&quot;, &quot;USChristmasDay&quot;, &quot;Dec26_wkday&quot;, &quot;Dec31_Mon_Fri&quot;) common_holiday &lt;- apply(training %&gt;% dplyr::select(one_of(holidays)), 1, function(x) ifelse(any(x == 1), 1, 0)) training &lt;- training %&gt;% mutate( holiday = common_holiday, weekday = ifelse(dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;), 0, 1) ) # get_resid() get_resid &lt;- function(terms, next_term, return_mod = FALSE) { ctrl$verboseIter &lt;- FALSE ctrl$predictionBounds &lt;- c(0, NA) set.seed(4194) mod &lt;- train(s_40380 ~ ., data = training[, c(&quot;s_40380&quot;, terms)], method = &quot;lm&quot;, metric = &quot;RMSE&quot;, maximize = FALSE, trControl = ctrl) x_mod &lt;- train(as.formula(paste(next_term,&quot;~ .&quot;)), data = training[, c(terms, next_term)], method = &quot;lm&quot;, metric = &quot;RMSE&quot;, maximize = FALSE, trControl = ctrl) if(!return_mod) { out &lt;- mod$pred out$Resample &lt;- ymd(out$Resample) out$Date &lt;- train_days[out$rowIndex] out$Month &lt;- training$month[out$rowIndex] out$holiday &lt;- training$holiday[out$rowIndex] out$weekday &lt;- training$weekday[out$rowIndex] out$Residual &lt;- out$obs - out$pred out$xResidual &lt;- x_mod$pred$obs - x_mod$pred$pred } else out &lt;- mod out } # There will be a warning about the &quot;outcome only has two possible values&quot;. # This can be ignored. theme_set(theme_bw()) base_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;), next_term = &quot;weekday&quot;) %&gt;% mutate(Model = &quot;Base Model&quot;) ## Warning in train.default(x, y, weights = w, ...): You are trying to do ## regression and your outcome only has two possible values Are you trying to do ## classification? If so, use a 2 level factor as your outcome column. pow_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;), next_term = &quot;l14_40380&quot;) %&gt;% mutate(Model = &quot;Base + Part of Week&quot;) l14_resid &lt;- get_resid( terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;, &quot;l14_40380&quot;), next_term = &quot;holiday&quot; ) %&gt;% mutate(Model = &quot;Base + Part of Week + 14-Day Lag&quot;) ## Warning in train.default(x, y, weights = w, ...): You are trying to do ## regression and your outcome only has two possible values Are you trying to do ## classification? If so, use a 2 level factor as your outcome column. ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. hol_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;, &quot;l14_40380&quot;, &quot;holiday&quot;), next_term = &quot;l14_40370&quot;) %&gt;% mutate(Model = &quot;Base + Part of Week + 14-Day Lag + Holiday&quot;) mod_lev &lt;- c(&quot;Base Model&quot;, &quot;Base + Part of Week&quot;, &quot;Base + Part of Week + 14-Day Lag&quot;, &quot;Base + Part of Week + 14-Day Lag + Holiday&quot;) model_resid &lt;- bind_rows(base_resid, pow_resid, l14_resid, hol_resid) %&gt;% mutate( Model = factor(Model, levels = mod_lev), holiday = ifelse(holiday == 1, &quot;yes&quot;, &quot;no&quot;), weekday = ifelse(weekday == 0, &quot;Weekend&quot;, &quot;Weekday&quot;) ) resid_hists &lt;- ggplot(model_resid, aes(x = Residual)) + geom_vline(xintercept = 0, lty = 2, col = &quot;darkgreen&quot;) + geom_histogram(binwidth = 0.5, col = rgb(1, 1, 1, 0), fill = &quot;blue&quot;, alpha = .5) + facet_wrap(~Model, ncol = 1) + xlab(&quot;Resampling Residual&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;(a)&quot;) day_resid &lt;- base_resid %&gt;% mutate(weekday = ifelse(weekday == 0, &quot;Weekend&quot;, &quot;Weekday&quot;)) %&gt;% ggplot(aes(x = xResidual, y = Residual)) + geom_smooth(se = FALSE, method = lm, col = &quot;black&quot;) + geom_point(aes(col = weekday, shape = weekday), alpha = .5) + xlab(&quot;POW Model Residuals&quot;) + ylab(&quot;Base Model \\n Residuals \\n&quot;) + theme( legend.position = c(.2, .8), legend.background = element_blank(), legend.title = element_blank() ) + ggtitle(&quot;(b)&quot;) l14_PR_resid &lt;- ggplot(pow_resid, aes(x = xResidual, y = Residual)) + geom_point(alpha = .5) + xlab(&quot;14-day Lag Model Residuals&quot;) + ylab(&quot;Base + POW Model \\n Residuals \\n&quot;) + ggtitle(&quot;(c)&quot;) hol_PR_resid &lt;- l14_resid %&gt;% mutate(holiday = ifelse(holiday == 1, &quot;yes&quot;, &quot;no&quot;)) %&gt;% ggplot(aes(x = xResidual, y = Residual)) + geom_smooth(se = FALSE, method = lm, col = &quot;black&quot;) + geom_point(aes(col = holiday, shape = holiday), alpha = .5) + xlab(&quot;Holiday Model Residuals&quot;) + ylab(&quot;Base + POW + \\n 14-day Lag Model \\n Residuals&quot;) + theme( legend.position = c(.2, .25), legend.background = element_blank(), legend.title = element_blank() ) + ggtitle(&quot;(d)&quot;) resid_hists | (day_resid / l14_PR_resid / hol_PR_resid) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; The distribution of residuals from the model resampling process for the base model and the base model plus other potentially useful predictors for explaining ridership at the Clark/Lake station. (a) The partial regression plot for the effect of part of the week. (b) The partial regression plot for the 14-day lag predictor of the Clark/Lake station. (c) The partial regression plot for holiday classification. (d) "],["meeting-videos-2.html", "4.9 Meeting Videos", " 4.9 Meeting Videos 4.9.1 Cohort 1 Meeting chat log LOG "],["encoding-categorical-predictors.html", "Chapter 5 Encoding Categorical Predictors", " Chapter 5 Encoding Categorical Predictors Categorical (also called nominal) predictors are those that contain qualitative data. Examples include: Education level ZIP code Text Day of the week Color A large majority of models require that all predictors be numeric. A summary of parsnip model preprocessors from: Tidy Modeling with R by Max Kuhn and Julia Silge knitr::opts_chunk$set(fig.path = &quot;images/&quot;) suppressPackageStartupMessages({ library(tidyverse) library(tidymodels) library(embed) library(cli) library(kableExtra) }) Table 5.1: Preprocessing methods for different models. model dummy zv impute decorrelate normalize transform bag_mars() ✔ × ✔ ◌ × ◌ bag_tree() × × × ◌¹ × × bart() × × × ◌¹ × × boost_tree() ×² ◌ ✔² ◌¹ × × C5_rules() × × × × × × cubist_rules() × × × × × × decision_tree() × × × ◌¹ × × discrim_flexible() ✔ × ✔ ✔ × ◌ discrim_linear() ✔ ✔ ✔ ✔ × ◌ discrim_regularized() ✔ ✔ ✔ ✔ × ◌ gen_additive_mod() ✔ ✔ ✔ ✔ × ◌ linear_reg() ✔ ✔ ✔ ✔ × ◌ logistic_reg() ✔ ✔ ✔ ✔ × ◌ mars() ✔ × ✔ ◌ × ◌ mlp() ✔ ✔ ✔ ✔ ✔ ✔ multinom_reg() ✔ ✔ ✔ ✔ ×² ◌ naive_Bayes() × ✔ ✔ ◌¹ × × nearest_neighbor() ✔ ✔ ✔ ◌ ✔ ✔ pls() ✔ ✔ ✔ × ✔ ✔ poisson_reg() ✔ ✔ ✔ ✔ × ◌ rand_forest() × ◌ ✔² ◌¹ × × rule_fit() ✔ × ✔ ◌¹ ✔ × svm_*() ✔ ✔ ✔ ✔ ✔ ✔ In the table, ✔ indicates that the method is required for the model and × indicates that it is not. The ◌ symbol means that the model may be helped by the technique but it is not required. Algorithms for tree-based models naturally handle splitting both numeric and categorical predictors. These algorithms employ a series if/then statements that sequentially split the data into groups. A naive Bayes model will create a cross-tabulation between a categorical predictor and the outcome class. We will return to this point in the final section of this chapter. Simple categorical variables can also be classified as ordered or unordered. "],["creating-dummy-variables-for-unordered-categories.html", "5.1 Creating Dummy Variables for Unordered Categories", " 5.1 Creating Dummy Variables for Unordered Categories There are many methods for doing this and, to illustrate, consider a simple example for the day of the week. If we take the seven possible values and convert them into binary dummy variables, the mathematical function required to make the translation is often referred to as a contrast. These six numeric predictors would take the place of the original categorical variable. Why only six? if the values of the six dummy variables are known, then the seventh can be directly inferred. When the model has an intercept, an additional initial column of ones for all rows is included. Estimating the parameters for a linear model (as well as other similar models) involve inverting the matrix. If the model includes an intercept and contains dummy variables for all seven days, then the seven day columns would add up (row-wise) to the intercept and this linear combination would prevent the matrix inverse from being computed (as it is singular). Less than full rank encodings are sometimes called “one-hot” encodings. Generating the full set of indicator variables may be advantageous for some models that are insensitive to linear dependencies (an example: glmnet) What is the interpretation of the dummy variables? Consider a linear model for the Chicago transit data that only uses the day of the week. Using the training set to fit the model, the intercept value estimates the mean of the reference cell, which is the average number of Sunday riders in the training set, estimated to be 3.84K people. The second model parameter, for Monday, is estimated to be 12.61K. In the reference cell model, the dummy variables represent the mean value above and beyond the reference cell mean. In this case, estimate indicates that there were 12.61K more riders on Monday than Sunday. train_df &lt;- tibble(m = month.abb, number = seq(1,12, by = 1)) recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing without One HOT (the default) contrasts with April&quot; ) |&gt; row_spec(row = 4, background = &quot;orange&quot;) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing without One HOT (the default) contrasts with April m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing with One HOT.&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing with One HOT. m_Apr m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 "],["encoding-predictors-for-many-categories.html", "5.2 Encoding Predictors for Many Categories", " 5.2 Encoding Predictors for Many Categories What happens when the number of factor levels gets very large? For example, there are more than 40K possible ZIP codes and, depending on how the data are collected, this might produce an overabundance of dummy variables for the size of the data available. Also, ZIP codes in highly populated areas may have a higher rate of occurrence in the data, leading to a “long tail” of locations that are infrequently observed. Also, resampling will exclude some of the rarer categories from the analysis set. The first way to handle this issue is to create the full set of dummy variables and simply remove the zero and low-variance predictors. Still, we may not desire to filter these out. Another approach is to feature engineer an “other” category that pools the rarely occurring categories, assuming that such a pooling is sensible. Another way to combine categories is to use a hashing function that maps each factor level key to a hash value. The number of possible hashes is set by the user and, for numerical purposes, is a power of 2. Some computationally interesting aspects to hash functions are The only data required is the value being hashed and the resulting number of hashes. The translation process is completely deterministic. Hash functions are unidirectional; once the hash values are created, there is no way of knowing the original values. If there are a known and finite set of original values, a table can be created to do the translation but, otherwise, the keys are indeterminable when only the hash value is known. There is no free lunch when using this procedure; some of the original categories will be mapped to the same hash value (called a “collision”). The number of collisions will be largely determined by the number of features that are produced. Categories involved in collisions are not related in any meaningful way. Because of the arbitrary nature of the collisions, it is possible to have different categories whose true underlying effect are counter to one another. This might have the effect of negating the impact of the hashed feature. Hashing functions have no notion of the probability that each key will occur. As such, it is conceivable that a category that occurs with great frequency is aliased with one that is rare. In this case, the more abundant value will have a much larger influence on the effect of that hashing feature. "],["approaches-for-novel-categories.html", "5.3 Approaches for Novel Categories", " 5.3 Approaches for Novel Categories Suppose that a model is built to predict the probability that an individual works in a STEM profession and that this model depends on city names. The model will be able to predict the probability of STEM profession if a new individual lives in one of the cities in the training set. But what happens to the model prediction when a new individual lives in a city that is not represented? One strategy would be to use the previously mentioned “other” category to capture new values. This approach can also be used with feature hashing. "],["supervised-encoding-methods.html", "5.4 Supervised Encoding Methods", " 5.4 Supervised Encoding Methods Beyond dummies, there are many other ways to craft one or more numerical features from a set of nominal predictors. They include 5.4.1 Likelihood Encoding In essence, the effect of the factor level on the outcome is measured and this effect is used as new numeric features. For example, for the Ames housing data, we might calculate the mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model. For classification problems, a simple logistic regression model can be used to measure the effect between the categorical outcome and the categorical predictor. If the outcome event occurs with rate \\[ p \\], the odds of that event is defined as \\[ p / ( 1 − p) \\]. This is an example of a single generalized linear model applied to the hair color feature, which woudl otherwise have 12 dummy levels. as_tibble(dplyr::starwars) |&gt; count(hair_color) ## # A tibble: 13 × 2 ## hair_color n ## &lt;chr&gt; &lt;int&gt; ## 1 auburn 1 ## 2 auburn, grey 1 ## 3 auburn, white 1 ## 4 black 13 ## 5 blond 3 ## 6 blonde 1 ## 7 brown 18 ## 8 brown, grey 1 ## 9 grey 1 ## 10 none 37 ## 11 unknown 1 ## 12 white 4 ## 13 &lt;NA&gt; 5 recipe(skin_color ~ hair_color + eye_color + mass, data = as_tibble(dplyr::starwars)) |&gt; embed::step_lencode_glm(hair_color, outcome = &quot;skin_color&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) |&gt; kable( caption = &quot;Starwars Characters hair_color GLM embedding&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) (#tab:chapter 5 glm numeric embeddings)Starwars Characters hair_color GLM embedding hair_color eye_color mass skin_color -2.862201 red 113 green -21.566069 yellow 55 fair, green, yellow -21.566069 blue 89 fair -21.566069 brown 79 tan -21.566069 brown 84 light -2.862201 black NA white, blue -21.566069 brown NA tan -21.566069 blue NA fair -2.862201 red 140 metal -21.566069 brown NA light While very fast, this method has drawbacks. For example, what happens when a factor level has a single value? Theoretically, the log-odds should be infinite in the appropriate direction but, numerically, it is usually capped at a large (and inaccurate) value. One way around this issue is to use some type of shrinkage method. For example, the overall log-odds can be determined and, if the quality of the data within a factor level is poor, then this level’s effect estimate can be biased towards an overall estimate that disregards the levels of the predictor. A common method for shrinking parameter estimates is Bayesian analysis. (one doubt – step_lencode_bayes appears to only work with two class outcomes ???) as_tibble(datasets::Titanic) |&gt; count(Class) recipe(Survived ~ Class + Sex + Age, data = as_tibble(datasets::Titanic)) |&gt; embed::step_lencode_bayes(Class, outcome = &quot;Survived&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) # A tibble: 10 × 4 Class Sex Age Survived &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 -0.0108 Female Adult Yes 2 -0.0108 Male Adult No 3 -0.00526 Male Child Yes 4 -0.00526 Female Child No 5 -0.00993 Male Adult No 6 -0.0104 Male Child Yes 7 -0.00526 Male Child No 8 -0.0108 Female Adult No 9 -0.00993 Male Child Yes 10 -0.0104 Female Child No Empirical Bayes methods can also be used, in the form of linear (and generalized linear) mixed models. One issue with effect encoding, independent of the estimation method, is that it increases the possibility of overfitting the training data. Use resampling. Another supervised approach comes from the deep learning literature on the analysis of textual data. In addition to the dimension reduction, there is the possibility that these methods can estimate semantic relationships between words so that words with similar themes (e.g., “dog”, “pet”, etc.) have similar values in the new encodings. This technique is not limited to text data and can be used to encode any type of qualitative variable. An example using The Office dialogue and one of the pre-trained GloVe embeddings. library(textrecipes) library(schrute) glove6b &lt;- textdata::embedding_glove6b(dimensions = 100) # the download is 822.2 Mb schrute::theoffice |&gt; slice_sample(n = 10) |&gt; select(character, text) recipe(character ~ text, data = schrute::theoffice) |&gt; step_tokenize(text, options = list(strip_punct = TRUE)) |&gt; step_stem(text) |&gt; step_word_embeddings(text, embeddings = glove6b) |&gt; prep() |&gt; bake(new_data = schrute::theoffice |&gt; slice_sample(n = 10)) The Office dialogue word embeddings with glove6b # A tibble: 10 × 101 character wordembe…¹ worde…² worde…³ worde…⁴ worde…⁵ worde…⁶ &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Angela -1.02 -0.377 0.797 -1.21 -0.802 0.656 2 Roy 0 0 0 0 0 0 3 Phyllis -1.20 -0.373 3.20 -1.60 -1.62 -0.160 4 Kevin -1.54 1.77 5.02 -4.68 -5.23 2.91 5 Roy -2.15 0.735 4.07 -2.20 -1.30 0.297 6 Jim -0.595 0.419 0.699 -0.328 -1.20 1.70 7 Kelly -2.17 4.38 5.97 -4.91 -4.21 4.13 8 Katy -0.891 -0.889 0.0937 -0.859 1.42 1.49 9 Kevin -0.0308 0.120 0.539 -0.437 -0.739 -0.153 10 Jim -0.395 0.240 1.14 -1.27 -1.47 1.39 # … with 94 more variables: wordembed_text_d7 &lt;dbl&gt;, # wordembed_text_d8 &lt;dbl&gt;, wordembed_text_d9 &lt;dbl&gt;, # wordembed_text_d10 &lt;dbl&gt;, wordembed_text_d11 &lt;dbl&gt;, # wordembed_text_d12 &lt;dbl&gt;, wordembed_text_d13 &lt;dbl&gt;, # wordembed_text_d14 &lt;dbl&gt;, wordembed_text_d15 &lt;dbl&gt;, # wordembed_text_d16 &lt;dbl&gt;, wordembed_text_d17 &lt;dbl&gt;, # wordembed_text_d18 &lt;dbl&gt;, wordembed_text_d19 &lt;dbl&gt;, … Note that in place of thousands of sparse dummy colums for each tokenized word, the training set consists of 100 numeric feature dimensions. See also Textrecipes series: Pretrained Word Embedding by Emil Hvitfeldt "],["encodings-for-ordered-data.html", "5.5 Encodings for Ordered Data", " 5.5 Encodings for Ordered Data Suppose that the factors have a relative ordering, like low, medium, and high. R uses a technique called polynomial contrasts to numerically characterize the relationships. values &lt;- c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) dat &lt;- data.frame(x = ordered(values, levels = values)) # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table model.matrix(~ x, dat) ## (Intercept) x.L x.Q ## 1 1 -7.071068e-01 0.4082483 ## 2 1 -7.850462e-17 -0.8164966 ## 3 1 7.071068e-01 0.4082483 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.poly&quot; # using recipes ---------------------------------------------------------------- # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table recipe(~ x, data = dat) |&gt; step_dummy(x) |&gt; prep() |&gt; bake(new_data = NULL) ## # A tibble: 3 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -7.07e- 1 0.408 ## 2 -7.85e-17 -0.816 ## 3 7.07e- 1 0.408 It is important to recognize that patterns described by polynomial contrasts may not effectively relate a predictor to the response. For example, in some cases, one might expect a trend where “low” and “middle” samples have a roughly equivalent response but “high” samples have a much different response. In this case, polynomial contrasts are unlikely to be effective at modeling this trend. Other alternatives to polynomial contrasts: Leave the predictors as unordered factors. Translate the ordered categories into a single set of numeric scores based on context-specific information. Simple visualizations and context-specific expertise can be used to understand whether either of these approaches are good ideas. "],["creating-features-for-text-data.html", "5.6 Creating Features for Text Data", " 5.6 Creating Features for Text Data Often, data contain textual fields that are gathered from questionnaires, articles, reviews, tweets, and other sources. Are there words or phrases that would make good predictors of the outcome? To determine this, the text data must first be processed and cleaned. One approach is to measure for “importance”, that is, keywords with odds-ratios of at least 2 (in either direction) to be considered for modeling. See also Supervised Machine Learning for Text Analysis in R for a much better explanation. Other methods for preprocessing text data include: removing commonly used stop words, such as “is”, “the”, “and”, etc. stemming the words so that similar words, such as the singular and plural versions, are represented as a single entity. filter for the most common tokens, and then calculate the term frequency-inverse document frequency (tf-idf) statistic for each token "],["factors-versus-dummy-variables-in-tree-based-models.html", "5.7 Factors versus Dummy Variables in Tree-Based Models", " 5.7 Factors versus Dummy Variables in Tree-Based Models Certain types of models have the ability to use categorical data in its natural form. For example, a Chicago ridership decision tree could split on if day in {Sun, Sat} then ridership = 4.4K else ridership = 17.3K Suppose the day of the week had been converted to dummy variables. What would have occurred? In this case, the model is slightly more complex since it can only create rules as a function of a single dummy variable at a time: if day = Sun then ridership = 3.84K else if day = Sat then ridership = 4.96K else ridership = 17.30K So, for decision trees and naiive bayes does it matter how the categorical features are encoded? To answer this question, a series of experiments was conducted. The results: For these data sets, there is no real difference in the area under the ROC curve between the encoding methods. In terms of performance, it appears that differences between the two encodings are rare (but can occur). One other statistic was computed for each of the simulations: the time to train the models. Here, there is very strong trend that factor-based models are more efficiently trained than their dummy variable counterparts. One other effect of how qualitative predictors are encoded is related to summary measures. Many of these techniques, especially tree-based models, calculate variable importance scores that are relative measures for how much a predictor affected the outcome. For example, trees measure the effect of a specific split on the improvement in model performance (e.g., impurity, residual error, etc.). As predictors are used in splits, these improvements are aggregated; these can be used as the importance scores. "],["summary.html", "5.8 Summary", " 5.8 Summary With the exception of tree-based models, categorical predictors must first be converted to numeric representations to enable other models to use the information. The simplest feature engineering technique is to convert each category to a separate binary dummy predictor. Some models require one fewer dummy predictors than the number of categories. Creating dummy predictors may not be the most effective way. If, for instance, the predictor has ordered categories, then polynomial contrasts may be better. Text fields, too, can be viewed as an agglomeration of categorical predictors and must be converted to numerics. "],["meeting-videos-3.html", "5.9 Meeting Videos", " 5.9 Meeting Videos 5.9.1 Cohort 1 No chat for this session LOG "],["engineering-numeric-predictors.html", "Chapter 6 Engineering Numeric Predictors", " Chapter 6 Engineering Numeric Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "6.1 SLIDE 1", " 6.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-4.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log LOG "],["detecting-interaction-effects.html", "Chapter 7 Detecting Interaction Effects", " Chapter 7 Detecting Interaction Effects Learning objectives: how predictors relate to the outcome when to apply for interaction effects how to make predictions selection for looking to key interactions "],["introduction-2.html", "7.1 Introduction", " 7.1 Introduction In this chapter we will be looking at the interaction effects caused by predictors acting together on the response variable. …additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other. As an example consided are the effects of water and fertilizer on the yield of a field corn crop. “With no water but some fertilizer, the crop of field corn will produce no yield since water is a necessary requirement for plant growth. Conversely, with a sufficient amount of water but no fertilizer, a crop of field corn will produce some yield. However, yield is best optimized with a sufficient amount of water and a sufficient amount of fertilizer. Hence water and fertilizer, when combined in the right amounts, produce a yield that is greater than what either would produce alone.” predictors are said to interact if their combined effect is different (less or greater) than what we would expect if we were to add the impact of each of their effects when considered alone. Correlations between predictors, for example, are not directly related to whether there is an interaction effect or not The individual variables (e.g., fertilizer and water) are referred to as the main effect terms when outside of an interaction. "],["four-type-of-interactions.html", "7.2 Four type of Interactions", " 7.2 Four type of Interactions additive is when \\(\\beta_3 \\approx{0}\\) antagonistic is when \\(\\beta_3 &lt; 0\\) synergistic is when \\(\\beta_3 &gt; 0\\) atypical is when \\(\\beta_3 \\neq 0\\) The main difference is that in the atypical interaction one of the two predictors doesn’t affect the response. see full code in the scripts folder: 1_geom_contour.R library(tidyverse) # simulated data set.seed(123) beta0&lt;- rep(0,200) beta1&lt;- rep(1,200) beta2&lt;- rep(1,200) x1&lt;- runif(200,min = 0, max = 1) x2 &lt;- runif(200,min = 0, max = 1) e &lt;- rnorm(200) ################################################## # synergism beta3&lt;- rep(10,200) # c(-10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~x1*x2,observed) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p1 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + #geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Synergistic&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) ################################################## # no interaction beta3 &lt;- rep(0,200) # c(10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~x1*x2,observed) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p2 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + # geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Additive&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) ################################################## # antagonism beta3&lt;- rep(-10,200) # c(-10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~ x1 * x2 , data = observed) # rnd effects (1 + x1 | x2) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p3 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + # geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Antagonistic&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) Visualizing interaction effects for the Ames data: see full code in the scripts folder: 2_manipulate.R The general model function for interaction effects: \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] Which predictors interact? interaction can be uncovered by more complex modeling techniques tree-based models random forests boosted tree model search techniques and svm Feature engineering helps improving the effectiveness of a models by featuring selection of predictors, so as a consequence simplify the detection of interaction effects. 7.2.1 Building the base-model for Ames data Here are the predictors divided by type, we will be looking at different ways to make a selection of the predictors and what are the best interactions for this data, which will be influencing model preformance. library(AmesHousing) ames &lt;- make_ames() %&gt;% janitor::clean_names() ames1 &lt;- ames %&gt;% # names%&gt;%sort select(# continuous gr_liv_area,lot_area, lot_frontage,year_built, year_sold,pool_area,longitude, latitude,full_bath, # qualitative neighborhood,bldg_type, central_air,ms_sub_class, foundation,roof_style,alley, garage_type,land_contour) "],["guiding-principles-in-the-search-for-interactions.html", "7.3 Guiding Principles in the Search for Interactions", " 7.3 Guiding Principles in the Search for Interactions Statistical experimental design to establish casual relationships between independent and dependent variables, foresees: control randomization replication Interactions can be of different degrees: The identification of the interactions can be challenging, and even more challenging can be the identification of the shepherd interaction effects. The framework for identifying significant interactions (Wu ans Hamada 2011) for experimental design and predictive modeling is based on: interaction hierarchy (degree of interaction) effect sparsity (only a fraction of the interaction effects can be effective) effect heredity (implies significant factors preceding interaction explain the most of the response) strong heredity (interaction only with significant preceeding factors) weak heredity (any interaction with one significant factor) High order interaction happen in real life data (interactions among species). "],["practical-considerations.html", "7.4 Practical Considerations", " 7.4 Practical Considerations Is it possible to identify all possible predictive interactions? Is it possible to evaluate all possible interactions? Should the interaction terms be created before or after the preprocessing part? Only a fraction of all possible pairwise interactions contain relevant information. With \\(p\\) predictors we have \\((p)(p-1)/2\\) pairwise interaction terms. Here is the difference in interaction effects before and after preprocessing: "],["the-brute-force-approach-to-identifying-predictive-interactions.html", "7.5 The Brute-Force Approach to Identifying Predictive Interactions", " 7.5 The Brute-Force Approach to Identifying Predictive Interactions False discoveries can influence model performance 7.5.1 Simple Screening Base-line approach is to evaluate the performance with nested statistical models: \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\text{error}\\] \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] see full code in the scripts folder: 3_comparisons_nested_models.R Objective function: for linear regression is the statistical likelihood (residual error) for logistic regression is the binomial likelihood Evaluation methods: The residual error (stat. likelihood) is compared and the hypothesis test evaluated with the p-value level to find differences between the results of estimations with and without interaction. If significant differences are found, p-value &lt; 0.05, there is less than 5% chance that the results are due to randomness. This is the case for false discoveries. Resampling and assessment evaluation. Use of metrics for visualizing the model performance: ROC, AUC, sensitivity, specificity, accuracy Methods for controlling false discoveries: Bonferroni correction (exponential penalty) False discovery Rate (FDR) see the code: Bonferroni and FDR adj For example, in case of the Ames data, using resampling and choosing the potential interaction with the smallest p-value, latitude and longitude appear to be interesting interaction factors, but this would require more investigations, to understand if this interaction is significant. The next step would be to compare the nested models with the ANOVA method. see an example: “comparisons_nested_models.R” 7.5.2 Penalized Regression One-at-a-time fashion evaluation of interaction terms, creates interaction terms to be added in the dataset. This method increases the number of predictors. Models to use when there are more predictors than observations: trees svm neural networks k-nearest neighbors penalized models (less interpretable, but allow for linear/logistic regression) How do we start with evaluating regression models? Minimize sum of squared errors \\[SSE=\\sum_{i=1}^n{(y_i-\\hat{y_i})^2}\\] \\[\\hat{y_i}=\\hat{\\beta_1}x_1+\\hat{\\beta_2}x_2+...+\\hat{\\beta_p}x_p\\] In case of penalized models: Ridge regression: \\(\\lambda_r\\) is called a penalty. To achieve better results, as regression coefficients grow large, the level of the penalty should rise. The penalty causes the resulting regression coefficients to become smaller and shrink towards zero. For combating collinearity. \\[SSE=\\sum{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda_r\\sum_{j=1}^P{\\beta_j^2}\\] Lasso: the least absolute shrinking, a modification to the ridge optimization criteria for the selection of predictors. \\[SSE=\\sum{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda_l\\sum_{j=1}^P{|\\beta_j|}\\] Approaches for blending both types of penalties together: glmnet model \\[\\lambda=\\lambda_r+\\lambda_l\\] where is \\(\\alpha\\) is the proportion of \\(\\lambda\\) associated with lasso penalty: full lasso: \\(\\alpha=1\\) mix: \\(\\alpha=0.5\\) full ridge: \\(\\alpha=0\\) \\[SSE=\\sum_{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda [(1-\\alpha)\\sum_{j=1}^P{\\beta_j^2+\\alpha\\sum_{j=1}^P{|\\beta_j|}]}\\] 7.5.3 Practical example with Ames data and glmnet see full code in the scripts folder: 5_ames_modeling.R "],["approaches-when-complete-enumeration-is-practically-impossible.html", "7.6 Approaches when Complete Enumeration is Practically Impossible", " 7.6 Approaches when Complete Enumeration is Practically Impossible 7.6.1 Guiding Principles and Two-stage Modeling Two-stage Modeling is another approach to use: use simple models such as lm or glm then add interaction effects use models ready for considering interactions In the residuals are the predictors missing information. 7.6.1.1 Example Observed data are \\(y\\) and \\(x_1\\), we know nothing about \\(x_2\\), and so \\(x_1*x_2\\). \\[y=x_1+x_2+10x_1x_2+\\text{error}\\] \\[y=\\beta_1x_1+x_2+\\text{error*}\\] \\[\\text{error*}=\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] Random measurement error remains unexplained. hierarchy principle: first look at pairwise interactions sparsity principle: look for active interactions heredity principle: search for interactions among the predictors identified in the first stage choose the type of heredity: determine the number of interaction terms For classification outcome (categorical response) the Pearson residual should be used: \\[\\frac{y_i-p_i}{\\sqrt{p_i(1-p_i)}}\\] 7.6.2 Tree-based Methods “Tree-based models are usually thought of as pure interaction due to their prediction equation, which can be written as a set of multiplicative statements.” tree-based methods uncover potential interactions between variables recursive partitioning identifies important interactions among predictors “In essence, the partitioning aspect of trees impedes their ability to represent smooth, global interactions.” Moreover, a tree-based model does well at approximating the level of interaction but, it breaks the space into rectangular regions, and needs many regions to capture all possible interactions. For this reason, ensembles of trees such as bagging (many samples of the original data generated with replacement) and boosting (sequence of trees with restricted depth, improved performance and weighted stats) are better performers. Here is a representation of the clusters for Ames data: Here is the outcome of a basic decision tree for the Ischemic Stroke data: see full code in the scripts folder: 6_ames_hclust.R Tree ensembles are so effective at identifying predictor-response relationships because they are aggregating many trees from slightly different versions of the original data. An example is Random Forest which is a variation of bagging. Partial dependency compares the joint effect of two (or more) predictors with the individual effect of each predictor in a model. This comparison is named H statistic, which is 0 if no interaction is found and &gt;0 otherwise. More about the H statistic can be found in Friedman and Popescu 2008. see full code in the scripts folder: 7_ames_H_stat.R see the code: Ames trees example with H statistic Out-of-bag (OOB) samples: Random forest uses bootstrap samples to create many models to be used in the ensemble. Since the bootstrap is being used, each tree has an associated assessment set (historically called the out-of-bag (OOB) samples) that we not used to fit the model. To be mentioned is this interesting package: pre package 7.6.3 The Feasible Solution Algorithm A preselction is done before searching for interaction among predictors. When linear and logistic models are used some predictor selection methods are applied: Forward selection: It starts with no predictors and select the best one, then select the second best and so on… Backward selection: It starts with all predictors and make a selection based on least contribution on optimization. Stepwise selection: It adds and removes predictors at a time at each step based on optimization criteria, until model’s results negatively impact model performance. FSA: The feasible solution algorithm (Miller’s approach extended by Hawkins) to find the optimal subset. Miller’s approach with 10 predictors, randomly select 3 of the predictors among all the others. It makes a model selecting one of the three predictors. Then the other two are modeled against all the remaining ones. If any of the new added predictors gives a better result, the first predictor is chosen. Then more swapping is performed among the first three predictors against all the others, untile the best one is found, or until it converges. Hawkins’ extension: q: random starts m: terms in the subset p: predictors The space is \\(q\\text{ x }m\\text{ x }p\\), in general the space is \\(p^m\\). Lambert’s approach add the interaction selection to the FSA algorithm. See the code for FSA Application to Ames data "],["other-potentially-useful-tools.html", "7.7 Other Potentially Useful Tools", " 7.7 Other Potentially Useful Tools Multivariate adaptive regression splines (MARS) is a nonlinear modeling technique for a continuous response MARS has also been extended to classification outcomes, and this method is called flexible discriminant analysis (FDA). Cubist (Kuhn and Johnson 2013) is a rule-based regression model that builds an initial tree and decomposes it into set of rules that are pruned and perhaps eliminated. "],["conclusion.html", "7.8 Conclusion", " 7.8 Conclusion It is challeging to detect all the real interacting effects on a data set with many predictors. A good advice from the expert in the field of data could be key to identify predictors that are force of interaction for that specific data. In addition, the application of pairwise interaction selection is still to be applied for looking at possible changes that might apply to data overtime. "],["meeting-videos-5.html", "7.9 Meeting Videos", " 7.9 Meeting Videos 7.9.1 Cohort 1 Meeting chat log LOG "],["handling-missing-data.html", "Chapter 8 Handling Missing Data", " Chapter 8 Handling Missing Data Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-2.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-6.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["working-with-profile-data.html", "Chapter 9 Working with Profile Data", " Chapter 9 Working with Profile Data Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "9.1 SLIDE 1", " 9.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["feature-selection-overview.html", "Chapter 10 Feature Selection Overview", " Chapter 10 Feature Selection Overview Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["greedy-search-methods.html", "Chapter 11 Greedy Search Methods", " Chapter 11 Greedy Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["global-search-methods.html", "Chapter 12 Global Search Methods", " Chapter 12 Global Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-6.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
