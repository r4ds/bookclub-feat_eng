[["index.html", "Feature Engineering and Selection Book Club Welcome", " Feature Engineering and Selection Book Club The R4DS Online Learning Community 2022-09-18 Welcome Welcome to the bookclub! This is a companion for the book Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson (Chapman and Hall/CRC, copyright August 2, 2019, 9781138079229). This companion is available at r4ds.io/feat_eng. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize the structure of the book Establish base lines for good practice Define feature engineering "],["structure-of-the-book.html", "1.1 Structure of the book", " 1.1 Structure of the book The book is divided into two main parts: Feature engineering (techniques for augmenting predictors - chapters 2-9) Predicting risk of Ischemic Review of the PMP (predictive modeling process) Exploratory visualization Encoding categorical predictors Engineering numeric predictors Detecting interaction effects Handling missing data Working with profile data (time series analysis) Feature selection (methods for filtering the enhanced predictors - chapters 10-12) Overview Greedy search methods (simple filters and eliminations) Golbal search methods (predictor space investigations) "],["good-practice-guidelines.html", "1.2 Good Practice guidelines", " 1.2 Good Practice guidelines There are some vital steps to take to modeling: knowledge of the process to model collect appropriate data understand variation in the response select relevant predictors utilize a range of models All of these are not enough when model lacks on performance. The answer might be the in the way the predictors are presented to the model. 1.2.1 What is feature engineering “…best re-representation of the predictors to improve model performance.” (ct. Preface) What are the possible ways to acheive a better performance? transform the predictors with special functions (log/exp) add an interaction term (prod/ratio) add a functional transformation (splines/poly) add a re-representation of the predictors (mean/med/standardz) imputing missing values (knn/bagging) Disclaimer: Risk of Overfitting! 1.2.2 Nature of modeling The estimation of uncertainty/noise is another very important step to take. “If a model is only 50% accurate should it be used to make inferences or predictions?” The trade-off between accuracy and interpretability is important, a neural network model might be less explicable but can provide a higher level of accuracy. Feature engineering is a matter of choice in finding the most suitable variable transformation for the best performance. More considerations about bad model reactions to: multicollinarity or correlation between predictors missing values irrelevant predictors "],["a-model-with-two-predictors.html", "1.3 A model with two predictors", " 1.3 A model with two predictors data(segmentationData) This example uses segmentationData. Data originates from an experiment from Hill et al. (2007), a study on “Impact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells.” BMC Bioinformatics. The data set includes a Case vector containing Train and Test variables, with a total of 61 different vectors, about cellular structures and morphology. Selected for this first example are two predictors: EqSphereAreaCh1 and PerimCh1. The objective is to predict shape parameters of poorly-segmented (PS) and well-segmented (WS) cells from the Class variable. This is the full list of variables in the set. ## [1] &quot;Cell&quot; &quot;Case&quot; ## [3] &quot;Class&quot; &quot;AngleCh1&quot; ## [5] &quot;AreaCh1&quot; &quot;AvgIntenCh1&quot; ## [7] &quot;AvgIntenCh2&quot; &quot;AvgIntenCh3&quot; ## [9] &quot;AvgIntenCh4&quot; &quot;ConvexHullAreaRatioCh1&quot; ## [11] &quot;ConvexHullPerimRatioCh1&quot; &quot;DiffIntenDensityCh1&quot; ## [13] &quot;DiffIntenDensityCh3&quot; &quot;DiffIntenDensityCh4&quot; ## [15] &quot;EntropyIntenCh1&quot; &quot;EntropyIntenCh3&quot; ## [17] &quot;EntropyIntenCh4&quot; &quot;EqCircDiamCh1&quot; ## [19] &quot;EqEllipseLWRCh1&quot; &quot;EqEllipseOblateVolCh1&quot; ## [21] &quot;EqEllipseProlateVolCh1&quot; &quot;EqSphereAreaCh1&quot; ## [23] &quot;EqSphereVolCh1&quot; &quot;FiberAlign2Ch3&quot; ## [25] &quot;FiberAlign2Ch4&quot; &quot;FiberLengthCh1&quot; ## [27] &quot;FiberWidthCh1&quot; &quot;IntenCoocASMCh3&quot; ## [29] &quot;IntenCoocASMCh4&quot; &quot;IntenCoocContrastCh3&quot; ## [31] &quot;IntenCoocContrastCh4&quot; &quot;IntenCoocEntropyCh3&quot; ## [33] &quot;IntenCoocEntropyCh4&quot; &quot;IntenCoocMaxCh3&quot; ## [35] &quot;IntenCoocMaxCh4&quot; &quot;KurtIntenCh1&quot; ## [37] &quot;KurtIntenCh3&quot; &quot;KurtIntenCh4&quot; ## [39] &quot;LengthCh1&quot; &quot;NeighborAvgDistCh1&quot; ## [41] &quot;NeighborMinDistCh1&quot; &quot;NeighborVarDistCh1&quot; ## [43] &quot;PerimCh1&quot; &quot;ShapeBFRCh1&quot; ## [45] &quot;ShapeLWRCh1&quot; &quot;ShapeP2ACh1&quot; ## [47] &quot;SkewIntenCh1&quot; &quot;SkewIntenCh3&quot; ## [49] &quot;SkewIntenCh4&quot; &quot;SpotFiberCountCh3&quot; ## [51] &quot;SpotFiberCountCh4&quot; &quot;TotalIntenCh1&quot; ## [53] &quot;TotalIntenCh2&quot; &quot;TotalIntenCh3&quot; ## [55] &quot;TotalIntenCh4&quot; &quot;VarIntenCh1&quot; ## [57] &quot;VarIntenCh3&quot; &quot;VarIntenCh4&quot; ## [59] &quot;WidthCh1&quot; &quot;XCentroid&quot; ## [61] &quot;YCentroid&quot; ## [1] 2019 61 Parsimony: ## Class Area Perimeter ## 1 PS 3278.726 154.89876 ## 2 WS 1727.410 84.56460 ## 3 PS 1194.932 101.09107 ## 4 WS 1027.222 68.71062 ## 5 PS 1035.608 73.40559 ## 6 PS 1433.918 79.47569 The dataset is already split between training and test sets, all that is to be added is cross-validation on the training set. set.seed(2222) folds &lt;- vfold_cv(train, v = 10) A first visualization of the relationship between the two predictors. Check for Class imbalance of the response variable This would be the first level transformation of the response, this type of transformation is considered a structural transformation, we will see more about it later in the book. PS WS tb_class 636.00 373.00 pr_class 0.63 0.37 up_samp_ws &lt;- pr_class[2] Recipes library(themis) log_rec_natural_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) log_rec_inverse_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) %&gt;% step_BoxCox(all_numeric()) Workflow logistic_reg_glm_spec &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) log_wfl_natural_units &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(log_rec_natural_units) log_fit_natural_units &lt;- log_wfl_natural_units %&gt;% fit(train) log_fit_natural_units %&gt;% extract_fit_parsnip() %&gt;% tidy() # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 1.58 0.248 6.36 1.99e-10 2 Area 0.00301 0.000281 10.7 8.95e-27 3 Perimeter -0.0682 0.00604 -11.3 1.47e-29 Prediction with_pred_natural_units &lt;- log_fit_natural_units %&gt;% augment(test) with_pred_natural_units %&gt;% head # A tibble: 6 × 6 Class Area Perimeter .pred_class .pred_PS .pred_WS &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 PS 742. 68.8 PS 0.705 0.295 2 PS 1140. 86.5 PS 0.707 0.293 3 WS 692. 49.5 WS 0.429 0.571 4 WS 709. 50.4 WS 0.431 0.569 5 PS 1006. 89.9 PS 0.820 0.180 6 WS 1983. 112. PS 0.516 0.484 Confusion Matrics Roc Curve with_pred_natural_units %&gt;% roc_curve(Class,.pred_PS) %&gt;% mutate(Format = &quot;Natural Units&quot;) %&gt;% ggplot(aes(1 - specificity, sensitivity))+ geom_line(aes(color = .threshold), size = 1)+ geom_abline(linetype = &quot;dashed&quot;, size = 1, color = &quot;gray&quot;) + scale_colour_continuous()+ theme_fivethirtyeight() + theme(axis.title = element_text()) Workflow set Let’s compare the two transformations with a workflow_set(): full_workflow &lt;- workflow_set( models = list(logitstic = logistic_reg_glm_spec), preproc = list(natural_units = log_rec_natural_units, inverse_units = log_rec_inverse_units)) system.time( grid_results &lt;- full_workflow %&gt;% workflow_map( seed = 1503, resamples = folds, grid = 25, control = control_grid( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE), verbose = TRUE) ) user system elapsed 7.682 0.004 7.693 grid_results # A workflow set/tibble: 2 × 4 wflow_id info option result &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 natural_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; 2 inverse_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; Roc curves for two different recipes roc &lt;- grid_results %&gt;% unnest(result) %&gt;% unnest(.predictions) %&gt;% select(wflow_id, .pred_PS, .pred_WS, .pred_class, Class) %&gt;% group_by(wflow_id) %&gt;% roc_curve(Class, .pred_PS) roc_curves &lt;- roc %&gt;% ggplot( aes(x = 1 - specificity, y = sensitivity, group = wflow_id, color = wflow_id) ) + geom_line(size = 0.5) + geom_abline(lty = 2, alpha = 0.5, color = &quot;gray50&quot;, size = 0.8)+ scale_color_tableau()+ theme_fivethirtyeight()+ theme(axis.title = element_text()) roc_curves "],["important-concepts.html", "1.4 Important concepts", " 1.4 Important concepts Overfitting Supervised and unsupervised Model bias and variance Experience and empirically driven modeling Generalizing the main boundaries, the risk of overfitting the model is always challenged by anomalous patterns new data can hide. 1.4.1 Acknowledge vulnerabilities To consider: small number of observations compared to the number of predictors low bias models can have a higher likelihood of overfitting supervised analysis can be used to detect predictors significance No free lunch therem (Wolpert, 1996) - knowledge is an important part of modeling variance-bias trade-off Low variance: linear/logistic regression and PLS High variance: trees, nearest neighbor, neural networks Bias: level of ability to closer estimation irrilevant predictors can causing excess model variation be data-driven rather than experience-driven big data does not mean better data unlabeled data can improve autoencoders modeling compensatory effect there may not be a unique set of predictors. Finally, one more important consideration is to consider Strategies for Supervised and Unsupervised feature selections. Supervised selection method can be divided into: wrapper methods, such as backwards and stepwise selection embedded methods, such as decision tree variable selection Unsupervised selection method variable encoding, such as dummy or indicator variables 1.4.2 The Modeling process Few steps summary: EDA summary and correlation model methods evaluation model tuning summary measures and EDA residual analysis/ check for systematic issues more feature engineering model selection final bake off prediction "],["predicting-ridership-on-chicago.html", "1.5 Predicting ridership on Chicago", " 1.5 Predicting ridership on Chicago This set will be widely used in the book to predict the number of people entering a train station daily. library(modeldata) modeldata::Chicago %&gt;% head ## # A tibble: 6 × 50 ## rider…¹ Austin Quinc…² Belmont Arche…³ Oak_P…⁴ Western Clark…⁵ Clinton Merch…⁶ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.7 1.46 8.37 4.60 2.01 1.42 3.32 15.6 2.40 6.48 ## 2 15.8 1.50 8.35 4.72 2.09 1.43 3.34 15.7 2.40 6.48 ## 3 15.9 1.52 8.36 4.68 2.11 1.49 3.36 15.6 2.37 6.40 ## 4 15.9 1.49 7.85 4.77 2.17 1.44 3.36 15.7 2.42 6.49 ## 5 15.4 1.50 7.62 4.72 2.06 1.42 3.27 15.6 2.42 5.80 ## 6 2.42 0.693 0.911 2.27 0.624 0.426 1.11 2.41 0.814 0.858 ## # … with 40 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, ## # Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;, Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, ## # Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;, California &lt;dbl&gt;, ## # temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;, ## # humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, ## # wind_max &lt;dbl&gt;, gust &lt;dbl&gt;, gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, ## # weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;, weather_cloud &lt;dbl&gt;, … 1.5.1 Extra Resources Cooking Your Data with Recipes Here is a nice example on how to Compute a sliding mean by Julia Silge caret-vs-tidymodels tidymodels-or-caret-how-they-compare "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log LOG "],["illustrative-example-predicting-risk-of-ischemic-stroke.html", "Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke", " Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "2.1 SLIDE 1", " 2.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-1.html", "2.2 Meeting Videos", " 2.2 Meeting Videos 2.2.1 Cohort 1 Meeting chat log LOG "],["a-review-of-the-predictive-modeling-process.html", "Chapter 3 A Review of the Predictive Modeling Process", " Chapter 3 A Review of the Predictive Modeling Process Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-2.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log LOG "],["exploratory-visualizations.html", "Chapter 4 Exploratory Visualizations", " Chapter 4 Exploratory Visualizations Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-2.html", "4.1 SLIDE 1", " 4.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-3.html", "4.2 Meeting Videos", " 4.2 Meeting Videos 4.2.1 Cohort 1 Meeting chat log LOG "],["encoding-categorical-predictors.html", "Chapter 5 Encoding Categorical Predictors", " Chapter 5 Encoding Categorical Predictors Categorical (also called nominal) predictors are those that contain qualitative data. Examples include: Education level ZIP code Text Day of the week Color A large majority of models require that all predictors be numeric. A summary of parsnip model preprocessors from: Tidy Modeling with R by Max Kuhn and Julia Silge knitr::opts_chunk$set(fig.path = &quot;images/&quot;) suppressPackageStartupMessages({ library(tidyverse) library(tidymodels) library(embed) library(cli) library(kableExtra) }) Table 5.1: Preprocessing methods for different models. model dummy zv impute decorrelate normalize transform bag_mars() ✔ × ✔ ◌ × ◌ bag_tree() × × × ◌¹ × × bart() × × × ◌¹ × × boost_tree() ×² ◌ ✔² ◌¹ × × C5_rules() × × × × × × cubist_rules() × × × × × × decision_tree() × × × ◌¹ × × discrim_flexible() ✔ × ✔ ✔ × ◌ discrim_linear() ✔ ✔ ✔ ✔ × ◌ discrim_regularized() ✔ ✔ ✔ ✔ × ◌ gen_additive_mod() ✔ ✔ ✔ ✔ × ◌ linear_reg() ✔ ✔ ✔ ✔ × ◌ logistic_reg() ✔ ✔ ✔ ✔ × ◌ mars() ✔ × ✔ ◌ × ◌ mlp() ✔ ✔ ✔ ✔ ✔ ✔ multinom_reg() ✔ ✔ ✔ ✔ ×² ◌ naive_Bayes() × ✔ ✔ ◌¹ × × nearest_neighbor() ✔ ✔ ✔ ◌ ✔ ✔ pls() ✔ ✔ ✔ × ✔ ✔ poisson_reg() ✔ ✔ ✔ ✔ × ◌ rand_forest() × ◌ ✔² ◌¹ × × rule_fit() ✔ × ✔ ◌¹ ✔ × svm_*() ✔ ✔ ✔ ✔ ✔ ✔ In the table, ✔ indicates that the method is required for the model and × indicates that it is not. The ◌ symbol means that the model may be helped by the technique but it is not required. Algorithms for tree-based models naturally handle splitting both numeric and categorical predictors. These algorithms employ a series if/then statements that sequentially split the data into groups. A naive Bayes model will create a cross-tabulation between a categorical predictor and the outcome class. We will return to this point in the final section of this chapter. Simple categorical variables can also be classified as ordered or unordered. "],["creating-dummy-variables-for-unordered-categories.html", "5.1 Creating Dummy Variables for Unordered Categories", " 5.1 Creating Dummy Variables for Unordered Categories There are many methods for doing this and, to illustrate, consider a simple example for the day of the week. If we take the seven possible values and convert them into binary dummy variables, the mathematical function required to make the translation is often referred to as a contrast. These six numeric predictors would take the place of the original categorical variable. Why only six? if the values of the six dummy variables are known, then the seventh can be directly inferred. When the model has an intercept, an additional initial column of ones for all rows is included. Estimating the parameters for a linear model (as well as other similar models) involve inverting the matrix. If the model includes an intercept and contains dummy variables for all seven days, then the seven day columns would add up (row-wise) to the intercept and this linear combination would prevent the matrix inverse from being computed (as it is singular). Less than full rank encodings are sometimes called “one-hot” encodings. Generating the full set of indicator variables may be advantageous for some models that are insensitive to linear dependencies (an example: glmnet) What is the interpretation of the dummy variables? Consider a linear model for the Chicago transit data that only uses the day of the week. Using the training set to fit the model, the intercept value estimates the mean of the reference cell, which is the average number of Sunday riders in the training set, estimated to be 3.84K people. The second model parameter, for Monday, is estimated to be 12.61K. In the reference cell model, the dummy variables represent the mean value above and beyond the reference cell mean. In this case, estimate indicates that there were 12.61K more riders on Monday than Sunday. train_df &lt;- tibble(m = month.abb, number = seq(1,12, by = 1)) recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing without One HOT (the default) contrasts with April&quot; ) |&gt; row_spec(row = 4, background = &quot;orange&quot;) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing without One HOT (the default) contrasts with April m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing with One HOT.&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing with One HOT. m_Apr m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 "],["encoding-predictors-for-many-categories.html", "5.2 Encoding Predictors for Many Categories", " 5.2 Encoding Predictors for Many Categories What happens when the number of factor levels gets very large? For example, there are more than 40K possible ZIP codes and, depending on how the data are collected, this might produce an overabundance of dummy variables for the size of the data available. Also, ZIP codes in highly populated areas may have a higher rate of occurrence in the data, leading to a “long tail” of locations that are infrequently observed. Also, resampling will exclude some of the rarer categories from the analysis set. The first way to handle this issue is to create the full set of dummy variables and simply remove the zero and low-variance predictors. Still, we may not desire to filter these out. Another approach is to feature engineer an “other” category that pools the rarely occurring categories, assuming that such a pooling is sensible. Another way to combine categories is to use a hashing function that maps each factor level key to a hash value. The number of possible hashes is set by the user and, for numerical purposes, is a power of 2. Some computationally interesting aspects to hash functions are The only data required is the value being hashed and the resulting number of hashes. The translation process is completely deterministic. Hash functions are unidirectional; once the hash values are created, there is no way of knowing the original values. If there are a known and finite set of original values, a table can be created to do the translation but, otherwise, the keys are indeterminable when only the hash value is known. There is no free lunch when using this procedure; some of the original categories will be mapped to the same hash value (called a “collision”). The number of collisions will be largely determined by the number of features that are produced. Categories involved in collisions are not related in any meaningful way. Because of the arbitrary nature of the collisions, it is possible to have different categories whose true underlying effect are counter to one another. This might have the effect of negating the impact of the hashed feature. Hashing functions have no notion of the probability that each key will occur. As such, it is conceivable that a category that occurs with great frequency is aliased with one that is rare. In this case, the more abundant value will have a much larger influence on the effect of that hashing feature. "],["approaches-for-novel-categories.html", "5.3 Approaches for Novel Categories", " 5.3 Approaches for Novel Categories Suppose that a model is built to predict the probability that an individual works in a STEM profession and that this model depends on city names. The model will be able to predict the probability of STEM profession if a new individual lives in one of the cities in the training set. But what happens to the model prediction when a new individual lives in a city that is not represented? One strategy would be to use the previously mentioned “other” category to capture new values. This approach can also be used with feature hashing. "],["supervised-encoding-methods.html", "5.4 Supervised Encoding Methods", " 5.4 Supervised Encoding Methods Beyond dummies, there are many other ways to craft one or more numerical features from a set of nominal predictors. They include 5.4.1 Likelihood Encoding In essence, the effect of the factor level on the outcome is measured and this effect is used as new numeric features. For example, for the Ames housing data, we might calculate the mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model. For classification problems, a simple logistic regression model can be used to measure the effect between the categorical outcome and the categorical predictor. If the outcome event occurs with rate \\[ p \\], the odds of that event is defined as \\[ p / ( 1 − p) \\]. This is an example of a single generalized linear model applied to the hair color feature, which woudl otherwise have 12 dummy levels. as_tibble(dplyr::starwars) |&gt; count(hair_color) ## # A tibble: 13 × 2 ## hair_color n ## &lt;chr&gt; &lt;int&gt; ## 1 auburn 1 ## 2 auburn, grey 1 ## 3 auburn, white 1 ## 4 black 13 ## 5 blond 3 ## 6 blonde 1 ## 7 brown 18 ## 8 brown, grey 1 ## 9 grey 1 ## 10 none 37 ## 11 unknown 1 ## 12 white 4 ## 13 &lt;NA&gt; 5 recipe(skin_color ~ hair_color + eye_color + mass, data = as_tibble(dplyr::starwars)) |&gt; embed::step_lencode_glm(hair_color, outcome = &quot;skin_color&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) |&gt; kable( caption = &quot;Starwars Characters hair_color GLM embedding&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) (#tab:chapter 5 glm numeric embeddings)Starwars Characters hair_color GLM embedding hair_color eye_color mass skin_color -2.862201 gold NA grey -21.566069 brown NA light -2.862201 red 90 mottled green -21.566069 blue 112 unknown -2.862201 orange 82 grey -21.566069 orange 1358 green-tan, brown -21.566069 blue NA fair -21.566069 brown NA light -2.862201 white 48 pale -2.862201 unknown 15 grey, blue While very fast, this method has drawbacks. For example, what happens when a factor level has a single value? Theoretically, the log-odds should be infinite in the appropriate direction but, numerically, it is usually capped at a large (and inaccurate) value. One way around this issue is to use some type of shrinkage method. For example, the overall log-odds can be determined and, if the quality of the data within a factor level is poor, then this level’s effect estimate can be biased towards an overall estimate that disregards the levels of the predictor. A common method for shrinking parameter estimates is Bayesian analysis. (one doubt – step_lencode_bayes appears to only work with two class outcomes ???) as_tibble(datasets::Titanic) |&gt; count(Class) recipe(Survived ~ Class + Sex + Age, data = as_tibble(datasets::Titanic)) |&gt; embed::step_lencode_bayes(Class, outcome = &quot;Survived&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) # A tibble: 10 × 4 Class Sex Age Survived &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 -0.0108 Female Adult Yes 2 -0.0108 Male Adult No 3 -0.00526 Male Child Yes 4 -0.00526 Female Child No 5 -0.00993 Male Adult No 6 -0.0104 Male Child Yes 7 -0.00526 Male Child No 8 -0.0108 Female Adult No 9 -0.00993 Male Child Yes 10 -0.0104 Female Child No Empirical Bayes methods can also be used, in the form of linear (and generalized linear) mixed models. One issue with effect encoding, independent of the estimation method, is that it increases the possibility of overfitting the training data. Use resampling. Another supervised approach comes from the deep learning literature on the analysis of textual data. In addition to the dimension reduction, there is the possibility that these methods can estimate semantic relationships between words so that words with similar themes (e.g., “dog”, “pet”, etc.) have similar values in the new encodings. This technique is not limited to text data and can be used to encode any type of qualitative variable. An example using The Office dialogue and one of the pre-trained GloVe embeddings. library(textrecipes) library(schrute) glove6b &lt;- textdata::embedding_glove6b(dimensions = 100) # the download is 822.2 Mb schrute::theoffice |&gt; slice_sample(n = 10) |&gt; select(character, text) recipe(character ~ text, data = schrute::theoffice) |&gt; step_tokenize(text, options = list(strip_punct = TRUE)) |&gt; step_stem(text) |&gt; step_word_embeddings(text, embeddings = glove6b) |&gt; prep() |&gt; bake(new_data = schrute::theoffice |&gt; slice_sample(n = 10)) The Office dialogue word embeddings with glove6b # A tibble: 10 × 101 character wordembe…¹ worde…² worde…³ worde…⁴ worde…⁵ worde…⁶ &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Angela -1.02 -0.377 0.797 -1.21 -0.802 0.656 2 Roy 0 0 0 0 0 0 3 Phyllis -1.20 -0.373 3.20 -1.60 -1.62 -0.160 4 Kevin -1.54 1.77 5.02 -4.68 -5.23 2.91 5 Roy -2.15 0.735 4.07 -2.20 -1.30 0.297 6 Jim -0.595 0.419 0.699 -0.328 -1.20 1.70 7 Kelly -2.17 4.38 5.97 -4.91 -4.21 4.13 8 Katy -0.891 -0.889 0.0937 -0.859 1.42 1.49 9 Kevin -0.0308 0.120 0.539 -0.437 -0.739 -0.153 10 Jim -0.395 0.240 1.14 -1.27 -1.47 1.39 # … with 94 more variables: wordembed_text_d7 &lt;dbl&gt;, # wordembed_text_d8 &lt;dbl&gt;, wordembed_text_d9 &lt;dbl&gt;, # wordembed_text_d10 &lt;dbl&gt;, wordembed_text_d11 &lt;dbl&gt;, # wordembed_text_d12 &lt;dbl&gt;, wordembed_text_d13 &lt;dbl&gt;, # wordembed_text_d14 &lt;dbl&gt;, wordembed_text_d15 &lt;dbl&gt;, # wordembed_text_d16 &lt;dbl&gt;, wordembed_text_d17 &lt;dbl&gt;, # wordembed_text_d18 &lt;dbl&gt;, wordembed_text_d19 &lt;dbl&gt;, … Note that in place of thousands of sparse dummy colums for each tokenized word, the training set consists of 100 numeric feature dimensions. See also Textrecipes series: Pretrained Word Embedding by Emil Hvitfeldt "],["encodings-for-ordered-data.html", "5.5 Encodings for Ordered Data", " 5.5 Encodings for Ordered Data Suppose that the factors have a relative ordering, like low, medium, and high. R uses a technique called polynomial contrasts to numerically characterize the relationships. values &lt;- c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) dat &lt;- data.frame(x = ordered(values, levels = values)) # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table model.matrix(~ x, dat) ## (Intercept) x.L x.Q ## 1 1 -7.071068e-01 0.4082483 ## 2 1 -7.850462e-17 -0.8164966 ## 3 1 7.071068e-01 0.4082483 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.poly&quot; # using recipes ---------------------------------------------------------------- # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table recipe(~ x, data = dat) |&gt; step_dummy(x) |&gt; prep() |&gt; bake(new_data = NULL) ## # A tibble: 3 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -7.07e- 1 0.408 ## 2 -7.85e-17 -0.816 ## 3 7.07e- 1 0.408 It is important to recognize that patterns described by polynomial contrasts may not effectively relate a predictor to the response. For example, in some cases, one might expect a trend where “low” and “middle” samples have a roughly equivalent response but “high” samples have a much different response. In this case, polynomial contrasts are unlikely to be effective at modeling this trend. Other alternatives to polynomial contrasts: Leave the predictors as unordered factors. Translate the ordered categories into a single set of numeric scores based on context-specific information. Simple visualizations and context-specific expertise can be used to understand whether either of these approaches are good ideas. "],["creating-features-for-text-data.html", "5.6 Creating Features for Text Data", " 5.6 Creating Features for Text Data Often, data contain textual fields that are gathered from questionnaires, articles, reviews, tweets, and other sources. Are there words or phrases that would make good predictors of the outcome? To determine this, the text data must first be processed and cleaned. One approach is to measure for “importance”, that is, keywords with odds-ratios of at least 2 (in either direction) to be considered for modeling. See also Supervised Machine Learning for Text Analysis in R for a much better explanation. Other methods for preprocessing text data include: removing commonly used stop words, such as “is”, “the”, “and”, etc. stemming the words so that similar words, such as the singular and plural versions, are represented as a single entity. filter for the most common tokens, and then calculate the term frequency-inverse document frequency (tf-idf) statistic for each token "],["factors-versus-dummy-variables-in-tree-based-models.html", "5.7 Factors versus Dummy Variables in Tree-Based Models", " 5.7 Factors versus Dummy Variables in Tree-Based Models Certain types of models have the ability to use categorical data in its natural form. For example, a Chicago ridership decision tree could split on if day in {Sun, Sat} then ridership = 4.4K else ridership = 17.3K Suppose the day of the week had been converted to dummy variables. What would have occurred? In this case, the model is slightly more complex since it can only create rules as a function of a single dummy variable at a time: if day = Sun then ridership = 3.84K else if day = Sat then ridership = 4.96K else ridership = 17.30K So, for decision trees and naiive bayes does it matter how the categorical features are encoded? To answer this question, a series of experiments was conducted. The results: For these data sets, there is no real difference in the area under the ROC curve between the encoding methods. In terms of performance, it appears that differences between the two encodings are rare (but can occur). One other statistic was computed for each of the simulations: the time to train the models. Here, there is very strong trend that factor-based models are more efficiently trained than their dummy variable counterparts. One other effect of how qualitative predictors are encoded is related to summary measures. Many of these techniques, especially tree-based models, calculate variable importance scores that are relative measures for how much a predictor affected the outcome. For example, trees measure the effect of a specific split on the improvement in model performance (e.g., impurity, residual error, etc.). As predictors are used in splits, these improvements are aggregated; these can be used as the importance scores. "],["summary.html", "5.8 Summary", " 5.8 Summary With the exception of tree-based models, categorical predictors must first be converted to numeric representations to enable other models to use the information. The simplest feature engineering technique is to convert each category to a separate binary dummy predictor. Some models require one fewer dummy predictors than the number of categories. Creating dummy predictors may not be the most effective way. If, for instance, the predictor has ordered categories, then polynomial contrasts may be better. Text fields, too, can be viewed as an agglomeration of categorical predictors and must be converted to numerics. "],["meeting-videos-4.html", "5.9 Meeting Videos", " 5.9 Meeting Videos 5.9.1 Cohort 1 No chat for this session LOG "],["engineering-numeric-predictors.html", "Chapter 6 Engineering Numeric Predictors", " Chapter 6 Engineering Numeric Predictors Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "6.1 SLIDE 1", " 6.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-5.html", "6.2 Meeting Videos", " 6.2 Meeting Videos 6.2.1 Cohort 1 Meeting chat log LOG "],["detecting-interaction-effects.html", "Chapter 7 Detecting Interaction Effects", " Chapter 7 Detecting Interaction Effects Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-4.html", "7.1 SLIDE 1", " 7.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-6.html", "7.2 Meeting Videos", " 7.2 Meeting Videos 7.2.1 Cohort 1 Meeting chat log LOG "],["handling-missing-data.html", "Chapter 8 Handling Missing Data", " Chapter 8 Handling Missing Data Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-5.html", "8.1 SLIDE 1", " 8.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-7.html", "8.2 Meeting Videos", " 8.2 Meeting Videos 8.2.1 Cohort 1 Meeting chat log LOG "],["working-with-profile-data.html", "Chapter 9 Working with Profile Data", " Chapter 9 Working with Profile Data Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-6.html", "9.1 SLIDE 1", " 9.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "9.2 Meeting Videos", " 9.2 Meeting Videos 9.2.1 Cohort 1 Meeting chat log LOG "],["feature-selection-overview.html", "Chapter 10 Feature Selection Overview", " Chapter 10 Feature Selection Overview Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-7.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["greedy-search-methods.html", "Chapter 11 Greedy Search Methods", " Chapter 11 Greedy Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-8.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["global-search-methods.html", "Chapter 12 Global Search Methods", " Chapter 12 Global Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-9.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-11.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
