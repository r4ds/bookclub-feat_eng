[["index.html", "Feature Engineering and Selection Book Club Welcome", " Feature Engineering and Selection Book Club The R4DS Online Learning Community 2022-10-14 Welcome Welcome to the bookclub! This is a companion for the book Feature Engineering and Selection: A Practical Approach for Predictive Models by Max Kuhn and Kjell Johnson (Chapman and Hall/CRC, copyright August 2, 2019, 9781138079229). This companion is available at r4ds.io/feat_eng. This website is being developed by the R4DS Online Learning Community. Follow along, and join the community to participate. This companion follows the R4DS Online Learning Community Code of Conduct. "],["book-club-meetings.html", "Book club meetings", " Book club meetings Each week, a volunteer will present a chapter from the book (or part of a chapter). This is the best way to learn the material. Presentations will usually consist of a review of the material, a discussion, and/or a demonstration of the principles presented in that chapter. More information about how to present is available in the github repo. Presentations will be recorded, and will be available on the R4DS Online Learning Community YouTube Channel. "],["pace.html", "Pace", " Pace We’ll try to cover 1 chapter/week, but… …It’s ok to split chapters when they feel like too much. We will try to meet every week, but will likely take some breaks for holidays, etc. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Learning objectives: Recognize the structure of the book Establish base lines for good practice Define feature engineering "],["structure-of-the-book.html", "1.1 Structure of the book", " 1.1 Structure of the book The book is divided into two main parts: Feature engineering (techniques for augmenting predictors - chapters 2-9) Predicting risk of Ischemic Review of the PMP (predictive modeling process) Exploratory visualization Encoding categorical predictors Engineering numeric predictors Detecting interaction effects Handling missing data Working with profile data (time series analysis) Feature selection (methods for filtering the enhanced predictors - chapters 10-12) Overview Greedy search methods (simple filters and eliminations) Golbal search methods (predictor space investigations) "],["good-practice-guidelines.html", "1.2 Good Practice guidelines", " 1.2 Good Practice guidelines There are some vital steps to take to modeling: knowledge of the process to model collect appropriate data understand variation in the response select relevant predictors utilize a range of models All of these are not enough when model lacks on performance. The answer might be the in the way the predictors are presented to the model. 1.2.1 What is feature engineering “…best re-representation of the predictors to improve model performance.” (ct. Preface) What are the possible ways to acheive a better performance? transform the predictors with special functions (log/exp) add an interaction term (prod/ratio) add a functional transformation (splines/poly) add a re-representation of the predictors (mean/med/standardz) imputing missing values (knn/bagging) Disclaimer: Risk of Overfitting! 1.2.2 Nature of modeling The estimation of uncertainty/noise is another very important step to take. “If a model is only 50% accurate should it be used to make inferences or predictions?” The trade-off between accuracy and interpretability is important, a neural network model might be less explicable but can provide a higher level of accuracy. Feature engineering is a matter of choice in finding the most suitable variable transformation for the best performance. More considerations about bad model reactions to: multicollinarity or correlation between predictors missing values irrelevant predictors "],["a-model-with-two-predictors.html", "1.3 A model with two predictors", " 1.3 A model with two predictors data(segmentationData) This example uses segmentationData. Data originates from an experiment from Hill et al. (2007), a study on “Impact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells.” BMC Bioinformatics. The data set includes a Case vector containing Train and Test variables, with a total of 61 different vectors, about cellular structures and morphology. Selected for this first example are two predictors: EqSphereAreaCh1 and PerimCh1. The objective is to predict shape parameters of poorly-segmented (PS) and well-segmented (WS) cells from the Class variable. This is the full list of variables in the set. ## [1] &quot;Cell&quot; &quot;Case&quot; ## [3] &quot;Class&quot; &quot;AngleCh1&quot; ## [5] &quot;AreaCh1&quot; &quot;AvgIntenCh1&quot; ## [7] &quot;AvgIntenCh2&quot; &quot;AvgIntenCh3&quot; ## [9] &quot;AvgIntenCh4&quot; &quot;ConvexHullAreaRatioCh1&quot; ## [11] &quot;ConvexHullPerimRatioCh1&quot; &quot;DiffIntenDensityCh1&quot; ## [13] &quot;DiffIntenDensityCh3&quot; &quot;DiffIntenDensityCh4&quot; ## [15] &quot;EntropyIntenCh1&quot; &quot;EntropyIntenCh3&quot; ## [17] &quot;EntropyIntenCh4&quot; &quot;EqCircDiamCh1&quot; ## [19] &quot;EqEllipseLWRCh1&quot; &quot;EqEllipseOblateVolCh1&quot; ## [21] &quot;EqEllipseProlateVolCh1&quot; &quot;EqSphereAreaCh1&quot; ## [23] &quot;EqSphereVolCh1&quot; &quot;FiberAlign2Ch3&quot; ## [25] &quot;FiberAlign2Ch4&quot; &quot;FiberLengthCh1&quot; ## [27] &quot;FiberWidthCh1&quot; &quot;IntenCoocASMCh3&quot; ## [29] &quot;IntenCoocASMCh4&quot; &quot;IntenCoocContrastCh3&quot; ## [31] &quot;IntenCoocContrastCh4&quot; &quot;IntenCoocEntropyCh3&quot; ## [33] &quot;IntenCoocEntropyCh4&quot; &quot;IntenCoocMaxCh3&quot; ## [35] &quot;IntenCoocMaxCh4&quot; &quot;KurtIntenCh1&quot; ## [37] &quot;KurtIntenCh3&quot; &quot;KurtIntenCh4&quot; ## [39] &quot;LengthCh1&quot; &quot;NeighborAvgDistCh1&quot; ## [41] &quot;NeighborMinDistCh1&quot; &quot;NeighborVarDistCh1&quot; ## [43] &quot;PerimCh1&quot; &quot;ShapeBFRCh1&quot; ## [45] &quot;ShapeLWRCh1&quot; &quot;ShapeP2ACh1&quot; ## [47] &quot;SkewIntenCh1&quot; &quot;SkewIntenCh3&quot; ## [49] &quot;SkewIntenCh4&quot; &quot;SpotFiberCountCh3&quot; ## [51] &quot;SpotFiberCountCh4&quot; &quot;TotalIntenCh1&quot; ## [53] &quot;TotalIntenCh2&quot; &quot;TotalIntenCh3&quot; ## [55] &quot;TotalIntenCh4&quot; &quot;VarIntenCh1&quot; ## [57] &quot;VarIntenCh3&quot; &quot;VarIntenCh4&quot; ## [59] &quot;WidthCh1&quot; &quot;XCentroid&quot; ## [61] &quot;YCentroid&quot; ## [1] 2019 61 Parsimony: ## Class Area Perimeter ## 1 PS 3278.726 154.89876 ## 2 WS 1727.410 84.56460 ## 3 PS 1194.932 101.09107 ## 4 WS 1027.222 68.71062 ## 5 PS 1035.608 73.40559 ## 6 PS 1433.918 79.47569 The dataset is already split between training and test sets, all that is to be added is cross-validation on the training set. set.seed(2222) folds &lt;- vfold_cv(train, v = 10) A first visualization of the relationship between the two predictors. Check for Class imbalance of the response variable This would be the first level transformation of the response, this type of transformation is considered a structural transformation, we will see more about it later in the book. PS WS tb_class 636.00 373.00 pr_class 0.63 0.37 up_samp_ws &lt;- pr_class[2] Recipes library(themis) log_rec_natural_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) log_rec_inverse_units &lt;- recipe(Class ~ Area + Perimeter, data = train) %&gt;% step_upsample(Class, over_ratio = up_samp_ws) %&gt;% step_BoxCox(all_numeric()) Workflow logistic_reg_glm_spec &lt;- logistic_reg() %&gt;% set_engine(&#39;glm&#39;) log_wfl_natural_units &lt;- workflow() %&gt;% add_model(logistic_reg_glm_spec) %&gt;% add_recipe(log_rec_natural_units) log_fit_natural_units &lt;- log_wfl_natural_units %&gt;% fit(train) log_fit_natural_units %&gt;% extract_fit_parsnip() %&gt;% tidy() # A tibble: 3 × 5 term estimate std.error statistic p.value &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 (Intercept) 1.58 0.248 6.36 1.99e-10 2 Area 0.00301 0.000281 10.7 8.95e-27 3 Perimeter -0.0682 0.00604 -11.3 1.47e-29 Prediction with_pred_natural_units &lt;- log_fit_natural_units %&gt;% augment(test) with_pred_natural_units %&gt;% head # A tibble: 6 × 6 Class Area Perimeter .pred_class .pred_PS .pred_WS &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; 1 PS 742. 68.8 PS 0.705 0.295 2 PS 1140. 86.5 PS 0.707 0.293 3 WS 692. 49.5 WS 0.429 0.571 4 WS 709. 50.4 WS 0.431 0.569 5 PS 1006. 89.9 PS 0.820 0.180 6 WS 1983. 112. PS 0.516 0.484 Confusion Matrics Roc Curve with_pred_natural_units %&gt;% roc_curve(Class,.pred_PS) %&gt;% mutate(Format = &quot;Natural Units&quot;) %&gt;% ggplot(aes(1 - specificity, sensitivity))+ geom_line(aes(color = .threshold), size = 1)+ geom_abline(linetype = &quot;dashed&quot;, size = 1, color = &quot;gray&quot;) + scale_colour_continuous()+ theme_fivethirtyeight() + theme(axis.title = element_text()) Workflow set Let’s compare the two transformations with a workflow_set(): full_workflow &lt;- workflow_set( models = list(logitstic = logistic_reg_glm_spec), preproc = list(natural_units = log_rec_natural_units, inverse_units = log_rec_inverse_units)) system.time( grid_results &lt;- full_workflow %&gt;% workflow_map( seed = 1503, resamples = folds, grid = 25, control = control_grid( save_pred = TRUE, parallel_over = &quot;everything&quot;, save_workflow = TRUE), verbose = TRUE) ) user system elapsed 7.791 0.024 7.816 grid_results # A workflow set/tibble: 2 × 4 wflow_id info option result &lt;chr&gt; &lt;list&gt; &lt;list&gt; &lt;list&gt; 1 natural_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; 2 inverse_units_logitstic &lt;tibble [1 × 4]&gt; &lt;opts[3]&gt; &lt;rsmp[+]&gt; Roc curves for two different recipes roc &lt;- grid_results %&gt;% unnest(result) %&gt;% unnest(.predictions) %&gt;% select(wflow_id, .pred_PS, .pred_WS, .pred_class, Class) %&gt;% group_by(wflow_id) %&gt;% roc_curve(Class, .pred_PS) roc_curves &lt;- roc %&gt;% ggplot( aes(x = 1 - specificity, y = sensitivity, group = wflow_id, color = wflow_id) ) + geom_line(size = 0.5) + geom_abline(lty = 2, alpha = 0.5, color = &quot;gray50&quot;, size = 0.8)+ scale_color_tableau()+ theme_fivethirtyeight()+ theme(axis.title = element_text()) roc_curves "],["important-concepts.html", "1.4 Important concepts", " 1.4 Important concepts Overfitting Supervised and unsupervised Model bias and variance Experience and empirically driven modeling Generalizing the main boundaries, the risk of overfitting the model is always challenged by anomalous patterns new data can hide. 1.4.1 Acknowledge vulnerabilities To consider: small number of observations compared to the number of predictors low bias models can have a higher likelihood of overfitting supervised analysis can be used to detect predictors significance No free lunch therem (Wolpert, 1996) - knowledge is an important part of modeling variance-bias trade-off Low variance: linear/logistic regression and PLS High variance: trees, nearest neighbor, neural networks Bias: level of ability to closer estimation irrilevant predictors can causing excess model variation be data-driven rather than experience-driven big data does not mean better data unlabeled data can improve autoencoders modeling compensatory effect there may not be a unique set of predictors. Finally, one more important consideration is to consider Strategies for Supervised and Unsupervised feature selections. Supervised selection method can be divided into: wrapper methods, such as backwards and stepwise selection embedded methods, such as decision tree variable selection Unsupervised selection method variable encoding, such as dummy or indicator variables 1.4.2 The Modeling process Few steps summary: EDA summary and correlation model methods evaluation model tuning summary measures and EDA residual analysis/ check for systematic issues more feature engineering model selection final bake off prediction "],["predicting-ridership-on-chicago.html", "1.5 Predicting ridership on Chicago", " 1.5 Predicting ridership on Chicago This set will be widely used in the book to predict the number of people entering a train station daily. library(modeldata) modeldata::Chicago %&gt;% head ## # A tibble: 6 × 50 ## rider…¹ Austin Quinc…² Belmont Arche…³ Oak_P…⁴ Western Clark…⁵ Clinton Merch…⁶ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.7 1.46 8.37 4.60 2.01 1.42 3.32 15.6 2.40 6.48 ## 2 15.8 1.50 8.35 4.72 2.09 1.43 3.34 15.7 2.40 6.48 ## 3 15.9 1.52 8.36 4.68 2.11 1.49 3.36 15.6 2.37 6.40 ## 4 15.9 1.49 7.85 4.77 2.17 1.44 3.36 15.7 2.42 6.49 ## 5 15.4 1.50 7.62 4.72 2.06 1.42 3.27 15.6 2.42 5.80 ## 6 2.42 0.693 0.911 2.27 0.624 0.426 1.11 2.41 0.814 0.858 ## # … with 40 more variables: Irving_Park &lt;dbl&gt;, Washington_Wells &lt;dbl&gt;, ## # Harlem &lt;dbl&gt;, Monroe &lt;dbl&gt;, Polk &lt;dbl&gt;, Ashland &lt;dbl&gt;, Kedzie &lt;dbl&gt;, ## # Addison &lt;dbl&gt;, Jefferson_Park &lt;dbl&gt;, Montrose &lt;dbl&gt;, California &lt;dbl&gt;, ## # temp_min &lt;dbl&gt;, temp &lt;dbl&gt;, temp_max &lt;dbl&gt;, temp_change &lt;dbl&gt;, dew &lt;dbl&gt;, ## # humidity &lt;dbl&gt;, pressure &lt;dbl&gt;, pressure_change &lt;dbl&gt;, wind &lt;dbl&gt;, ## # wind_max &lt;dbl&gt;, gust &lt;dbl&gt;, gust_max &lt;dbl&gt;, percip &lt;dbl&gt;, percip_max &lt;dbl&gt;, ## # weather_rain &lt;dbl&gt;, weather_snow &lt;dbl&gt;, weather_cloud &lt;dbl&gt;, … 1.5.1 Extra Resources Cooking Your Data with Recipes Here is a nice example on how to Compute a sliding mean by Julia Silge caret-vs-tidymodels tidymodels-or-caret-how-they-compare "],["meeting-videos.html", "1.6 Meeting Videos", " 1.6 Meeting Videos 1.6.1 Cohort 1 Meeting chat log LOG "],["illustrative-example-predicting-risk-of-ischemic-stroke.html", "Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke", " Chapter 2 Illustrative Example: Predicting Risk of Ischemic Stroke Learning objectives: Understanding the computing part with two examples "],["introduction-1.html", "2.1 Introduction", " 2.1 Introduction Here we see how to make things in practice with two case studies. "],["example-1.html", "2.2 Example 1", " 2.2 Example 1 Code for Ischemic Stroke case study. Code requires these packages, across all of Chapter 2: library(corrplot) library(utils) library(pROC) library(plotly) library(caret) library(patchwork) library(tidymodels) theme_set(theme_bw()) Load the stroke_data.R.data from the Ischemic_Stroke folder here: https://github.com/topepo/FES/tree/master/Data_Sets load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Ischemic_Stroke/stroke_data.RData?raw=true&quot;)) load(url(&quot;https://github.com/topepo/FES/blob/master/02_Predicting_Risk_of_Ischemic_Stroke/stroke_rfe.RData?raw=true&quot;)) #pre_split data how many in each class by set stroke_train %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Training&quot;) %&gt;% bind_rows( stroke_test %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Testing&quot;) ) %&gt;% spread(Stroke, n) ## Data N Y ## 1 Testing 18 19 ## 2 Training 44 45 #sampling using tidymodels/rsample package all_stroke &lt;- bind_rows(stroke_train, stroke_test) #put all the data back together tidy_sample &lt;- #sample, but fixing proportion of Stroke between test and train initial_split(all_stroke, prop = 0.71, strata = Stroke) tidy_testing &lt;- testing(tidy_sample) #extract testing tidy_training &lt;- training(tidy_sample) #extract training tidy_training %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Training&quot;) %&gt;% bind_rows( tidy_testing %&gt;% count(Stroke) %&gt;% mutate(Data = &quot;Testing&quot;) ) %&gt;% spread(Stroke, n) ## Data N Y ## 1 Testing 18 19 ## 2 Training 44 45 # distribution of training and testing is either exactly the same or one off depending on seed 2.2.1 Predictor Quality The first thing is to just look at your data: what’s missing what’s normal(ish) what’s the range what’s the data type dplyr’s glimpse is a good starting point. dplyr::glimpse(tidy_training) ## Rows: 89 ## Columns: 29 ## $ Stroke &lt;fct&gt; N, N, N, N, N, N, N, N, N, N, N, N, N, N, … ## $ NASCET &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ CALCVol &lt;dbl&gt; 235.25260, 31.43360, 360.73754, 433.34638,… ## $ CALCVolProp &lt;dbl&gt; 0.070442702, 0.016164769, 0.073350960, 0.1… ## $ MATXVol &lt;dbl&gt; 3156.835, 3032.861, 4444.045, 3106.593, 24… ## $ MATXVolProp &lt;dbl&gt; 0.7599582, 0.8133063, 0.7839963, 0.7520699… ## $ LRNCVol &lt;dbl&gt; 224.87171, 368.56066, 277.26275, 36.50728,… ## $ LRNCVolProp &lt;dbl&gt; 0.09108513, 0.13398944, 0.06073955, 0.0146… ## $ MaxCALCArea &lt;dbl&gt; 12.350494, 7.130660, 21.297476, 27.488064,… ## $ MaxCALCAreaProp &lt;dbl&gt; 0.3657684, 0.2112469, 0.3862498, 0.4901225… ## $ MaxDilationByArea &lt;dbl&gt; 520.98259, 91.72005, 60.42487, 240.78404, … ## $ MaxMATXArea &lt;dbl&gt; 71.24743, 27.21084, 43.43857, 38.78870, 36… ## $ MaxMATXAreaProp &lt;dbl&gt; 0.9523705, 0.9455539, 0.9526153, 0.9439308… ## $ MaxLRNCArea &lt;dbl&gt; 21.686815, 6.434661, 9.403324, 3.242581, 3… ## $ MaxLRNCAreaProp &lt;dbl&gt; 0.42957812, 0.28151013, 0.35606305, 0.0725… ## $ MaxMaxWallThickness &lt;dbl&gt; 2.409943, 2.540334, 3.411158, 3.695132, 4.… ## $ MaxRemodelingRatio &lt;dbl&gt; 5.697931, 1.739927, 2.059370, 2.909791, 2.… ## $ MaxStenosisByArea &lt;dbl&gt; 18.99554, 30.23761, 41.56107, 42.17789, 43… ## $ MaxWallArea &lt;dbl&gt; 106.20676, 33.36714, 62.27910, 59.76256, 5… ## $ WallVol &lt;dbl&gt; 4192.170, 3917.040, 5814.552, 2489.344, 29… ## $ MaxStenosisByDiameter &lt;dbl&gt; 10.54411, 18.64620, 36.52606, 31.82890, 33… ## $ age &lt;int&gt; 72, 76, 82, 83, 85, 56, 60, 70, 63, 77, 90… ## $ sex &lt;int&gt; 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, … ## $ SmokingHistory &lt;int&gt; 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, … ## $ AtrialFibrillation &lt;int&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, … ## $ CoronaryArteryDisease &lt;int&gt; 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, … ## $ DiabetesHistory &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ HypercholesterolemiaHistory &lt;int&gt; 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, … ## $ HypertensionHistory &lt;int&gt; 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, … # skimr is better (by a lot) # https://github.com/ropensci/skimr skimr::skim(all_stroke) Table 2.1: Data summary Name all_stroke Number of rows 126 Number of columns 29 _______________________ Column type frequency: factor 1 numeric 28 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts Stroke 0 1 FALSE 2 Y: 64, N: 62 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist NASCET 0 1 0.35 0.48 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▅ CALCVol 0 1 199.45 213.49 4.02 77.12 146.76 253.95 1517.54 ▇▂▁▁▁ CALCVolProp 0 1 0.07 0.06 0.00 0.04 0.06 0.10 0.44 ▇▂▁▁▁ MATXVol 0 1 3074.68 738.98 939.34 2525.41 3070.49 3596.27 4821.52 ▁▅▇▇▂ MATXVolProp 0 1 0.77 0.06 0.48 0.75 0.78 0.81 0.87 ▁▁▂▇▇ LRNCVol 0 1 387.00 307.26 15.66 156.85 317.70 508.37 1612.40 ▇▅▂▁▁ LRNCVolProp 0 1 0.11 0.07 0.01 0.06 0.10 0.15 0.49 ▇▅▁▁▁ MaxCALCArea 0 1 20.40 13.13 1.75 10.86 16.54 26.77 63.35 ▇▇▃▁▁ MaxCALCAreaProp 0 1 0.35 0.16 0.01 0.24 0.33 0.42 0.96 ▂▇▅▁▁ MaxDilationByArea 0 1 451.18 870.06 15.53 78.83 150.26 370.28 5920.63 ▇▁▁▁▁ MaxMATXArea 0 1 65.49 58.05 17.19 36.63 45.73 67.48 341.12 ▇▁▁▁▁ MaxMATXAreaProp 0 1 0.95 0.09 0.83 0.94 0.95 0.96 1.94 ▇▁▁▁▁ MaxLRNCArea 0 1 17.62 17.83 1.26 6.82 12.87 20.51 134.34 ▇▂▁▁▁ MaxLRNCAreaProp 0 1 0.33 0.15 0.03 0.20 0.32 0.44 0.74 ▃▇▆▅▁ MaxMaxWallThickness 0 1 5.86 9.23 1.31 3.62 4.72 5.51 83.56 ▇▁▁▁▁ MaxRemodelingRatio 0 1 4.75 4.48 1.21 2.31 3.22 5.35 27.88 ▇▁▁▁▁ MaxStenosisByArea 0 1 74.80 19.67 19.00 61.81 79.34 90.00 100.00 ▁▃▃▇▇ MaxWallArea 0 1 89.94 74.97 19.97 53.21 66.67 92.80 454.17 ▇▂▁▁▁ WallVol 0 1 4156.53 1083.81 1416.75 3464.17 4056.41 5066.84 6306.81 ▁▅▇▆▃ MaxStenosisByDiameter 0 1 63.76 22.64 10.54 47.63 63.42 78.97 100.00 ▁▆▇▅▆ age 0 1 72.10 10.83 39.00 65.00 72.00 80.00 90.00 ▁▂▇▇▇ sex 0 1 0.56 0.50 0.00 0.00 1.00 1.00 1.00 ▆▁▁▁▇ SmokingHistory 0 1 0.58 0.50 0.00 0.00 1.00 1.00 1.00 ▆▁▁▁▇ AtrialFibrillation 0 1 0.12 0.33 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▁ CoronaryArteryDisease 0 1 0.28 0.45 0.00 0.00 0.00 1.00 1.00 ▇▁▁▁▃ DiabetesHistory 0 1 0.22 0.42 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ HypercholesterolemiaHistory 0 1 0.53 0.50 0.00 0.00 1.00 1.00 1.00 ▇▁▁▁▇ HypertensionHistory 0 1 0.77 0.42 0.00 1.00 1.00 1.00 1.00 ▂▁▁▁▇ The book says original dataset has 4 missing data points, but already been median imputed via recipes::step_impute_median() imputation methhods discussed more in Chapter 8, some models hate missing data if you have missing data… use naniar/visdat to visualize datasets and understand where you’re missing data getting-started-w-naniar visdat::vis_dat(tidy_training) # https://bookdown.org/max/FES/numeric-one-to-one.html#numeric-one-to-one # this is a plot of the distribution of MaxLRNCArea, very right skewed fig_2_2_a &lt;- all_stroke %&gt;% ggplot(aes(x = MaxLRNCArea)) + geom_histogram(bins = 15, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;MaxLRNCArea&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(a)&quot;) + theme_bw() fig_2_2_a # same plot but after yeojohnson transformmation, become normal-like fig_2_2_b &lt;- recipe(Stroke ~ ., data = all_stroke) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% prep(.) %&gt;% bake(., new_data = NULL) %&gt;% ggplot(aes(x = MaxLRNCArea)) + geom_histogram(bins = 15, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;Transformed MaxLRNCArea&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(b)&quot;) + theme_bw() #sidebyside with patchwork fig_2_2_a + fig_2_2_b 2.2.2 understanding interactions and multicollinearity some models hate correlated traits this only looks at imaging traits, (why is it called risk?) risk_train &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_of(VC_preds)) %&gt;% # center the data step_scale(all_of(VC_preds)) %&gt;% # scale the data step_YeoJohnson(all_of(VC_preds)) %&gt;% # YeoJohnson transform https://recipes.tidymodels.org/reference/step_YeoJohnson.html prep(.) %&gt;% bake(., new_data = NULL) %&gt;% # juice is superseded by bake select(-one_of(c(&quot;Stroke&quot;, &quot;NASCET&quot;, risk_preds))) #select everything but these risk_corr &lt;- cor(risk_train) #make a correlation matrix corrplot(risk_corr, addgrid.col = rgb(0, 0, 0, .05), order = &quot;hclust&quot;) #plot that # you can remove these with step_corr(all_predictors(), threshold = 0.75) %&gt;% risk_train_step_corr &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_of(VC_preds)) %&gt;% # center the data step_scale(all_of(VC_preds)) %&gt;% # scale the data step_YeoJohnson(all_of(VC_preds)) %&gt;% # YeoJohnson transform https://recipes.tidymodels.org/reference/step_YeoJohnson.html step_corr(all_predictors(), threshold = 0.75) %&gt;% # remove &quot;extra&quot; predictors with correlations higher than 0.75 prep(.) %&gt;% bake(., new_data = NULL) %&gt;% # juice is superseded by bake select(-one_of(c(&quot;Stroke&quot;, &quot;NASCET&quot;, risk_preds))) #select everything but these risk_corr_step_corr &lt;- cor(risk_train_step_corr) #make a correlation matrix corrplot(risk_corr_step_corr, addgrid.col = rgb(0, 0, 0, .05), order = &quot;hclust&quot;) #plot that #BUT WE&#39;RE NOT DOING THAT YET! Chapter 3 shows more methods on this. "],["example-2.html", "2.3 Example 2", " 2.3 Example 2 Code for Section 2.4 at https://bookdown.org/max/FES/stroke-tour.html#stroke-exploration Code to compare 2-way interaction models to their main effects model a and b are two models from train() compare_models_1way &lt;- function(a, b, metric = a$metric[1], ...) { mods &lt;- list(a, b) rs &lt;- resamples(mods) diffs &lt;- diff(rs, metric = metric[1], ...) diffs$statistics[[1]][[1]] } risk_preds is contained in the original data file and has the predictor names for the risk related variables 2.3.1 Create a “null model” with no predictors to get baseline performance Compare the models with single predictors to the risk model. These data make https://bookdown.org/max/FES/stroke-tour.html#tab:stroke-strokeRiskAssociations VC_preds and risk_preds contain the predictor names for different sets. one_predictor_res &lt;- data.frame(Predictor = c(VC_preds, risk_preds), Improvement = NA, Pvalue = NA, ROC = NA, stringsAsFactors = FALSE) for (i in 1:nrow(one_predictor_res)) { set.seed(63331) var_mod &lt;- train(Stroke ~ ., data = stroke_train[, c(&quot;Stroke&quot;, one_predictor_res$Predictor[i])], method = &quot;glm&quot;, metric = &quot;ROC&quot;, trControl = ctrl) tmp_diff &lt;- compare_models_1way(var_mod, null_mod, alternative = &quot;greater&quot;) one_predictor_res$ROC[i] &lt;- getTrainPerf(var_mod)[1, &quot;TrainROC&quot;] one_predictor_res$Improvement[i] &lt;- tmp_diff$estimate one_predictor_res$Pvalue[i] &lt;- tmp_diff$p.value } 2.3.2 With Tidymodels attempting this with tidymodels, not exactly the same need to make some sort of resample object to feed workflow_map model_folds &lt;- vfold_cv(tidy_training, v = 10, repeats = 5) # model_boots &lt;- bootstraps(tidy_training, times = 50) #want to bootstrap this? #defining the null model null_class_model &lt;- null_model() %&gt;% set_engine(&quot;parsnip&quot;) %&gt;% set_mode(&quot;classification&quot;) null_model &lt;- workflow_set(preproc = c(Stroke ~ .), models = list(null_mod = null_class_model)) # defining the model lm_model &lt;- logistic_reg( mode = &quot;classification&quot;, # outcome is a classification Stroke Y/N engine = &quot;glm&quot;, #using glm like the example penalty = NULL, mixture = NULL ) # make named list of single variable formulae single_var_formulae &lt;- c(VC_preds, risk_preds) %&gt;% paste0(&quot;Stroke ~ &quot;, .) %&gt;% set_names(., c(VC_preds, risk_preds)) %&gt;% as.list() %&gt;% map(., as.formula) Create the workflow set, all of our models use the same type of model and input data single_var_models &lt;- workflow_set(preproc = single_var_formulae, models = list(lm = lm_model)) all_models &lt;- bind_rows(null_model, single_var_models) # control grid/resamples allow processing of resampled data and parallel processing # here we are asking only to save the predictions control &lt;- control_resamples(save_pred = TRUE) doParallel::registerDoParallel() all_models1 &lt;- all_models %&gt;% #map over each model and its resamples, use the control parameters, and be noisy workflow_map(., resamples = model_folds, control = control, verbose = TRUE) # save(all_models1, file = &quot;data/all_models1.RData&quot;, compress = &quot;xz&quot;) # per model what are the predictions, summmarize over resamples collect_predictions(all_models1, summarize = TRUE) ## # A tibble: 2,492 × 9 ## wflow_id .config preproc model .row Stroke .pred_N .pred_Y .pred…¹ ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 formula_null_mod Preproce… formula null… 1 N 0.495 0.505 Y ## 2 formula_null_mod Preproce… formula null… 2 N 0.478 0.522 Y ## 3 formula_null_mod Preproce… formula null… 3 N 0.495 0.505 Y ## 4 formula_null_mod Preproce… formula null… 4 N 0.475 0.525 Y ## 5 formula_null_mod Preproce… formula null… 5 N 0.485 0.515 Y ## 6 formula_null_mod Preproce… formula null… 6 N 0.498 0.502 Y ## 7 formula_null_mod Preproce… formula null… 7 N 0.488 0.512 Y ## 8 formula_null_mod Preproce… formula null… 8 N 0.485 0.515 Y ## 9 formula_null_mod Preproce… formula null… 9 N 0.491 0.509 Y ## 10 formula_null_mod Preproce… formula null… 10 N 0.482 0.518 Y ## # … with 2,482 more rows, and abbreviated variable name ¹​.pred_class # per model what are the outputs in terms of fit (auc_roc) and accuracy collect_metrics(all_models1) ## # A tibble: 56 × 9 ## wflow_id .config preproc model .metric .esti…¹ mean n std_err ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 formula_null_mod Preproces… formula null… accura… binary 0.378 50 0.0131 ## 2 formula_null_mod Preproces… formula null… roc_auc binary 0.5 50 0 ## 3 CALCVol_lm Preproces… formula logi… accura… binary 0.388 50 0.0183 ## 4 CALCVol_lm Preproces… formula logi… roc_auc binary 0.457 50 0.0291 ## 5 CALCVolProp_lm Preproces… formula logi… accura… binary 0.441 50 0.0196 ## 6 CALCVolProp_lm Preproces… formula logi… roc_auc binary 0.502 50 0.0260 ## 7 MATXVol_lm Preproces… formula logi… accura… binary 0.346 50 0.0177 ## 8 MATXVol_lm Preproces… formula logi… roc_auc binary 0.380 50 0.0277 ## 9 MATXVolProp_lm Preproces… formula logi… accura… binary 0.415 50 0.0222 ## 10 MATXVolProp_lm Preproces… formula logi… roc_auc binary 0.516 50 0.0285 ## # … with 46 more rows, and abbreviated variable name ¹​.estimator # plot the output to show which individual parameters have the most impact autoplot( all_models1, rank_metric = &quot;roc_auc&quot;, # &lt;- how to order models metric = &quot;roc_auc&quot;, # &lt;- which metric to visualize select_best = FALSE # &lt;- one point per workflow ) + geom_text(aes(y = mean - 1/10*mean, label = wflow_id), angle = 90, hjust = 1) + theme(legend.position = &quot;none&quot;) # per model, sort by auc_roc all_models1 %&gt;% rank_results() %&gt;% filter(.metric == &quot;roc_auc&quot;) ## # A tibble: 28 × 9 ## wflow_id .config .metric mean std_err n prepr…¹ model rank ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 MaxRemodelingRatio_lm Prepro… roc_auc 0.682 0.0300 50 formula logi… 1 ## 2 MaxMaxWallThickness_… Prepro… roc_auc 0.669 0.0276 50 formula logi… 2 ## 3 MaxStenosisByArea_lm Prepro… roc_auc 0.651 0.0256 50 formula logi… 3 ## 4 MaxStenosisByDiamete… Prepro… roc_auc 0.640 0.0281 50 formula logi… 4 ## 5 MaxDilationByArea_lm Prepro… roc_auc 0.625 0.0302 50 formula logi… 5 ## 6 MaxLRNCArea_lm Prepro… roc_auc 0.622 0.0281 50 formula logi… 6 ## 7 LRNCVolProp_lm Prepro… roc_auc 0.598 0.0260 50 formula logi… 7 ## 8 MaxMATXArea_lm Prepro… roc_auc 0.595 0.0298 50 formula logi… 8 ## 9 MaxWallArea_lm Prepro… roc_auc 0.591 0.0300 50 formula logi… 9 ## 10 CoronaryArteryDiseas… Prepro… roc_auc 0.574 0.0208 50 formula logi… 10 ## # … with 18 more rows, and abbreviated variable name ¹​preprocessor # Data in table 2.3 # https://bookdown.org/max/FES/stroke-tour.html#tab:stroke-strokeRiskAssociations one_predictor_res %&gt;% dplyr::filter(Predictor %in% risk_preds) %&gt;% arrange(Pvalue) ## Predictor Improvement Pvalue ROC ## 1 CoronaryArteryDisease 0.079000 0.0002957741 0.579000 ## 2 DiabetesHistory 0.066500 0.0003019908 0.566500 ## 3 HypertensionHistory 0.065000 0.0004269919 0.565000 ## 4 age 0.083375 0.0010729715 0.583375 ## 5 AtrialFibrillation 0.044000 0.0013131334 0.544000 ## 6 SmokingHistory -0.009500 0.6520765973 0.490500 ## 7 sex -0.034500 0.9287682162 0.465500 ## 8 HypercholesterolemiaHistory -0.101500 0.9999999044 0.398500 # Figure 2.4 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-vascuCAPAssocations vc_pred &lt;- recipe(Stroke ~ ., data = stroke_train %&gt;% dplyr::select(Stroke, !!!VC_preds)) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% prep(stroke_train %&gt;% dplyr::select(Stroke, !!!VC_preds)) %&gt;% bake(., new_data = NULL) %&gt;% gather(Predictor, value, -Stroke) vc_pred%&gt;%head ## # A tibble: 6 × 3 ## Stroke Predictor value ## &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 N CALCVol 9.51 ## 2 N CALCVol 4.90 ## 3 N CALCVol 7.63 ## 4 Y CALCVol 13.2 ## 5 N CALCVol 6.93 ## 6 N CALCVol 2.27 pred_max &lt;- vc_pred %&gt;% group_by(Predictor) %&gt;% summarize(max_val = max(value)) %&gt;% inner_join(one_predictor_res %&gt;% dplyr::select(Pvalue, Predictor)) %&gt;% mutate( x = 1.5, value = 1.25 * max_val, label = paste0(&quot;p-value: &quot;, format.pval(Pvalue, digits = 2, sci = FALSE, eps = .0001)) ) ## Joining, by = &quot;Predictor&quot; pred_max%&gt;%head ## # A tibble: 6 × 6 ## Predictor max_val Pvalue x value label ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 CALCVol 15.7 0.000872 1.5 19.6 p-value: 0.00087 ## 2 CALCVolProp 0.440 0.000856 1.5 0.550 p-value: 0.00086 ## 3 LRNCVol 23.2 1.00 1.5 29.0 p-value: 1.00000 ## 4 LRNCVolProp 0.495 0.207 1.5 0.618 p-value: 0.20723 ## 5 MATXVol 41681. 1.00 1.5 52101. p-value: 1.00000 ## 6 MATXVolProp 0.870 1.00 1.5 1.09 p-value: 1.00000 new_order &lt;- pred_max$Predictor[order(pred_max$Pvalue)] vc_pred &lt;- vc_pred %&gt;% mutate(Predictor = factor(Predictor, levels = new_order)) pred_max &lt;- pred_max %&gt;% mutate(Predictor = factor(Predictor, levels = new_order)) fig_2_4 &lt;- ggplot(vc_pred, aes(x = Stroke, y = value)) + geom_boxplot() + geom_point(alpha = 0.3, cex = .5) + geom_text(data = pred_max, aes(x = x, label = label), size = 3) + facet_wrap(~Predictor, scales = &quot;free_y&quot;) + ylab(&quot;&quot;) fig_2_4 # Figure 2.5 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-maxRemodelingRatioROC fig_2_5 &lt;- roc_curve(stroke_train, Stroke, MaxRemodelingRatio) %&gt;% # used opposite values ggplot(aes(x = specificity, y = 1-sensitivity)) + geom_abline(alpha = .5, lty = 2) + geom_path() fig_2_5 2.3.3 Interaction exploration Here they create all the pairs of all of the image analysis components there are 171 interactions pairs &lt;- combn(VC_preds, 2) %&gt;% t() %&gt;% as.data.frame(stringsAsFactors = FALSE) %&gt;% mutate( Improvement = NA, Pvalue = NA, ROC = NA ) Run comparisons with caret retained_pairs &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5 &amp; Pvalue &lt;= 0.2) retained_pairs%&gt;%head ## V1 V2 Improvement Pvalue ROC ## 1 MATXVol MaxMaxWallThickness 0.07635 0.001903838 0.65435 ## 2 MATXVol MaxRemodelingRatio 0.11085 0.001345363 0.55150 ## 3 MATXVol MaxStenosisByArea 0.02325 0.086210254 0.60960 ## 4 MATXVol MaxStenosisByDiameter 0.01930 0.197228736 0.61550 ## 5 LRNCVol MaxMATXArea 0.02590 0.152003535 0.55690 ## 6 LRNCVol MaxRemodelingRatio 0.03345 0.119774508 0.59510 # Figure 2.6 # https://bookdown.org/max/FES/stroke-tour.html#fig:stroke-interactionScreening vol_plot &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5) %&gt;% mutate(Term = paste(V1, &quot;by&quot;, V2, &quot;\\nROC:&quot;, round(ROC, 2))) %&gt;% ggplot(aes(x = Improvement, y = -log10(Pvalue))) + xlab(&quot;Improvement&quot;) + geom_point(alpha = .2, aes(size = ROC, text = Term)) vol_plot &lt;- ggplotly(vol_plot, tooltip = &quot;Term&quot;) vol_plot Create interaction formula of things that matter most int_form &lt;- pairs1 %&gt;% dplyr::filter(ROC &gt; 0.5 &amp; Pvalue &lt;= 0.2 &amp; Improvement &gt; 0) %&gt;% mutate(form = paste0(V1, &quot;:&quot;, V2)) %&gt;% pull(form) %&gt;% paste(collapse = &quot;+&quot;) int_form &lt;- paste(&quot;~&quot;, int_form) int_form &lt;- as.formula(int_form) int_form%&gt;%head ## ~MATXVol:MaxMaxWallThickness + MATXVol:MaxRemodelingRatio + MATXVol:MaxStenosisByArea + ## MATXVol:MaxStenosisByDiameter + LRNCVol:MaxMATXArea + LRNCVol:MaxRemodelingRatio + ## MaxCALCAreaProp:MaxMATXAreaProp + MaxCALCAreaProp:MaxRemodelingRatio + ## MaxDilationByArea:MaxMaxWallThickness + MaxDilationByArea:MaxRemodelingRatio + ## MaxMATXAreaProp:MaxLRNCArea + MaxMATXAreaProp:MaxRemodelingRatio + ## MaxMATXAreaProp:MaxWallArea + MaxMaxWallThickness:MaxStenosisByArea + ## MaxMaxWallThickness:WallVol + MaxRemodelingRatio:MaxWallArea + ## MaxRemodelingRatio:WallVol + MaxStenosisByArea:WallVol This part of the script is to work through all of of the potential models: original risk set alone imaging predictors alone risk and imaging predictors together imaging predictors and interactions of imaging predictors, and risk, imaging predictors, and interactions of imaging predictors All the models are run below. risk_train &lt;- stroke_train %&gt;% dplyr::select(one_of(risk_preds), Stroke) risk_train%&gt;%head ## age sex SmokingHistory AtrialFibrillation CoronaryArteryDisease ## 1 72 1 1 0 0 ## 2 76 1 1 0 0 ## 4 72 0 0 0 0 ## 5 61 1 1 0 0 ## 7 65 1 0 0 0 ## 8 64 1 1 0 1 ## DiabetesHistory HypercholesterolemiaHistory HypertensionHistory Stroke ## 1 0 0 1 N ## 2 1 1 1 N ## 4 0 0 0 N ## 5 1 1 1 Y ## 7 0 0 1 N ## 8 0 1 1 N image_train &lt;- stroke_train %&gt;% dplyr::select(one_of(VC_preds), Stroke) image_train%&gt;%head ## CALCVol CALCVolProp MATXVol MATXVolProp LRNCVol LRNCVolProp MaxCALCArea ## 1 235.252599 0.070442702 3156.835 0.7599582 224.87171 0.09108513 12.350494 ## 2 31.433595 0.016164769 3032.861 0.8133063 368.56066 0.13398944 7.130660 ## 4 113.404823 0.038081488 3835.220 0.7825256 321.15893 0.08303659 16.286916 ## 5 780.823789 0.213432061 3518.877 0.7610895 140.51735 0.03206536 63.350869 ## 7 84.055774 0.041383842 2990.273 0.7498691 293.26992 0.07539753 17.583561 ## 8 5.644322 0.002824946 3359.323 0.8492801 55.76888 0.01983567 2.841252 ## MaxCALCAreaProp MaxDilationByArea MaxMATXArea MaxMATXAreaProp MaxLRNCArea ## 1 0.36576842 520.98259 71.24743 0.9523705 21.686815 ## 2 0.21124686 91.72005 27.21084 0.9455539 6.434661 ## 4 0.40881121 270.96930 38.12211 0.9459098 5.705054 ## 5 0.57620574 2270.45120 341.12089 0.9691989 6.046787 ## 7 0.32150685 95.15505 56.57457 0.9213197 7.213451 ## 8 0.07734609 298.42121 33.92709 0.9612049 4.595196 ## MaxLRNCAreaProp MaxMaxWallThickness MaxRemodelingRatio MaxStenosisByArea ## 1 0.4295781 2.409943 5.697931 18.99554 ## 2 0.2815101 2.540334 1.739927 30.23761 ## 4 0.1547786 3.708515 2.831636 33.93906 ## 5 0.1870965 6.115838 15.647750 34.30985 ## 7 0.2169263 3.975168 1.912069 36.59954 ## 8 0.2053274 2.581908 2.181675 40.31766 ## MaxWallArea WallVol MaxStenosisByDiameter Stroke ## 1 106.20676 4192.170 10.54411 N ## 2 33.36714 3917.040 18.64620 N ## 4 55.34671 4935.327 19.73511 N ## 5 426.47858 4909.504 20.28832 Y ## 7 59.82696 4045.053 49.29705 N ## 8 34.79742 3960.832 30.50857 N fiveStats &lt;- function(...) c(twoClassSummary(...), defaultSummary(...)) internal_ctrl = trainControl(method = &quot;none&quot;, classProbs = TRUE, allowParallel = FALSE) lrFuncsNew &lt;- caretFuncs lrFuncsNew$summary &lt;- fiveStats rfeCtrl &lt;- rfeControl(functions = lrFuncsNew, method = &quot;repeatedcv&quot;, repeats = 5, rerank = FALSE, returnResamp = &quot;all&quot;, saveDetails = TRUE, verbose = FALSE) RFE procedure using risk predictors All pair-wise interactions. risk_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_interact(~ all_predictors():all_predictors()) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_int_filtered_rfe &lt;- rfe( risk_int_filtered_recipe, data = risk_train, sizes = 1:36, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) # Main effects risk_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_main_filtered_rfe &lt;- rfe( risk_main_filtered_recipe, data = risk_train, sizes = 1:8, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) RFE procedure using imaging predictors. img_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_interact(int_form) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_int_filtered_rfe &lt;- rfe( img_int_filtered_recipe, data = image_train, sizes = 1:35, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_main_filtered_rfe &lt;- rfe( img_main_filtered_recipe, data = image_train, sizes = 1:19, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_int_filtered_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_interact(int_form) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_int_filtered_rfe &lt;- rfe( both_int_filtered_recipe, data = stroke_train, sizes = 1:44, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_main_filtered_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_corr(all_predictors(), threshold = 0.75) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_main_filtered_rfe &lt;- rfe( both_main_filtered_recipe, data = stroke_train, sizes = 1:28, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) risk_int_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_interact(~ all_predictors():all_predictors()) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_int_rfe &lt;- rfe( risk_int_recipe, data = risk_train, sizes = 1:36, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) risk_main_recipe &lt;- recipe(Stroke ~ ., data = risk_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) risk_main_rfe &lt;- rfe( risk_main_recipe, data = risk_train, sizes = 1:8, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_int_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_interact(int_form) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_int_rfe &lt;- rfe( img_int_recipe, data = image_train, sizes = 1:35, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) img_main_recipe &lt;- recipe(Stroke ~ ., data = image_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) img_main_rfe &lt;- rfe( img_main_recipe, data = image_train, sizes = 1:19, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_int_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_interact(int_form) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_int_rfe &lt;- rfe( both_int_recipe, data = stroke_train, sizes = 1:44, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) both_main_recipe &lt;- recipe(Stroke ~ ., data = stroke_train) %&gt;% step_center(all_predictors()) %&gt;% step_scale(all_predictors()) %&gt;% step_YeoJohnson(all_predictors()) %&gt;% step_zv(all_predictors()) set.seed(63331) both_main_rfe &lt;- rfe( both_main_recipe, data = stroke_train, sizes = 1:28, rfeControl = rfeCtrl, metric = &quot;ROC&quot;, ## train options method = &quot;glm&quot;, trControl = internal_ctrl ) format_data &lt;- function(x, lab, int = FALSE) { dat &lt;- x %&gt;% pluck(&quot;results&quot;) %&gt;% mutate(Predictors = !!lab) %&gt;% dplyr::select(ROC, Variables, Predictors, Variables, Num_Resamples) %&gt;% mutate(Model = &quot;Main Effects&quot;) if (int) dat$Model &lt;- &quot;Interactions&quot; dat } filtered_dat &lt;- bind_rows( format_data(risk_main_filtered_rfe, lab = &quot;Risk Predictors&quot;), format_data(risk_int_filtered_rfe, lab = &quot;Risk Predictors&quot;, TRUE), format_data(img_main_filtered_rfe, lab = &quot;Imaging Predictors&quot;), format_data(img_int_filtered_rfe, lab = &quot;Imaging Predictors&quot;, TRUE), format_data(both_main_filtered_rfe, lab = &quot;All Predictors&quot;), format_data(both_int_filtered_rfe, lab = &quot;All Predictors&quot;, TRUE) ) %&gt;% mutate( Predictors = factor( Predictors, levels = c(&quot;Risk Predictors&quot;, &quot;Imaging Predictors&quot;, &quot;All Predictors&quot;) ), Model = factor(Model, levels = c(&quot;Main Effects&quot;, &quot;Interactions&quot;)), Filtering = &quot;Correlation Filter&quot; ) unfiltered_dat &lt;- bind_rows( format_data(risk_main_rfe, lab = &quot;Risk Predictors&quot;), format_data(risk_int_rfe, lab = &quot;Risk Predictors&quot;, TRUE), format_data(img_main_rfe, lab = &quot;Imaging Predictors&quot;), format_data(img_int_rfe, lab = &quot;Imaging Predictors&quot;, TRUE), format_data(both_main_rfe, lab = &quot;All Predictors&quot;), format_data(both_int_rfe, lab = &quot;All Predictors&quot;, TRUE) ) %&gt;% mutate( Predictors = factor( Predictors, levels = c(&quot;Risk Predictors&quot;, &quot;Imaging Predictors&quot;, &quot;All Predictors&quot;) ), Model = factor(Model, levels = c(&quot;Main Effects&quot;, &quot;Interactions&quot;)), Filtering = &quot;No Filter&quot; ) rfe_data &lt;- bind_rows(filtered_dat, unfiltered_dat) %&gt;% mutate( Filtering = factor(Filtering, levels = c(&quot;No Filter&quot;, &quot;Correlation Filter&quot;)) ) # https://bookdown.org/max/FES/predictive-modeling-across-sets.html#fig:stroke-rfe-res ggplot(rfe_data, aes(x = Variables, y = ROC, col = Model)) + geom_point(size = 0.75) + geom_line() + facet_grid(Filtering ~ Predictors) + scale_color_manual(values = c(&quot;#6A3D9A&quot;, &quot;#CAB2D6&quot;)) # https://bookdown.org/max/FES/predictive-modeling-across-sets.html#tab:stroke-rfe-tab rfe_tab &lt;- img_main_filtered_rfe %&gt;% pluck(&quot;variables&quot;) %&gt;% filter(Variables == img_main_filtered_rfe$optsize) %&gt;% group_by(var) %&gt;% count() %&gt;% arrange(desc(n)) %&gt;% mutate(final = ifelse(var %in% img_main_filtered_rfe$optVariables, &quot;Yes&quot;, &quot;No&quot;)) %&gt;% ungroup() Meeting Videos 2.3.4 Cohort 1 Meeting chat log LOG "],["a-review-of-the-predictive-modeling-process.html", "Chapter 3 A Review of the Predictive Modeling Process", " Chapter 3 A Review of the Predictive Modeling Process Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1.html", "3.1 SLIDE 1", " 3.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-1.html", "3.2 Meeting Videos", " 3.2 Meeting Videos 3.2.1 Cohort 1 Meeting chat log LOG "],["exploratory-visualizations.html", "Chapter 4 Exploratory Visualizations", " Chapter 4 Exploratory Visualizations Learning objectives: Perform exploratory data visualization for the Chicago train ridership and OKCupid datasets. Perform univariate and bivariate visualizations for numerical variables. Perform visualizations for categorical variables. Perform post-modeling visualizations. "],["data-visualization-chart.html", "4.1 Data Visualization Chart", " 4.1 Data Visualization Chart Source: Exploratory Data Analysis for Feature Selection in Machine Learning - Google Cloud Another reference for data visualization using R Data Visualization with R "],["introduction-to-the-chicago-train-ridership-dataset.html", "4.2 Introduction to the Chicago Train Ridership Dataset", " 4.2 Introduction to the Chicago Train Ridership Dataset Source: Wikimedia Commons, Creative Commons license Our interest is predicting the ridership at the Clark/Lake in the Chicago Loop. Source: Google Maps "],["chicago-train-ridership-dataset.html", "4.3 Chicago Train Ridership dataset", " 4.3 Chicago Train Ridership dataset library(tidyverse) library(lubridate) library(plotly) library(patchwork) library(here) ## here() starts at /home/runner/work/bookclub-feat_eng/bookclub-feat_eng library(heatmaply) ## Loading required package: viridis ## Loading required package: viridisLite ## ## Attaching package: &#39;viridis&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## viridis_pal ## ## ====================== ## Welcome to heatmaply version 1.4.0 ## ## Type citation(&#39;heatmaply&#39;) for how to cite the package. ## Type ?heatmaply for the main documentation. ## ## The github page is: https://github.com/talgalili/heatmaply/ ## Please submit your suggestions and bug-reports at: https://github.com/talgalili/heatmaply/issues ## You may ask questions at stackoverflow, use the r and heatmaply tags: ## https://stackoverflow.com/questions/tagged/heatmaply ## ====================== library(RColorBrewer) library(skimr) library(vcd) ## Loading required package: grid library(colorspace) library(FactoMineR) library(caret) load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Chicago_trains/chicago.RData?raw=true&quot;)) load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/Chicago_trains/stations.RData?raw=true&quot;)) Create train_plot_data train_plot_data &lt;- training %&gt;% mutate(date = train_days) %&gt;% relocate(date, .before = everything()) train_plot_data ## # A tibble: 5,698 × 1,092 ## date s_40380 dow doy week month year Advent1st Advent2nd Advent…¹ ## &lt;date&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2001-01-22 15.7 Mon 22 4 Jan 2001 0 0 0 ## 2 2001-01-23 15.8 Tue 23 4 Jan 2001 0 0 0 ## 3 2001-01-24 15.9 Wed 24 4 Jan 2001 0 0 0 ## 4 2001-01-25 15.9 Thu 25 4 Jan 2001 0 0 0 ## 5 2001-01-26 15.4 Fri 26 4 Jan 2001 0 0 0 ## 6 2001-01-27 2.42 Sat 27 4 Jan 2001 0 0 0 ## 7 2001-01-28 1.47 Sun 28 4 Jan 2001 0 0 0 ## 8 2001-01-29 15.5 Mon 29 5 Jan 2001 0 0 0 ## 9 2001-01-30 15.9 Tue 30 5 Jan 2001 0 0 0 ## 10 2001-01-31 15.9 Wed 31 5 Jan 2001 0 0 0 ## # … with 5,688 more rows, 1,082 more variables: Advent4th &lt;dbl&gt;, ## # AllSaints &lt;dbl&gt;, AllSouls &lt;dbl&gt;, Annunciation &lt;dbl&gt;, Ascension &lt;dbl&gt;, ## # AshWednesday &lt;dbl&gt;, AssumptionOfMary &lt;dbl&gt;, BirthOfVirginMary &lt;dbl&gt;, ## # BoxingDay &lt;dbl&gt;, CaRemembranceDay &lt;dbl&gt;, CelebrationOfHolyCross &lt;dbl&gt;, ## # ChristmasEve &lt;dbl&gt;, ChristTheKing &lt;dbl&gt;, CorpusChristi &lt;dbl&gt;, Easter &lt;dbl&gt;, ## # EasterMonday &lt;dbl&gt;, EasterSunday &lt;dbl&gt;, Epiphany &lt;dbl&gt;, ## # MassOfArchangels &lt;dbl&gt;, PalmSunday &lt;dbl&gt;, Pentecost &lt;dbl&gt;, … "],["preliminary-exploratory-visualizations.html", "4.4 Preliminary exploratory visualizations", " 4.4 Preliminary exploratory visualizations Ridership line plot by month g1 &lt;- train_plot_data %&gt;% select(date, rides = s_40380) %&gt;% mutate(date = floor_date(date, &quot;month&quot;)) %&gt;% arrange(date) %&gt;% group_by(date) %&gt;% summarise(rides = sum(rides), .groups = &#39;drop&#39;) %&gt;% ggplot(aes(date, rides)) + geom_line(size = 1) + geom_smooth(method = &#39;loess&#39;, se = FALSE, color = &#39;steelblue&#39;) + scale_x_date(date_labels = &quot;%b-%Y&quot;, date_breaks =&quot;2 year&quot;)+ labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Monthly Ridership Volume (Jan 2001 - Aug 2016)&#39; ) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) ggplotly(g1) ## `geom_smooth()` using formula &#39;y ~ x&#39; Boxplot rides by day of the week g2 &lt;- train_plot_data %&gt;% select(dow, rides = s_40380) %&gt;% ggplot(aes(dow, rides, fill = dow)) + geom_boxplot() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Ridership by Day of the Week&#39;) + theme(legend.position = &#39;none&#39;) ggplotly(g2) Violinplot rides by day of the week train_plot_data %&gt;% select(dow, rides = s_40380) %&gt;% ggplot(aes(dow, rides, fill = dow)) + geom_violin() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Station Ridership by Day of the Week&#39;) + theme(legend.position = &#39;none&#39;) Boxplot rides by month g3 &lt;- train_plot_data %&gt;% select(month, rides = s_40380) %&gt;% ggplot(aes(month, rides, fill = month)) + geom_boxplot() + labs(x = &#39;&#39;, y = &quot;Rides (000&#39;s)&quot;, title = &#39;Chicago Clark/Lake Train Monthly Station Ridership&#39;) + theme(legend.position = &#39;none&#39;) ggplotly(g3) "],["visualizations-for-numeric-data.html", "4.5 Visualizations for Numeric Data", " 4.5 Visualizations for Numeric Data 4.5.1 Box Plots, Violin Plots, and Histograms Understanding the distribution of the response g4 &lt;- train_plot_data %&gt;% ggplot(aes(x = &quot;&quot;, y = s_40380)) + geom_boxplot(fill = &quot;blue&quot;, alpha = 0.5) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + theme( axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2, 29) ggplotly(g4) y_hist &lt;- ggplot(train_plot_data, aes(s_40380)) + geom_histogram(binwidth = .7, col = &quot;#D53E4F&quot;, fill = &quot;#D53E4F&quot;, alpha = .5) + xlab(&quot;Clark/Lake Rides (x1000)&quot;) + ylab(&quot;Frequency&quot;) + ggtitle(&quot;(a)&quot;) + xlim(-2,29) + theme( axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) y_box &lt;- ggplot(train_plot_data, aes(x = &quot;&quot;, y = s_40380)) + geom_boxplot(alpha = 0.2) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + ggtitle(&quot;(b)&quot;) + theme( axis.title.x = element_blank(), axis.text.x = element_blank(), axis.ticks.x = element_blank(), axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2,29) y_violin &lt;- ggplot(train_plot_data, aes(x = &quot;&quot;, y = s_40380)) + geom_violin(alpha = 0.2) + ylab(&quot;Clark/Lake Rides (x1000)&quot;) + ggtitle(&quot;(c)&quot;) + theme( axis.title.y = element_blank(), axis.text.y = element_blank(), axis.ticks.y = element_blank() ) + coord_flip() + ylim(-2,29) y_hist / y_box / y_violin ## Warning: Removed 2 rows containing missing values (geom_bar). The histogram (a) allows us to see that there are two peaks or modes in ridership distribution indicating that there may be two mechanisms affecting ridership. The box plot (b) does not have the ability to see multiple peaks in the data. However the violin plot (c) provides a compact visualization that identifies the distributional nuance. 4.5.2 Augmenting Visualizations through Faceting, Colors, and Shapes Distribution of daily ridership at the Clark/Lake stop from 2001 to 2016 colored and faceted by weekday and weekend. l10_breaks &lt;- scales::trans_breaks(&quot;log10&quot;, function(x) 10^x) l10_labels &lt;- scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)) all_pred %&gt;% mutate(pow = as.factor(ifelse(dow %in% c(&quot;Sat&quot;,&quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;))) %&gt;% ggplot(aes(s_40380 * 1000, fill = pow, col = pow)) + facet_wrap( ~ pow, nrow = 2, scales = &quot;free_y&quot;) + geom_histogram(binwidth = .03, alpha = .5) + scale_fill_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + scale_x_log10(breaks = l10_breaks, labels = l10_labels) + xlab(&quot;Clark/Lakes Rides&quot;) + ylab(&quot;Frequency&quot;) + theme(legend.position=&quot;none&quot;) 4.5.3 Scatterplots Scatterplots are numeric-to-numeric visualizations. Example: A scatter plot of the 14-day lag ridership at the Clark/Lake station and the current-day ridership at the same station. # create &#39;pow&#39; (weekend/weekday flag) train_plot_data &lt;- train_plot_data %&gt;% mutate( pow = ifelse(dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;), pow = factor(pow) ) train_plot_data %&gt;% select(l14_40380, s_40380, pow) %&gt;% ggplot(aes(l14_40380,s_40380, col = pow)) + geom_point(alpha=0.5) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + xlab(&quot;Two-week Lag in Ridership (x1000)&quot;) + ylab(&quot;Current Day Ridership (x1000)&quot;) + theme(legend.title=element_blank()) + coord_equal() In general, there is a strong linear relationship between the 14-day lag and current-day ridership. However, there are many 14-day lag/current day pairs of days that lie far off from the overall scatter of points. 4.5.4 Scatterplots - Exclude U.S. holidays Let’s filter major U.S. holidays from the train_plot_data. commonHolidays &lt;- c(&quot;USNewYearsDay&quot;, &quot;Jan02_Mon_Fri&quot;, &quot;USMLKingsBirthday&quot;, &quot;USPresidentsDay&quot;, &quot;USMemorialDay&quot;, &quot;USIndependenceDay&quot;, &quot;Jul03_Mon_Fri&quot;, &quot;Jul05_Mon_Fri&quot;, &quot;USLaborDay&quot;, &quot;USThanksgivingDay&quot;, &quot;Day_after_Thx&quot;, &quot;ChristmasEve&quot;, &quot;USChristmasDay&quot;, &quot;Dec26_wkday&quot;, &quot;Dec31_Mon_Fri&quot;) any_holiday &lt;- train_plot_data %&gt;% dplyr::select(date, !!commonHolidays) %&gt;% gather(holiday, value, -date) %&gt;% group_by(date) %&gt;% summarize(common_holiday = max(value)) %&gt;% ungroup() %&gt;% mutate(common_holiday = ifelse(common_holiday == 1, &quot;Holiday&quot;, &quot;Non-holiday&quot;)) %&gt;% inner_join(train_plot_data, by = &quot;date&quot;) holiday_values &lt;- any_holiday %&gt;% dplyr::select(date, common_holiday) make_lag &lt;- function(x, lag = 14) { x$date &lt;- x$date + days(lag) prefix &lt;- ifelse(lag &lt; 10, paste0(&quot;0&quot;, lag), lag) prefix &lt;- paste0(&quot;l&quot;, prefix, &quot;_holiday&quot;) names(x) &lt;- gsub(&quot;common_holiday&quot;, prefix, names(x)) x } lag_hol &lt;- make_lag(holiday_values, lag = 14) holiday_data &lt;- any_holiday %&gt;% left_join(lag_hol, by = &quot;date&quot;) %&gt;% mutate( year = factor(year), l14_holiday = ifelse(is.na(l14_holiday), &quot;Non-holiday&quot;, l14_holiday) ) no_holiday_plot &lt;- holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot; &amp; l14_holiday == &quot;Non-holiday&quot;) %&gt;% ggplot(aes(l14_40380, s_40380, col = pow)) + geom_point(alpha=0.5) + scale_color_manual(values = c(&quot;#D53E4F&quot;, &quot;#3ed5c4&quot;)) + xlab(&quot;14-Day Lag&quot;) + ylab(&quot;Current Day&quot;) + theme(legend.title=element_blank())+ coord_equal() no_holiday_plot Filtering the holidays, the weekday scatterplot compactness improves (less outliers scattered on both sides of the cluster). 4.5.5 Heatmaps For the ridership data, we will create a month and day predictor, a year predictor, and an indicator of weekday ridership less than 10,000 rides. heatmap_data &lt;- all_pred %&gt;% mutate(pow = as.factor(ifelse(dow %in% c(&quot;Sat&quot;,&quot;Sun&quot;), &quot;Weekend&quot;, &quot;Weekday&quot;))) %&gt;% dplyr::select(date, s_40380, pow) %&gt;% mutate( mmdd = format(as.Date(date), &quot;%m-%d&quot;), yyyy = format(as.Date(date), &quot;%Y&quot;), lt10 = ifelse(s_40380 &lt; 10 &amp; pow==&quot;Weekday&quot;, 1, 0) ) # U.S. holidays break_vals &lt;- c(&quot;01-01&quot;,&quot;01-15&quot;,&quot;02-01&quot;,&quot;02-15&quot;,&quot;03-01&quot;,&quot;03-15&quot;,&quot;04-01&quot;, &quot;04-15&quot;,&quot;05-01&quot;,&quot;05-15&quot;,&quot;06-01&quot;,&quot;06-15&quot;, &quot;07-01&quot;,&quot;07-15&quot;, &quot;08-01&quot;, &quot;08-15&quot;,&quot;09-01&quot;,&quot;09-15&quot;,&quot;10-01&quot;,&quot;10-15&quot;,&quot;11-01&quot;, &quot;11-15&quot;,&quot;12-01&quot;,&quot;12-15&quot;) heatmap_data %&gt;% ggplot(aes(yyyy, mmdd)) + geom_tile(aes(fill = lt10), colour = &quot;white&quot;) + scale_fill_gradient(low = &quot;transparent&quot;, high = &quot;red&quot;) + scale_y_discrete( breaks = break_vals ) + xlab(&quot;Year&quot;) + ylab(&quot;Month &amp; Day&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) This visualization indicates that the distinct patterns of low ridership on weekdays occur on and around major US holidays. 4.5.6 Correlation Matrix Plots Visualization of the correlation matrix of the 14-day lag ridership station predictors for non-holiday, weekdays in 2016. cor_mat &lt;- holiday_data %&gt;% dplyr::filter(year == &quot;2016&quot;) %&gt;% dplyr::select(matches(&quot;l14_[0-9]&quot;), pow, common_holiday) %&gt;% dplyr::filter(pow == &quot;Weekday&quot; &amp; common_holiday == &quot;Non-holiday&quot;) %&gt;% dplyr::select(-pow, -common_holiday) %&gt;% cor() cor_map &lt;- heatmaply_cor( cor_mat, symm = TRUE, cexRow = .0001, cexCol = .0001, branches_lwd = .1 ) cor_map Ridership across stations is positively correlated (red) for nearly all pairs of stations. This means that low ridership at one station corresponds to relatively low ridership at another station, and high ridership at one station corresponds to relatively high ridership at another station. For feature selection, the high degree of correlation is a clear indicator that the information present across the stations is redundant and could be eliminated or reduced. 4.5.7 Line plots Monthly average ridership per year by weekday (excluding holidays) or weekend. year_cols &lt;- colorRampPalette(colors = brewer.pal(n = 9, &quot;YlOrRd&quot;)[-1])(16) holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot;) %&gt;% dplyr::mutate(year = factor(year)) %&gt;% group_by( month = lubridate::month(date, label = TRUE, abbr = TRUE), year, pow ) %&gt;% dplyr::summarize(average_ridership = mean(s_40380, na.rm = TRUE)) %&gt;% ggplot(aes(month, average_ridership)) + facet_wrap( ~ pow, ncol = 2) + geom_line(aes(group = year, col = year), size = 1.1) + xlab(&quot;&quot;) + ylab(&quot;Geometric Mean Ridership (x1000)&quot;) + scale_color_manual(values = year_cols) + guides( col = guide_legend( title = &quot;&quot;, nrow = 2, byrow = TRUE ) ) + theme(legend.position = &quot;top&quot;) ## `summarise()` has grouped output by &#39;month&#39;, &#39;year&#39;. You can override using the ## `.groups` argument. Weekend ridership also shows annual trends but exhibits more variation within the trends for some years. The Weekend line plots have the highest variation during 2008, with much higher ridership in the summer. Monthly average gas price per gallon (USD) per year. holiday_data %&gt;% dplyr::filter(common_holiday == &quot;Non-holiday&quot;) %&gt;% mutate(year = factor(year)) %&gt;% group_by( month = lubridate::month(date, label = TRUE, abbr = TRUE), year ) %&gt;% dplyr::summarize(average_l14_gas_price = mean(l14_gas_price, na.rm = TRUE)) %&gt;% ggplot(aes(x = month, y = average_l14_gas_price)) + geom_line(aes(group = year, col = year), size = 1.3) + xlab(&quot;&quot;) + ylab(&quot;Average Gas Price/Gallon ($)&quot;) + scale_color_manual(values = year_cols) + guides( col = guide_legend( title = &quot;&quot;, nrow = 2, byrow = TRUE ) ) + theme(legend.position = &quot;top&quot;) ## `summarise()` has grouped output by &#39;month&#39;. You can override using the ## `.groups` argument. Prices spike in the summer of 2008, which is at the same time that weekend ridership spikes. 4.5.8 Principal Component Analysis (PCA) Principal component analysis of the 14-day station lag ridership. lag_14_data &lt;- holiday_data %&gt;% dplyr::select(matches(&quot;l14_[0-9]&quot;)) PCA_station &lt;- prcomp(lag_14_data) var_explained &lt;- c(0, PCA_station$sdev ^ 2) cumulative_var &lt;- cumsum(var_explained) pct_var_explained &lt;- 100 * cumulative_var / max(cumulative_var) var_df &lt;- tibble( Component = seq_along(pct_var_explained) - 1, pct_var_explained = pct_var_explained ) score_data &lt;- tibble( y = holiday_data$s_40380, year = factor(holiday_data$year), pow = holiday_data$pow, PC1 = PCA_station$x[, 1], PC2 = PCA_station$x[, 2], dow = holiday_data$dow ) pca_rng &lt;- extendrange(c(score_data$PC1, score_data$PC2)) var_plot &lt;- var_df %&gt;% dplyr::filter(Component &lt;= 50) %&gt;% ggplot(aes(x = Component, y = pct_var_explained)) + geom_line(size = 1.3) + ylim(0, 100) + xlab(&quot;Component&quot;) + ylab(&quot;Percent Variance Explained&quot;) + ggtitle(&quot;(a)&quot;) score_plot12 &lt;- ggplot(score_data, aes(PC1,PC2)) + geom_point(size = 1, alpha = 0.25) + xlab(&quot;Component 1&quot;) + ylab(&quot;Component 2&quot;) + xlim(pca_rng) + ylim(pca_rng) + ggtitle(&quot;(b)&quot;) score1_vs_day &lt;- ggplot(score_data, aes(x = dow, y = PC1)) + geom_violin(adjust = 1.5) + ylab(&quot;Component 1&quot;) + xlab(&quot;&quot;) + ylim(pca_rng) + ggtitle(&quot;(c)&quot;) score2_vs_year &lt;- ggplot(score_data, aes(x = year, y = PC2, col = year)) + geom_violin(adjust = 1.5) + ylab(&quot;Component 2&quot;) + xlab(&quot;&quot;) + # ylim(pca_rng) + scale_color_manual(values = year_cols)+ theme(legend.position = &quot;none&quot;) + ggtitle(&quot;(d)&quot;) (var_plot + score_plot12) / score1_vs_day / score2_vs_year The cumulative variability summarized across the first 10 components (a). A scatter plot of the first two principal components. The first component focuses on variation due to part of the week while the second component focuses on variation due to time (year) (b). The relationship between the first principal component and ridership for each day of the week at the Clark/Lake station (c). The relationship between second principal component and ridership for each year at the Clark/Lake station (d). "],["visualizations-for-categorical-data-exploring-the-okcupid-dataset.html", "4.6 Visualizations for Categorical Data: Exploring the OKCupid dataset", " 4.6 Visualizations for Categorical Data: Exploring the OKCupid dataset OkCupid is an online dating site that serves international users. Kim and Escobedo-Land (2015) describe a data set where over 50,000 profiles from the San Francisco area. load(url(&quot;https://github.com/topepo/FES/blob/master/Data_Sets/OkCupid/okc.RData?raw=true&quot;)) First look at the dataset # bind &#39;okc_test&#39; okc &lt;- okc_train %&gt;% bind_rows(okc_test) Skim okc skimr::skim(okc) %&gt;% knitr::kable() skim_type skim_variable n_missing complete_rate factor.ordered factor.n_unique factor.top_counts numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist factor diet 0 1 FALSE 19 die: 19691, mos: 15229, any: 5440, str: 4583 NA NA NA NA NA NA NA NA factor drinks 0 1 FALSE 7 soc: 36756, rar: 5328, oft: 4531, not: 2868 NA NA NA NA NA NA NA NA factor drugs 0 1 FALSE 4 nev: 32805, dru: 11758, som: 6818, oft: 366 NA NA NA NA NA NA NA NA factor education 0 1 FALSE 33 gra: 21423, gra: 8151, wor: 5193, ed_: 3572 NA NA NA NA NA NA NA NA factor income 0 1 FALSE 13 mis: 40584, inc: 2876, inc: 1572, inc: 1080 NA NA NA NA NA NA NA NA factor offspring 0 1 FALSE 16 kid: 29360, doe: 6744, doe: 3653, doe: 3322 NA NA NA NA NA NA NA NA factor pets 0 1 FALSE 16 pet: 15181, lik: 13642, lik: 6534, lik: 3988 NA NA NA NA NA NA NA NA factor religion 0 1 FALSE 10 rel: 15333, agn: 8043, oth: 7231, ath: 6316 NA NA NA NA NA NA NA NA factor sign 0 1 FALSE 13 sig: 7818, leo: 3922, gem: 3911, can: 3784 NA NA NA NA NA NA NA NA factor smokes 0 1 FALSE 6 no: 38941, smo: 3619, som: 3268, whe: 2688 NA NA NA NA NA NA NA NA factor status 0 1 FALSE 5 sin: 48032, see: 1814, ava: 1624, mar: 269 NA NA NA NA NA NA NA NA factor where_state 0 1 FALSE 36 cal: 51672, new: 15, ill: 6, mas: 4 NA NA NA NA NA NA NA NA factor where_town 0 1 FALSE 51 san: 26683, oak: 6214, ber: 3616, san: 1168 NA NA NA NA NA NA NA NA factor religion_modifer 0 1 FALSE 5 rel: 25718, but: 11469, and: 8309, and: 4221 NA NA NA NA NA NA NA NA factor sign_modifer 0 1 FALSE 4 sig: 17909, and: 17709, but: 15511, and: 618 NA NA NA NA NA NA NA NA factor Class 0 1 FALSE 2 oth: 42190, ste: 9557 NA NA NA NA NA NA NA NA numeric age 0 1 NA NA NA 3.255509e+01 9.518919e+00 18 26.000000 30.000000 37.000000 109.000000 ▇▂▁▁▁ numeric height 0 1 NA NA NA 6.833121e+01 3.979818e+00 1 66.000000 68.000000 71.000000 95.000000 ▁▁▁▇▁ numeric last_online 0 1 NA NA NA 3.916536e+01 7.625808e+01 0 1.000000 4.000000 30.000000 370.000000 ▇▁▁▁▁ numeric cpp 0 1 NA NA NA 2.898700e-03 5.376220e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_fluently 0 1 NA NA NA 1.252250e-02 1.112020e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_okay 0 1 NA NA NA 9.855600e-03 9.878610e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric cpp_poorly 0 1 NA NA NA 7.382100e-03 8.560210e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp 0 1 NA NA NA 5.991000e-04 2.446880e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_fluently 0 1 NA NA NA 1.488000e-03 3.854640e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_okay 0 1 NA NA NA 2.319000e-03 4.810030e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric lisp_poorly 0 1 NA NA NA 2.280300e-03 4.769870e-02 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric asian 0 1 NA NA NA 1.371094e-01 3.439661e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric black 0 1 NA NA NA 5.637040e-02 2.306379e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric hispanic_latin 0 1 NA NA NA 8.937720e-02 2.852901e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric indian 0 1 NA NA NA 2.467780e-02 1.551426e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric middle_eastern 0 1 NA NA NA 1.571110e-02 1.243564e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric native_american 0 1 NA NA NA 2.125730e-02 1.442422e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric other 0 1 NA NA NA 6.083440e-02 2.390287e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric pacific_islander 0 1 NA NA NA 2.475510e-02 1.553793e-01 0 0.000000 0.000000 0.000000 1.000000 ▇▁▁▁▁ numeric white 0 1 NA NA NA 6.474578e-01 4.777663e-01 0 0.000000 1.000000 1.000000 1.000000 ▅▁▁▁▇ numeric essay_length 0 1 NA NA NA 3.132866e+00 6.777757e-01 0 3.009026 3.269513 3.480438 4.983486 ▁▁▂▇▁ numeric profile 0 1 NA NA NA 2.587400e+04 1.493822e+04 1 12937.500000 25874.000000 38810.500000 51747.000000 ▇▇▇▇▇ Plot Class okc %&gt;% ggplot(aes(Class, fill = Class)) + geom_bar() + theme(legend.position = &#39;none&#39;) 4.6.1 Visualizing Relationships between Outcomes and Predictors 4.6.1.1 Outcome and Categorical Predictor Let’s plot the frequency of the stated religion, partitioned and color by the outcome (Class). binom_stats &lt;- function(x, ...) { x &lt;- x$Class[!is.na(x$Class)] res &lt;- prop.test(x = sum(x == &quot;stem&quot;), n = length(x), ...) data.frame(Proportion = unname(res$estimate), Lower = res$conf.int[1], Upper = res$conf.int[2]) } stem_rate &lt;- mean(okc_train$Class == &quot;stem&quot;) religion_rates &lt;- okc_train %&gt;% group_by(religion) %&gt;% do(binom_stats(.)) %&gt;% arrange(Proportion) %&gt;% ungroup() %&gt;% mutate(religion = gsub(&quot;religion_&quot;, &quot;&quot;, religion), religion = reorder(factor(religion), Proportion)) okc_train &lt;- okc_train %&gt;% mutate( religion2 = gsub(&quot;religion_&quot;, &quot;&quot;, as.character(religion)), religion2 = factor(religion2, levels = as.character(religion_rates$religion)) ) bars &lt;- ggplot(okc_train, aes(x = religion2, fill = Class)) + geom_bar(position = position_dodge()) + scale_fill_brewer(palette = &quot;Paired&quot;) + xlab(&quot;&quot;) + theme(legend.position = &quot;top&quot;, axis.text = element_text(size = 8)) + ggtitle(&quot;(a)&quot;) stacked_vars &lt;- ggplot(okc_train, aes(x = religion2, fill = Class)) + geom_bar(position = &quot;fill&quot;) + scale_fill_brewer(palette = &quot;Paired&quot;) + xlab(&quot;&quot;) + ylab(&quot;Proportion&quot;) + theme(legend.position = &quot;none&quot;, axis.text = element_text(size = 8)) + ggtitle(&quot;(b)&quot;) ci_plots &lt;- ggplot(religion_rates, aes(x = religion, y = Proportion)) + geom_hline(yintercept = stem_rate, col = &quot;red&quot;, alpha = .35, lty = 2) + geom_point() + geom_errorbar(aes(ymin = Lower, ymax = Upper), width = .1) + theme(axis.text = element_text(size = 8)) + xlab(&quot;&quot;) + ggtitle(&quot;(c)&quot;) bars / stacked_vars / ci_plots Does religion appear to be related to the outcome? Since there is a gradation of rates of STEM professions between the groups, it would appear so. 4.6.1.2 Outcome and Numerical Predictor Now, let’s visualize the relationship between a categorical outcome (Class) and a numerical predictor (essay_length). l10_breaks &lt;- scales::trans_breaks(&quot;log10&quot;, function(x) 10^x) l10_labels &lt;- scales::trans_format(&quot;log10&quot;, scales::math_format(10^.x)) gam_dat &lt;- okc_train %&gt;% dplyr::select(essay_length, Class) %&gt;% arrange(essay_length) gam_small &lt;- gam_dat %&gt;% distinct(essay_length) gam_mod &lt;- mgcv::gam(Class ~ s(essay_length), data = gam_dat, family = binomial()) gam_small &lt;- gam_small %&gt;% mutate( link = -predict(gam_mod, gam_small, type = &quot;link&quot;), se = predict(gam_mod, gam_small, type = &quot;link&quot;, se.fit = TRUE)$se.fit, upper = link + qnorm(.975) * se, lower = link - qnorm(.975) * se, lower = binomial()$linkinv(lower), upper = binomial()$linkinv(upper), probability = binomial()$linkinv(link) ) brks &lt;- l10_breaks(exp(okc_train$essay_length)) essay_hist &lt;- ggplot(okc_train, aes(x = exp(essay_length))) + geom_histogram(binwidth = .1, col = &quot;#FEB24C&quot;, fill = &quot;#FED976&quot;) + facet_wrap(~ Class, ncol = 1) + scale_x_log10(breaks = brks, labels = l10_labels) + xlab(&quot;Essay Character Length&quot;) + theme_bw() + theme(plot.margin = unit(c(0,1,0,1.2), &quot;cm&quot;)) + ggtitle(&quot;(a)&quot;) essay_gam &lt;- ggplot(gam_small, aes(x = exp(essay_length))) + geom_line(aes(y = probability)) + geom_ribbon(aes(ymin = lower, ymax = upper), fill = &quot;grey&quot;, alpha = .5) + geom_hline(yintercept = stem_rate, col = &quot;red&quot;, alpha = .35, lty = 2) + scale_x_log10(breaks = brks, labels = l10_labels) + theme_bw() + xlab(&quot;&quot;) + theme(plot.margin = unit(c(0,1,0,1.2), &quot;cm&quot;))+ ggtitle(&quot;(b)&quot;) essay_hist / essay_gam The black line represents the class probability of the logistic regression model and the bands denote 95% confidence intervals around the fit. The horizontal red line indicates the baseline probability of STEM profiles from the training set. This predictor might be worth including in a model but is unlikely to show a strong effect on its own. 4.6.2 Exploring Relationships Between Categorical Predictors When considering relationships between categorical data, there are several options. Once a cross-tabulation between variables is created, mosaic plots can once again be used to understand the relationship between variables. okc_train &lt;- okc_train %&gt;% mutate( drugs = factor(as.character(drugs), levels = c(&quot;drugs_missing&quot;, &quot;never&quot;, &quot;sometimes&quot;, &quot;often&quot;)), drinks = factor(as.character(drinks), levels = c(&quot;drinks_missing&quot;, &quot;not_at_all&quot;, &quot;rarely&quot;, &quot;socially&quot;, &quot;often&quot;, &quot;very_often&quot;, &quot;desperately&quot;)) ) dd_tab &lt;- table(okc_train$drugs, okc_train$drinks, dnn = c(&quot;Drugs&quot;, &quot;Alcohol&quot;)) # Formatting for slightly better printing plot_tab &lt;- dd_tab dimnames(plot_tab)[[1]][1] &lt;- &quot;missing&quot; dimnames(plot_tab)[[2]] &lt;- gsub(&quot;_&quot;, &quot; &quot;, dimnames(plot_tab)[[2]]) dimnames(plot_tab)[[2]][1] &lt;- &quot;missing&quot; dimnames(plot_tab)[[2]][6] &lt;- &quot;often\\n&quot; dimnames(plot_tab)[[2]][6] &lt;- &quot;very often&quot; dimnames(plot_tab)[[2]][7] &lt;- &quot;\\ndesperately&quot; mosaic( t(plot_tab), highlighting = TRUE, highlighting_fill = rainbow_hcl, margins = unit(c(6, 1, 1, 8), &quot;lines&quot;), labeling = labeling_border( rot_labels = c(90, 0, 0, 0), just_labels = c(&quot;left&quot;, &quot;right&quot;, &quot;center&quot;, &quot;right&quot;), offset_varnames = unit(c(3, 1, 1, 4), &quot;lines&quot;) ), keep_aspect_ratio = FALSE ) In the cross-tabulation between alcohol and drug use, the χ2 statistic is very large (4043.8) for its degrees of freedom (18) and is associated with a very small p-value (0). This indicates that there is a strong association between these two variables. ca_obj &lt;- CA(dd_tab, graph = FALSE) ca_drugs &lt;- as.data.frame(ca_obj$row$coord) ca_drugs$label &lt;- gsub(&quot;_&quot;, &quot; &quot;, rownames(ca_drugs)) ca_drugs$Variable &lt;- &quot;Drugs&quot; ca_drinks &lt;- as.data.frame(ca_obj$col$coord) ca_drinks$label &lt;- gsub(&quot;_&quot;, &quot; &quot;, rownames(ca_drinks)) ca_drinks$Variable &lt;- &quot;Alcohol&quot; ca_rng &lt;- extendrange(c(ca_drinks$`Dim 1`, ca_drinks$`Dim 2`)) ca_x &lt;- paste0(&quot;Dimension #1 (&quot;, round(ca_obj$eig[&quot;dim 1&quot;, &quot;percentage of variance&quot;], 0), &quot;%)&quot;) ca_y &lt;- paste0(&quot;Dimension #2 (&quot;, round(ca_obj$eig[&quot;dim 2&quot;, &quot;percentage of variance&quot;], 0), &quot;%)&quot;) ca_coord &lt;- rbind(ca_drugs, ca_drinks) ca_plot &lt;- ggplot(ca_coord, aes(x = `Dim 1`, y = `Dim 2`, col = Variable)) + geom_vline(xintercept = 0) + geom_hline(yintercept = 0) + geom_text(aes(label = label)) + xlim(ca_rng) + ylim(ca_rng) + xlab(ca_x) + ylab(ca_y) + coord_equal() ca_plot ## Warning: Removed 1 rows containing missing values (geom_text). The correspondence analysis principal coordinates for the drug and alcohol data in the OkCupid data. "],["post-modeling-exploratory-visualizations.html", "4.7 Post Modeling Exploratory Visualizations", " 4.7 Post Modeling Exploratory Visualizations Multiple linear regression has a rich set of diagnostics based on model residuals that aid in understanding the model fit and in identifying relationships that may be useful to include in the model. One tool from regression diagnosis that is helpful for identifying useful predictors is the partial regression plot (Neter et al. 1996). This plot utilizes residuals from two distinct linear regression models to unearth the potential usefulness of a predictor in a model. (refer to textbook for math formulas) For the Chicago data, a rolling forecast origin scheme (Section 3.4.4) was used for resampling. Figure 4.18 "],["residual-diagnostic-plots.html", "4.8 Residual Diagnostic Plots", " 4.8 Residual Diagnostic Plots The response for the regression model is the ridership at the Clark/Lake station, and our initial model will contain the predictors of week, month and year. As we saw earlier in this chapter, the distribution has two peaks, which we found were due to the part of the week (weekday versus weekend). To investigate the importance of part of the week we then regress the base predictors on part of the week and compute the hold-out residuals from this model. We can see that including part of the week in the model further reduces the residual distribution as illustrated in the histogram labeled Base + Part of Week. Next, let’s explore the importance of the 14-day lag of ridership at the Clark/Lake station. holidays &lt;- c(&quot;USNewYearsDay&quot;, &quot;Jan02_Mon_Fri&quot;, &quot;USMLKingsBirthday&quot;, &quot;USPresidentsDay&quot;, &quot;USMemorialDay&quot;, &quot;USIndependenceDay&quot;, &quot;Jul03_Mon_Fri&quot;, &quot;Jul05_Mon_Fri&quot;, &quot;USLaborDay&quot;, &quot;USThanksgivingDay&quot;, &quot;Day_after_Thx&quot;, &quot;ChristmasEve&quot;, &quot;USChristmasDay&quot;, &quot;Dec26_wkday&quot;, &quot;Dec31_Mon_Fri&quot;) common_holiday &lt;- apply(training %&gt;% dplyr::select(one_of(holidays)), 1, function(x) ifelse(any(x == 1), 1, 0)) training &lt;- training %&gt;% mutate( holiday = common_holiday, weekday = ifelse(dow %in% c(&quot;Sat&quot;, &quot;Sun&quot;), 0, 1) ) # get_resid() get_resid &lt;- function(terms, next_term, return_mod = FALSE) { ctrl$verboseIter &lt;- FALSE ctrl$predictionBounds &lt;- c(0, NA) set.seed(4194) mod &lt;- train(s_40380 ~ ., data = training[, c(&quot;s_40380&quot;, terms)], method = &quot;lm&quot;, metric = &quot;RMSE&quot;, maximize = FALSE, trControl = ctrl) x_mod &lt;- train(as.formula(paste(next_term,&quot;~ .&quot;)), data = training[, c(terms, next_term)], method = &quot;lm&quot;, metric = &quot;RMSE&quot;, maximize = FALSE, trControl = ctrl) if(!return_mod) { out &lt;- mod$pred out$Resample &lt;- ymd(out$Resample) out$Date &lt;- train_days[out$rowIndex] out$Month &lt;- training$month[out$rowIndex] out$holiday &lt;- training$holiday[out$rowIndex] out$weekday &lt;- training$weekday[out$rowIndex] out$Residual &lt;- out$obs - out$pred out$xResidual &lt;- x_mod$pred$obs - x_mod$pred$pred } else out &lt;- mod out } # There will be a warning about the &quot;outcome only has two possible values&quot;. # This can be ignored. theme_set(theme_bw()) base_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;), next_term = &quot;weekday&quot;) %&gt;% mutate(Model = &quot;Base Model&quot;) ## Warning in train.default(x, y, weights = w, ...): You are trying to do ## regression and your outcome only has two possible values Are you trying to do ## classification? If so, use a 2 level factor as your outcome column. pow_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;), next_term = &quot;l14_40380&quot;) %&gt;% mutate(Model = &quot;Base + Part of Week&quot;) l14_resid &lt;- get_resid( terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;, &quot;l14_40380&quot;), next_term = &quot;holiday&quot; ) %&gt;% mutate(Model = &quot;Base + Part of Week + 14-Day Lag&quot;) ## Warning in train.default(x, y, weights = w, ...): You are trying to do ## regression and your outcome only has two possible values Are you trying to do ## classification? If so, use a 2 level factor as your outcome column. ## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, : ## There were missing values in resampled performance measures. hol_resid &lt;- get_resid(terms = c(&quot;year&quot;, &quot;month&quot;, &quot;week&quot;, &quot;weekday&quot;, &quot;l14_40380&quot;, &quot;holiday&quot;), next_term = &quot;l14_40370&quot;) %&gt;% mutate(Model = &quot;Base + Part of Week + 14-Day Lag + Holiday&quot;) mod_lev &lt;- c(&quot;Base Model&quot;, &quot;Base + Part of Week&quot;, &quot;Base + Part of Week + 14-Day Lag&quot;, &quot;Base + Part of Week + 14-Day Lag + Holiday&quot;) model_resid &lt;- bind_rows(base_resid, pow_resid, l14_resid, hol_resid) %&gt;% mutate( Model = factor(Model, levels = mod_lev), holiday = ifelse(holiday == 1, &quot;yes&quot;, &quot;no&quot;), weekday = ifelse(weekday == 0, &quot;Weekend&quot;, &quot;Weekday&quot;) ) resid_hists &lt;- ggplot(model_resid, aes(x = Residual)) + geom_vline(xintercept = 0, lty = 2, col = &quot;darkgreen&quot;) + geom_histogram(binwidth = 0.5, col = rgb(1, 1, 1, 0), fill = &quot;blue&quot;, alpha = .5) + facet_wrap(~Model, ncol = 1) + xlab(&quot;Resampling Residual&quot;) + ylab(&quot;Count&quot;) + ggtitle(&quot;(a)&quot;) day_resid &lt;- base_resid %&gt;% mutate(weekday = ifelse(weekday == 0, &quot;Weekend&quot;, &quot;Weekday&quot;)) %&gt;% ggplot(aes(x = xResidual, y = Residual)) + geom_smooth(se = FALSE, method = lm, col = &quot;black&quot;) + geom_point(aes(col = weekday, shape = weekday), alpha = .5) + xlab(&quot;POW Model Residuals&quot;) + ylab(&quot;Base Model \\n Residuals \\n&quot;) + theme( legend.position = c(.2, .8), legend.background = element_blank(), legend.title = element_blank() ) + ggtitle(&quot;(b)&quot;) l14_PR_resid &lt;- ggplot(pow_resid, aes(x = xResidual, y = Residual)) + geom_point(alpha = .5) + xlab(&quot;14-day Lag Model Residuals&quot;) + ylab(&quot;Base + POW Model \\n Residuals \\n&quot;) + ggtitle(&quot;(c)&quot;) hol_PR_resid &lt;- l14_resid %&gt;% mutate(holiday = ifelse(holiday == 1, &quot;yes&quot;, &quot;no&quot;)) %&gt;% ggplot(aes(x = xResidual, y = Residual)) + geom_smooth(se = FALSE, method = lm, col = &quot;black&quot;) + geom_point(aes(col = holiday, shape = holiday), alpha = .5) + xlab(&quot;Holiday Model Residuals&quot;) + ylab(&quot;Base + POW + \\n 14-day Lag Model \\n Residuals&quot;) + theme( legend.position = c(.2, .25), legend.background = element_blank(), legend.title = element_blank() ) + ggtitle(&quot;(d)&quot;) resid_hists | (day_resid / l14_PR_resid / hol_PR_resid) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; The distribution of residuals from the model resampling process for the base model and the base model plus other potentially useful predictors for explaining ridership at the Clark/Lake station. (a) The partial regression plot for the effect of part of the week. (b) The partial regression plot for the 14-day lag predictor of the Clark/Lake station. (c) The partial regression plot for holiday classification. (d) "],["meeting-videos-2.html", "4.9 Meeting Videos", " 4.9 Meeting Videos 4.9.1 Cohort 1 Meeting chat log LOG "],["encoding-categorical-predictors.html", "Chapter 5 Encoding Categorical Predictors", " Chapter 5 Encoding Categorical Predictors Categorical (also called nominal) predictors are those that contain qualitative data. Examples include: Education level ZIP code Text Day of the week Color A large majority of models require that all predictors be numeric. A summary of parsnip model preprocessors from: Tidy Modeling with R by Max Kuhn and Julia Silge knitr::opts_chunk$set(fig.path = &quot;images/&quot;) suppressPackageStartupMessages({ library(tidyverse) library(tidymodels) library(embed) library(cli) library(kableExtra) }) Table 5.1: Preprocessing methods for different models. model dummy zv impute decorrelate normalize transform bag_mars() ✔ × ✔ ◌ × ◌ bag_tree() × × × ◌¹ × × bart() × × × ◌¹ × × boost_tree() ×² ◌ ✔² ◌¹ × × C5_rules() × × × × × × cubist_rules() × × × × × × decision_tree() × × × ◌¹ × × discrim_flexible() ✔ × ✔ ✔ × ◌ discrim_linear() ✔ ✔ ✔ ✔ × ◌ discrim_regularized() ✔ ✔ ✔ ✔ × ◌ gen_additive_mod() ✔ ✔ ✔ ✔ × ◌ linear_reg() ✔ ✔ ✔ ✔ × ◌ logistic_reg() ✔ ✔ ✔ ✔ × ◌ mars() ✔ × ✔ ◌ × ◌ mlp() ✔ ✔ ✔ ✔ ✔ ✔ multinom_reg() ✔ ✔ ✔ ✔ ×² ◌ naive_Bayes() × ✔ ✔ ◌¹ × × nearest_neighbor() ✔ ✔ ✔ ◌ ✔ ✔ pls() ✔ ✔ ✔ × ✔ ✔ poisson_reg() ✔ ✔ ✔ ✔ × ◌ rand_forest() × ◌ ✔² ◌¹ × × rule_fit() ✔ × ✔ ◌¹ ✔ × svm_*() ✔ ✔ ✔ ✔ ✔ ✔ In the table, ✔ indicates that the method is required for the model and × indicates that it is not. The ◌ symbol means that the model may be helped by the technique but it is not required. Algorithms for tree-based models naturally handle splitting both numeric and categorical predictors. These algorithms employ a series if/then statements that sequentially split the data into groups. A naive Bayes model will create a cross-tabulation between a categorical predictor and the outcome class. We will return to this point in the final section of this chapter. Simple categorical variables can also be classified as ordered or unordered. "],["creating-dummy-variables-for-unordered-categories.html", "5.1 Creating Dummy Variables for Unordered Categories", " 5.1 Creating Dummy Variables for Unordered Categories There are many methods for doing this and, to illustrate, consider a simple example for the day of the week. If we take the seven possible values and convert them into binary dummy variables, the mathematical function required to make the translation is often referred to as a contrast. These six numeric predictors would take the place of the original categorical variable. Why only six? if the values of the six dummy variables are known, then the seventh can be directly inferred. When the model has an intercept, an additional initial column of ones for all rows is included. Estimating the parameters for a linear model (as well as other similar models) involve inverting the matrix. If the model includes an intercept and contains dummy variables for all seven days, then the seven day columns would add up (row-wise) to the intercept and this linear combination would prevent the matrix inverse from being computed (as it is singular). Less than full rank encodings are sometimes called “one-hot” encodings. Generating the full set of indicator variables may be advantageous for some models that are insensitive to linear dependencies (an example: glmnet) What is the interpretation of the dummy variables? Consider a linear model for the Chicago transit data that only uses the day of the week. Using the training set to fit the model, the intercept value estimates the mean of the reference cell, which is the average number of Sunday riders in the training set, estimated to be 3.84K people. The second model parameter, for Monday, is estimated to be 12.61K. In the reference cell model, the dummy variables represent the mean value above and beyond the reference cell mean. In this case, estimate indicates that there were 12.61K more riders on Monday than Sunday. train_df &lt;- tibble(m = month.abb, number = seq(1,12, by = 1)) recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = FALSE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing without One HOT (the default) contrasts with April&quot; ) |&gt; row_spec(row = 4, background = &quot;orange&quot;) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing without One HOT (the default) contrasts with April m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 recipe(number ~ m, data = train_df) |&gt; step_dummy(all_nominal_predictors(), one_hot = TRUE) |&gt; prep() |&gt; bake(new_data = NULL, all_predictors()) |&gt; kable( caption = &quot;Preprocessing with One HOT.&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) |&gt; scroll_box(width = &quot;800px&quot;) (#tab:chapter 5 one hot dummies)Preprocessing with One HOT. m_Apr m_Aug m_Dec m_Feb m_Jan m_Jul m_Jun m_Mar m_May m_Nov m_Oct m_Sep 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 "],["encoding-predictors-for-many-categories.html", "5.2 Encoding Predictors for Many Categories", " 5.2 Encoding Predictors for Many Categories What happens when the number of factor levels gets very large? For example, there are more than 40K possible ZIP codes and, depending on how the data are collected, this might produce an overabundance of dummy variables for the size of the data available. Also, ZIP codes in highly populated areas may have a higher rate of occurrence in the data, leading to a “long tail” of locations that are infrequently observed. Also, resampling will exclude some of the rarer categories from the analysis set. The first way to handle this issue is to create the full set of dummy variables and simply remove the zero and low-variance predictors. Still, we may not desire to filter these out. Another approach is to feature engineer an “other” category that pools the rarely occurring categories, assuming that such a pooling is sensible. Another way to combine categories is to use a hashing function that maps each factor level key to a hash value. The number of possible hashes is set by the user and, for numerical purposes, is a power of 2. Some computationally interesting aspects to hash functions are The only data required is the value being hashed and the resulting number of hashes. The translation process is completely deterministic. Hash functions are unidirectional; once the hash values are created, there is no way of knowing the original values. If there are a known and finite set of original values, a table can be created to do the translation but, otherwise, the keys are indeterminable when only the hash value is known. There is no free lunch when using this procedure; some of the original categories will be mapped to the same hash value (called a “collision”). The number of collisions will be largely determined by the number of features that are produced. Categories involved in collisions are not related in any meaningful way. Because of the arbitrary nature of the collisions, it is possible to have different categories whose true underlying effect are counter to one another. This might have the effect of negating the impact of the hashed feature. Hashing functions have no notion of the probability that each key will occur. As such, it is conceivable that a category that occurs with great frequency is aliased with one that is rare. In this case, the more abundant value will have a much larger influence on the effect of that hashing feature. "],["approaches-for-novel-categories.html", "5.3 Approaches for Novel Categories", " 5.3 Approaches for Novel Categories Suppose that a model is built to predict the probability that an individual works in a STEM profession and that this model depends on city names. The model will be able to predict the probability of STEM profession if a new individual lives in one of the cities in the training set. But what happens to the model prediction when a new individual lives in a city that is not represented? One strategy would be to use the previously mentioned “other” category to capture new values. This approach can also be used with feature hashing. "],["supervised-encoding-methods.html", "5.4 Supervised Encoding Methods", " 5.4 Supervised Encoding Methods Beyond dummies, there are many other ways to craft one or more numerical features from a set of nominal predictors. They include 5.4.1 Likelihood Encoding In essence, the effect of the factor level on the outcome is measured and this effect is used as new numeric features. For example, for the Ames housing data, we might calculate the mean or median sale price of a house for each neighborhood from the training data and use this statistic to represent the factor level in the model. For classification problems, a simple logistic regression model can be used to measure the effect between the categorical outcome and the categorical predictor. If the outcome event occurs with rate \\[ p \\], the odds of that event is defined as \\[ p / ( 1 − p) \\]. This is an example of a single generalized linear model applied to the hair color feature, which woudl otherwise have 12 dummy levels. as_tibble(dplyr::starwars) |&gt; count(hair_color) ## # A tibble: 13 × 2 ## hair_color n ## &lt;chr&gt; &lt;int&gt; ## 1 auburn 1 ## 2 auburn, grey 1 ## 3 auburn, white 1 ## 4 black 13 ## 5 blond 3 ## 6 blonde 1 ## 7 brown 18 ## 8 brown, grey 1 ## 9 grey 1 ## 10 none 37 ## 11 unknown 1 ## 12 white 4 ## 13 &lt;NA&gt; 5 recipe(skin_color ~ hair_color + eye_color + mass, data = as_tibble(dplyr::starwars)) |&gt; embed::step_lencode_glm(hair_color, outcome = &quot;skin_color&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) |&gt; kable( caption = &quot;Starwars Characters hair_color GLM embedding&quot; ) |&gt; kable_styling(&quot;striped&quot;, full_width = FALSE) (#tab:chapter 5 glm numeric embeddings)Starwars Characters hair_color GLM embedding hair_color eye_color mass skin_color -2.862201 red 113 green -21.566069 yellow 55 fair, green, yellow -21.566069 blue 89 fair -21.566069 brown 79 tan -21.566069 brown 84 light -2.862201 black NA white, blue -21.566069 brown NA tan -21.566069 blue NA fair -2.862201 red 140 metal -21.566069 brown NA light While very fast, this method has drawbacks. For example, what happens when a factor level has a single value? Theoretically, the log-odds should be infinite in the appropriate direction but, numerically, it is usually capped at a large (and inaccurate) value. One way around this issue is to use some type of shrinkage method. For example, the overall log-odds can be determined and, if the quality of the data within a factor level is poor, then this level’s effect estimate can be biased towards an overall estimate that disregards the levels of the predictor. A common method for shrinking parameter estimates is Bayesian analysis. (one doubt – step_lencode_bayes appears to only work with two class outcomes ???) as_tibble(datasets::Titanic) |&gt; count(Class) recipe(Survived ~ Class + Sex + Age, data = as_tibble(datasets::Titanic)) |&gt; embed::step_lencode_bayes(Class, outcome = &quot;Survived&quot;) |&gt; prep() |&gt; bake(new_data = NULL) |&gt; slice_sample(n = 10) # A tibble: 10 × 4 Class Sex Age Survived &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; 1 -0.0108 Female Adult Yes 2 -0.0108 Male Adult No 3 -0.00526 Male Child Yes 4 -0.00526 Female Child No 5 -0.00993 Male Adult No 6 -0.0104 Male Child Yes 7 -0.00526 Male Child No 8 -0.0108 Female Adult No 9 -0.00993 Male Child Yes 10 -0.0104 Female Child No Empirical Bayes methods can also be used, in the form of linear (and generalized linear) mixed models. One issue with effect encoding, independent of the estimation method, is that it increases the possibility of overfitting the training data. Use resampling. Another supervised approach comes from the deep learning literature on the analysis of textual data. In addition to the dimension reduction, there is the possibility that these methods can estimate semantic relationships between words so that words with similar themes (e.g., “dog”, “pet”, etc.) have similar values in the new encodings. This technique is not limited to text data and can be used to encode any type of qualitative variable. An example using The Office dialogue and one of the pre-trained GloVe embeddings. library(textrecipes) library(schrute) glove6b &lt;- textdata::embedding_glove6b(dimensions = 100) # the download is 822.2 Mb schrute::theoffice |&gt; slice_sample(n = 10) |&gt; select(character, text) recipe(character ~ text, data = schrute::theoffice) |&gt; step_tokenize(text, options = list(strip_punct = TRUE)) |&gt; step_stem(text) |&gt; step_word_embeddings(text, embeddings = glove6b) |&gt; prep() |&gt; bake(new_data = schrute::theoffice |&gt; slice_sample(n = 10)) The Office dialogue word embeddings with glove6b # A tibble: 10 × 101 character wordembe…¹ worde…² worde…³ worde…⁴ worde…⁵ worde…⁶ &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 Angela -1.02 -0.377 0.797 -1.21 -0.802 0.656 2 Roy 0 0 0 0 0 0 3 Phyllis -1.20 -0.373 3.20 -1.60 -1.62 -0.160 4 Kevin -1.54 1.77 5.02 -4.68 -5.23 2.91 5 Roy -2.15 0.735 4.07 -2.20 -1.30 0.297 6 Jim -0.595 0.419 0.699 -0.328 -1.20 1.70 7 Kelly -2.17 4.38 5.97 -4.91 -4.21 4.13 8 Katy -0.891 -0.889 0.0937 -0.859 1.42 1.49 9 Kevin -0.0308 0.120 0.539 -0.437 -0.739 -0.153 10 Jim -0.395 0.240 1.14 -1.27 -1.47 1.39 # … with 94 more variables: wordembed_text_d7 &lt;dbl&gt;, # wordembed_text_d8 &lt;dbl&gt;, wordembed_text_d9 &lt;dbl&gt;, # wordembed_text_d10 &lt;dbl&gt;, wordembed_text_d11 &lt;dbl&gt;, # wordembed_text_d12 &lt;dbl&gt;, wordembed_text_d13 &lt;dbl&gt;, # wordembed_text_d14 &lt;dbl&gt;, wordembed_text_d15 &lt;dbl&gt;, # wordembed_text_d16 &lt;dbl&gt;, wordembed_text_d17 &lt;dbl&gt;, # wordembed_text_d18 &lt;dbl&gt;, wordembed_text_d19 &lt;dbl&gt;, … Note that in place of thousands of sparse dummy colums for each tokenized word, the training set consists of 100 numeric feature dimensions. See also Textrecipes series: Pretrained Word Embedding by Emil Hvitfeldt "],["encodings-for-ordered-data.html", "5.5 Encodings for Ordered Data", " 5.5 Encodings for Ordered Data Suppose that the factors have a relative ordering, like low, medium, and high. R uses a technique called polynomial contrasts to numerically characterize the relationships. values &lt;- c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) dat &lt;- data.frame(x = ordered(values, levels = values)) # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table model.matrix(~ x, dat) ## (Intercept) x.L x.Q ## 1 1 -7.071068e-01 0.4082483 ## 2 1 -7.850462e-17 -0.8164966 ## 3 1 7.071068e-01 0.4082483 ## attr(,&quot;assign&quot;) ## [1] 0 1 1 ## attr(,&quot;contrasts&quot;) ## attr(,&quot;contrasts&quot;)$x ## [1] &quot;contr.poly&quot; # using recipes ---------------------------------------------------------------- # https://bookdown.org/max/FES/encodings-for-ordered-data.html#tab:categorical-ordered-table recipe(~ x, data = dat) |&gt; step_dummy(x) |&gt; prep() |&gt; bake(new_data = NULL) ## # A tibble: 3 × 2 ## x_1 x_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -7.07e- 1 0.408 ## 2 -7.85e-17 -0.816 ## 3 7.07e- 1 0.408 It is important to recognize that patterns described by polynomial contrasts may not effectively relate a predictor to the response. For example, in some cases, one might expect a trend where “low” and “middle” samples have a roughly equivalent response but “high” samples have a much different response. In this case, polynomial contrasts are unlikely to be effective at modeling this trend. Other alternatives to polynomial contrasts: Leave the predictors as unordered factors. Translate the ordered categories into a single set of numeric scores based on context-specific information. Simple visualizations and context-specific expertise can be used to understand whether either of these approaches are good ideas. "],["creating-features-for-text-data.html", "5.6 Creating Features for Text Data", " 5.6 Creating Features for Text Data Often, data contain textual fields that are gathered from questionnaires, articles, reviews, tweets, and other sources. Are there words or phrases that would make good predictors of the outcome? To determine this, the text data must first be processed and cleaned. One approach is to measure for “importance”, that is, keywords with odds-ratios of at least 2 (in either direction) to be considered for modeling. See also Supervised Machine Learning for Text Analysis in R for a much better explanation. Other methods for preprocessing text data include: removing commonly used stop words, such as “is”, “the”, “and”, etc. stemming the words so that similar words, such as the singular and plural versions, are represented as a single entity. filter for the most common tokens, and then calculate the term frequency-inverse document frequency (tf-idf) statistic for each token "],["factors-versus-dummy-variables-in-tree-based-models.html", "5.7 Factors versus Dummy Variables in Tree-Based Models", " 5.7 Factors versus Dummy Variables in Tree-Based Models Certain types of models have the ability to use categorical data in its natural form. For example, a Chicago ridership decision tree could split on if day in {Sun, Sat} then ridership = 4.4K else ridership = 17.3K Suppose the day of the week had been converted to dummy variables. What would have occurred? In this case, the model is slightly more complex since it can only create rules as a function of a single dummy variable at a time: if day = Sun then ridership = 3.84K else if day = Sat then ridership = 4.96K else ridership = 17.30K So, for decision trees and naiive bayes does it matter how the categorical features are encoded? To answer this question, a series of experiments was conducted. The results: For these data sets, there is no real difference in the area under the ROC curve between the encoding methods. In terms of performance, it appears that differences between the two encodings are rare (but can occur). One other statistic was computed for each of the simulations: the time to train the models. Here, there is very strong trend that factor-based models are more efficiently trained than their dummy variable counterparts. One other effect of how qualitative predictors are encoded is related to summary measures. Many of these techniques, especially tree-based models, calculate variable importance scores that are relative measures for how much a predictor affected the outcome. For example, trees measure the effect of a specific split on the improvement in model performance (e.g., impurity, residual error, etc.). As predictors are used in splits, these improvements are aggregated; these can be used as the importance scores. "],["summary.html", "5.8 Summary", " 5.8 Summary With the exception of tree-based models, categorical predictors must first be converted to numeric representations to enable other models to use the information. The simplest feature engineering technique is to convert each category to a separate binary dummy predictor. Some models require one fewer dummy predictors than the number of categories. Creating dummy predictors may not be the most effective way. If, for instance, the predictor has ordered categories, then polynomial contrasts may be better. Text fields, too, can be viewed as an agglomeration of categorical predictors and must be converted to numerics. "],["meeting-videos-3.html", "5.9 Meeting Videos", " 5.9 Meeting Videos 5.9.1 Cohort 1 No chat for this session LOG "],["engineering-numeric-predictors.html", "Chapter 6 Engineering Numeric Predictors", " Chapter 6 Engineering Numeric Predictors Learning objectives: Learn about common issues and techniques when handling continuous predictors Often dealing with continuous predictors can be corrected by the model you select Skewed data? Use tree-based methods K-nearest neighbor and support vector machines should be avoided Highly correlated variables? Use Partial Least Squares Multiple linear regression and neural networks should be avoided Feature Engineering techniques to: Address problematic characteristics of individual predictors Expand individual predictors to better represent complex relationships Consolidate redundant information "],["problematic-characteristics-of-predictors.html", "6.1 Problematic Characteristics of predictors", " 6.1 Problematic Characteristics of predictors One of the first things we can do when transforming our data is to rescale it to improve model performance. There are several different methods that can be used in different situations. Box-Cox and Yeo Johnson transformations can be used to correct for highly skewed predictors. Standardizing strategies such as scaling, centering, and smoothing can also be used to ensure your predictors have similar qualities. 6.1.1 Dealing with Skewed Data Do a Box-Cox Transformation! Originally developed to transform the outcome variable, box-cox uses maximum likelihood estimation to estimate a transformation parameter λ. The transformation allows the data to follow a normal distribution. The plot below shows the before and after effects. 6.1.1.1 What does a Box-Cox transformation do? Because the estimated parameter is in the exponent, it is called a POWER transformation. Different values of the parameter mimick no transformation (1), log transformation (0), square root (0.5), or inverse (-1). Because it is so flexible it can handle many different types of distributions Several caveats: Can only be used when predictors are greater or equal to 0 Yeo-Johnson transformation can be used when there are negative values. Best used for models that use polynomial transformations like linear regression, neural networks, or support vector machines. 6.1.1.2 Logit Transformation Proportions, or data that falls between 0 and 1, are a special case that warrants their own transformation! Sometimes when a proportion is the outcome variable, model predictions can include results that are outside these bounds. Using a logit transformation, we can correct for that by changing the values between negative and positive infinity. If π is the variable, the logit transformations is: \\[ logit(π) = log(\\frac{π}{1-π}) \\] After the predictions are calculated, you can use the inverse logit to return the data to its original form. 6.1.2 Standardizing Why standardize? When your predictors are all in different units, with varying ranges, some may have an out-sized influence on the outcome when that should not be the case. This is especially true for models that use a distance computation like kNN, PCA, SVM, lasso and ridge regression, and variable importance calculation. Logistic regression and tree based methods not so much…. The main techniques illustrated in the book are centering, scaling, and smoothing sequential (time) data. 6.1.2.1 Centering and Scaling Centering is pretty straight forward. For each variable, every value is subtracted by its average. After which, all variables will have a mean of zero. Scaling is the process of dividing a variable by the corresponding training set’s standard deviation. This ensures that that variables have a standard deviation of one. In addition to improving model performance, centering and scaling can also help better interpret our models for a couple reasons It is easier to interpret the y-intercept when the predictors have been centered and scaled Centering allows for an easier interpretation of coefficients with different magnitudes 6.1.2.2 Time data: Smoothing When your model uses sequential, or time related, there can be a lot of noise in your model. By computing a running average,or a running median, we can smooth out our predictor and reduce noise. These are computed by taking the average of a point and one point before and after. The size of the moving window is extremely important. Small windows might not involve much smoothing at all, and large windows may miss out on important trends. Computing the running median is especially important when there are major outliers in your sequential data. A sequence of outcome values over time. The raw data contain outliers on days 10 and 45. The smoothed values are the result of a 3-point running median. "],["expanding-numeric-transformations.html", "6.2 Expanding Numeric Transformations", " 6.2 Expanding Numeric Transformations Just like we can expand qualitative features into multiple useful columns, we can also take one numeric predictor and expand it into several more. The following subsection divides these expansions into two categories: nonlinear features via basis expansions and splines, and discretize predictors 6.2.1 Nonlinear Features via Basis Expansions and Splines 6.2.1.1 Basis Expansions REMINDER: A basis is a set of vectors V such that every vector V can be written uniquely as a finite combination of vectors of the basis. A basis expansion takes a feature \\(x\\) and derives a set of functions \\(f_i(x)\\) that can be combined using a linear combination, sort of like polynomial contrasts in the previous categorical section. A cubic basis expansion would produce two additional features, a squared and a cubed version of the original. \\(\\beta\\) terms are computed for each feature using linear regression. But what if these trends are not linear throughout the prediction space? The sale price and lot area data for the training set. 6.2.1.2 Polynomial Splines Polynomial splines can be used for basis expansion instead of typical linear regression to divide the prediction space into regions, whose boundaries are called knots. Each region can have its own fitted set of polynomial functions, which can describe complex relationships. Usually you select percentiles to create the knots. Determining the number of knots can be done by doing a grid search, a visual inspection of the results, generalized cross-validation (GCV) which is used in some spline functions. The previous methods were created prior to being exposed to the outcome variable, but that doesn’t have to be the case. Generalized Additive Models (GAMS) work very similarly to polynomial splines, and can adaptively model multiple different basis functions for each predictor. Loess regression is also a supervised nonlinear smoother. (a) Features created using a natural spline basis function for the lot area predictor. The blue lines correspond to the knots. (b) The fit after the regression model estimated the model coefficients for each feature. Lastly, a multivariate adaptive regression spline (MARS) model includes a single knot, or hinge transformation. (a) Features created using a natural spline basis function for the lot area predictor. The blue lines correspond to the knots. (b) The fit after the regression model estimated the model coefficients for each feature. 6.2.2 Discretize Predictors as a Last Resort If we discretize a predictor, or turn it into categorical, we are essentially binning the data. Unsupervised approaches to binning are rather straight forward using percentiles or the median, but they can also be set based on specific cut-off points to improve performance. The books mentions many reason why this should NOT be done. Despite simplifying modeling, you lose much of the nuance. Discretizing your predictors can make it harder to model trends, it can increase the probability of modeling a relationship that is not there, and the cut-off points are rather arbitrary. Raw data (a) and binned (b) simulation data. Also, the results of a simulation (c) where the predictor is noninformative. "],["new-features-from-multiple-predictors.html", "6.3 New Features from Multiple predictors", " 6.3 New Features from Multiple predictors The previous two sections focused on adjusting our features and creating new features from a single predictor, but we can also create new features from all of our predictors at once. This can correct issues like multicollinearity and outliers, while also reducing the dimensionality and speeding up computation times. There are a ton of different techniques for this that are essentially broken down into four main parts: linear projections, autoencoders, Spatial sign, and Distance and Depth features. 6.3.1 Linear Projection Methods More predictors is not always better! Especially if you have redundant information. This section is all about identifying meaningful projections of the original data. The unsupervised methods, like PCA, tend not to increase model performance, yet they do save computational time. Supervised approaches, like partial least squares, DO however if you have enough data to prevent overfitting. 6.3.1.1 Principal Component Analysis (PCA) PCA finds a linear combination of the original predictors that summarizes the maximum amount of variation. Since they are orthogonal, the predictor space is partitioned in a way that does not overlap. (uncorrelated) Principal component analysis of two highly correlated predictors We can see from the graph below that the first component USUALLY contains the most information. We can also see that some methods do much better than others. Score values from several linear projection methods for the weekend ridership at Clark and Lake. The x-axis values are the scores for each method and the y-axis is ridership (in thousands). Visualizing the principal components with a heat map can help us identify which predictors are impacting each component. The figure below visualizes the Chicago train dataset. We can see after the first component that lines are clustered together meaning the line probably has an effect on our outcome. ## oper 1 step center [training] ## oper 2 step scale [training] ## oper 3 step pca [training] ## The retained training set is ~ 0.26 Mb in memory. ## PC01 PC02 PC06 PC07 PC20 PC11 ## 0.788363004 -0.340091805 0.160229143 0.140769244 0.128354958 0.107559442 ## PC19 PC04 PC16 PC03 PC18 PC10 ## -0.099391044 0.093115558 -0.087066549 0.053463985 0.050741445 -0.049221526 ## PC09 PC14 PC15 PC12 PC08 PC17 ## 0.037859109 0.034199452 -0.024362732 0.018376755 0.016049202 0.014475841 ## PC13 PC05 ## 0.007675887 0.001579482 6.3.1.2 Kernel PCA PCA is really effective when the predictors are linearly correlated, but what if the relationship is not actually linear but quadratic? That’s were kernel PCA comes in. There are MANY different kernels based on the shape of the predictor space (Polynomial, Gaussian etc), so you really need to inspect your data before setting kPCA up. Because kPCA is more flexible, it can often lead to much better results. The firgure below shows how using basic PCA would actually lead to a poor fit. A comparison of PCA and kernel PCA. (a) The training set data. (b) The simulated relation between x 1 and the outcome. (c) The PCA results (d) kPCA using a polynomial kernel shows better results. (e) Residual distributions for the two dimension reduction methods. 6.3.1.3 Independent Component Analysis PCA components are uncorrelated with each other, but they are often not independent.That’s where ICA comes in! ICA is similar to PCA except that it components are also as statistically independent as possible. Unlike PCA, there is no ordering in ICA. Preprocessing is essential for ICA. Predictors must be centered and then “whitened”, which means that PCA is calculated BEFORE ICA, which seems rather odd. 6.3.1.4 Non-negative factorization when features are greater than or equal to zero. Most popular for text data! where the predictors are word counts, imaging, and biological measures. 6.3.1.5 Partial Least Squares PLS is supervised version of pca that guides dimension reduction optimally related to the response. Finds latent variables that have optimal covariance with the response. Because of this supervized approach, you generally need fewer components than PCA, BUT you risk overfitting. Model results for the Chicago weekend data using different dimension reduction methods (each with 20 components). 6.3.1.6 Autoencoders Computationally complex multivariate method for finding representations of the predictor space, primarily used in deep learning. Generally they don’t have any actual interpretation but they have some strengths. Autoencoders are really good when there is a large amount of unlabled data. Example: used in the pharmaceutical industry to estimate how good a drug might be based on its chemical structure. To simulate a drug discovery project just starting up, a random set of 50 data points were used as a training set and another random set of 25 were allocated to a test set. The remaining 4327 data points were treated as unlabeled data that do not have melting point data. The results of fitting and autoencoder to a drug discovery data set. Panel (a) is the holdout data used to measure the MSE of the fitting process and (b) the resampling profiles of K-nearest neighbors with (black) and without (grey) the application of the autoencoder. 6.3.1.7 Spatial Sign Spatial Sign is primarily used in image analysis, transforms the predictors based on their center to the distribution and projects them onto an nD sphere. Scatterplots of a classification data set before (a) and after (b) the spatial sign transformation. THE EXAMPLE THEY GIVE INVOLVES CLASSIFYING IMAGES OF ANIMAL POOP! Spatial sign is really good at decreasing impact of outliers. 6.3.1.8 Distance and depth features Distance and Depth Features take a semi-supervised approach for classification problems. Predictors are recomputed based on the distance to class centroid, sort of like knearest neighbor. Examples of class distance to centroids (a) and depth calculations (b). The asterisk denotes a new sample being predicted and the squares correspond to class centroids. "],["meeting-videos-4.html", "6.4 Meeting Videos", " 6.4 Meeting Videos 6.4.1 Cohort 1 Meeting chat log LOG "],["detecting-interaction-effects.html", "Chapter 7 Detecting Interaction Effects", " Chapter 7 Detecting Interaction Effects Learning objectives: how predictors relate to the outcome when to apply for interaction effects how to make predictions selection for looking to key interactions "],["introduction-2.html", "7.1 Introduction", " 7.1 Introduction In this chapter we will be looking at the interaction effects caused by predictors acting together on the response variable. …additional variation in the response can be explained by the effect of two or more predictors working in conjunction with each other. As an example consided are the effects of water and fertilizer on the yield of a field corn crop. “With no water but some fertilizer, the crop of field corn will produce no yield since water is a necessary requirement for plant growth. Conversely, with a sufficient amount of water but no fertilizer, a crop of field corn will produce some yield. However, yield is best optimized with a sufficient amount of water and a sufficient amount of fertilizer. Hence water and fertilizer, when combined in the right amounts, produce a yield that is greater than what either would produce alone.” predictors are said to interact if their combined effect is different (less or greater) than what we would expect if we were to add the impact of each of their effects when considered alone. Correlations between predictors, for example, are not directly related to whether there is an interaction effect or not The individual variables (e.g., fertilizer and water) are referred to as the main effect terms when outside of an interaction. "],["four-type-of-interactions.html", "7.2 Four type of Interactions", " 7.2 Four type of Interactions additive is when \\(\\beta_3 \\approx{0}\\) antagonistic is when \\(\\beta_3 &lt; 0\\) synergistic is when \\(\\beta_3 &gt; 0\\) atypical is when \\(\\beta_3 \\neq 0\\) The main difference is that in the atypical interaction one of the two predictors doesn’t affect the response. see full code in the scripts folder: 1_geom_contour.R library(tidyverse) # simulated data set.seed(123) beta0&lt;- rep(0,200) beta1&lt;- rep(1,200) beta2&lt;- rep(1,200) x1&lt;- runif(200,min = 0, max = 1) x2 &lt;- runif(200,min = 0, max = 1) e &lt;- rnorm(200) ################################################## # synergism beta3&lt;- rep(10,200) # c(-10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~x1*x2,observed) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p1 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + #geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Synergistic&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) ################################################## # no interaction beta3 &lt;- rep(0,200) # c(10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~x1*x2,observed) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p2 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + # geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Additive&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) ################################################## # antagonism beta3&lt;- rep(-10,200) # c(-10,0,10) # antagonism, no interaction, or synergism y = beta0 + beta1*x1 + beta2*x2 + beta3*(x1*x2) + e observed&lt;- tibble(y,x1,x2) mod &lt;- lm(y~ x1 * x2 , data = observed) # rnd effects (1 + x1 | x2) observed$z &lt;- predict(mod,observed) grid &lt;- with(observed, interp::interp(x=x1,y=x2,z)) griddf &lt;- subset(data.frame(x = rep(grid$x, nrow(grid$z)), y = rep(grid$y, each = ncol(grid$z)), z = as.numeric(grid$z)),!is.na(z)) p3 &lt;- ggplot(griddf, aes(x, y, z = z)) + geom_contour(aes(colour = after_stat(level)),size=2) + # geom_point(data = observed,aes(x1,x2)) + scale_color_viridis_c()+ labs(title=&quot;Antagonistic&quot;,color=&quot;Prediction&quot;,x=&quot;x1&quot;,y=&quot;x2&quot;)+ theme_bw()+ theme(legend.position = &quot;top&quot;) Visualizing interaction effects for the Ames data: see full code in the scripts folder: 2_manipulate.R The general model function for interaction effects: \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] Which predictors interact? interaction can be uncovered by more complex modeling techniques tree-based models random forests boosted tree model search techniques and svm Feature engineering helps improving the effectiveness of a models by featuring selection of predictors, so as a consequence simplify the detection of interaction effects. 7.2.1 Building the base-model for Ames data Here are the predictors divided by type, we will be looking at different ways to make a selection of the predictors and what are the best interactions for this data, which will be influencing model preformance. library(AmesHousing) ames &lt;- make_ames() %&gt;% janitor::clean_names() ames1 &lt;- ames %&gt;% # names%&gt;%sort select(# continuous gr_liv_area,lot_area, lot_frontage,year_built, year_sold,pool_area,longitude, latitude,full_bath, # qualitative neighborhood,bldg_type, central_air,ms_sub_class, foundation,roof_style,alley, garage_type,land_contour) "],["guiding-principles-in-the-search-for-interactions.html", "7.3 Guiding Principles in the Search for Interactions", " 7.3 Guiding Principles in the Search for Interactions Statistical experimental design to establish casual relationships between independent and dependent variables, foresees: control randomization replication Interactions can be of different degrees: The identification of the interactions can be challenging, and even more challenging can be the identification of the shepherd interaction effects. The framework for identifying significant interactions (Wu ans Hamada 2011) for experimental design and predictive modeling is based on: interaction hierarchy (degree of interaction) effect sparsity (only a fraction of the interaction effects can be effective) effect heredity (implies significant factors preceding interaction explain the most of the response) strong heredity (interaction only with significant preceeding factors) weak heredity (any interaction with one significant factor) High order interaction happen in real life data (interactions among species). "],["practical-considerations.html", "7.4 Practical Considerations", " 7.4 Practical Considerations Is it possible to identify all possible predictive interactions? Is it possible to evaluate all possible interactions? Should the interaction terms be created before or after the preprocessing part? Only a fraction of all possible pairwise interactions contain relevant information. With \\(p\\) predictors we have \\((p)(p-1)/2\\) pairwise interaction terms. Here is the difference in interaction effects before and after preprocessing: "],["the-brute-force-approach-to-identifying-predictive-interactions.html", "7.5 The Brute-Force Approach to Identifying Predictive Interactions", " 7.5 The Brute-Force Approach to Identifying Predictive Interactions False discoveries can influence model performance 7.5.1 Simple Screening Base-line approach is to evaluate the performance with nested statistical models: \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\text{error}\\] \\[y=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] see full code in the scripts folder: 3_comparisons_nested_models.R Objective function: for linear regression is the statistical likelihood (residual error) for logistic regression is the binomial likelihood Evaluation methods: The residual error (stat. likelihood) is compared and the hypothesis test evaluated with the p-value level to find differences between the results of estimations with and without interaction. If significant differences are found, p-value &lt; 0.05, there is less than 5% chance that the results are due to randomness. This is the case for false discoveries. Resampling and assessment evaluation. Use of metrics for visualizing the model performance: ROC, AUC, sensitivity, specificity, accuracy Methods for controlling false discoveries: Bonferroni correction (exponential penalty) False discovery Rate (FDR) see the code: Bonferroni and FDR adj For example, in case of the Ames data, using resampling and choosing the potential interaction with the smallest p-value, latitude and longitude appear to be interesting interaction factors, but this would require more investigations, to understand if this interaction is significant. The next step would be to compare the nested models with the ANOVA method. see an example: “comparisons_nested_models.R” 7.5.2 Penalized Regression One-at-a-time fashion evaluation of interaction terms, creates interaction terms to be added in the dataset. This method increases the number of predictors. Models to use when there are more predictors than observations: trees svm neural networks k-nearest neighbors penalized models (less interpretable, but allow for linear/logistic regression) How do we start with evaluating regression models? Minimize sum of squared errors \\[SSE=\\sum_{i=1}^n{(y_i-\\hat{y_i})^2}\\] \\[\\hat{y_i}=\\hat{\\beta_1}x_1+\\hat{\\beta_2}x_2+...+\\hat{\\beta_p}x_p\\] In case of penalized models: Ridge regression: \\(\\lambda_r\\) is called a penalty. To achieve better results, as regression coefficients grow large, the level of the penalty should rise. The penalty causes the resulting regression coefficients to become smaller and shrink towards zero. For combating collinearity. \\[SSE=\\sum{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda_r\\sum_{j=1}^P{\\beta_j^2}\\] Lasso: the least absolute shrinking, a modification to the ridge optimization criteria for the selection of predictors. \\[SSE=\\sum{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda_l\\sum_{j=1}^P{|\\beta_j|}\\] Approaches for blending both types of penalties together: glmnet model \\[\\lambda=\\lambda_r+\\lambda_l\\] where is \\(\\alpha\\) is the proportion of \\(\\lambda\\) associated with lasso penalty: full lasso: \\(\\alpha=1\\) mix: \\(\\alpha=0.5\\) full ridge: \\(\\alpha=0\\) \\[SSE=\\sum_{i=1}^n{(y_i-\\hat{y_i})^2}+\\lambda [(1-\\alpha)\\sum_{j=1}^P{\\beta_j^2+\\alpha\\sum_{j=1}^P{|\\beta_j|}]}\\] 7.5.3 Practical example with Ames data and glmnet see full code in the scripts folder: 5_ames_modeling.R "],["approaches-when-complete-enumeration-is-practically-impossible.html", "7.6 Approaches when Complete Enumeration is Practically Impossible", " 7.6 Approaches when Complete Enumeration is Practically Impossible 7.6.1 Guiding Principles and Two-stage Modeling Two-stage Modeling is another approach to use: use simple models such as lm or glm then add interaction effects use models ready for considering interactions In the residuals are the predictors missing information. 7.6.1.1 Example Observed data are \\(y\\) and \\(x_1\\), we know nothing about \\(x_2\\), and so \\(x_1*x_2\\). \\[y=x_1+x_2+10x_1x_2+\\text{error}\\] \\[y=\\beta_1x_1+x_2+\\text{error*}\\] \\[\\text{error*}=\\beta_2x_2+\\beta_3x_1x_2+\\text{error}\\] Random measurement error remains unexplained. hierarchy principle: first look at pairwise interactions sparsity principle: look for active interactions heredity principle: search for interactions among the predictors identified in the first stage choose the type of heredity: determine the number of interaction terms For classification outcome (categorical response) the Pearson residual should be used: \\[\\frac{y_i-p_i}{\\sqrt{p_i(1-p_i)}}\\] 7.6.2 Tree-based Methods “Tree-based models are usually thought of as pure interaction due to their prediction equation, which can be written as a set of multiplicative statements.” tree-based methods uncover potential interactions between variables recursive partitioning identifies important interactions among predictors “In essence, the partitioning aspect of trees impedes their ability to represent smooth, global interactions.” Moreover, a tree-based model does well at approximating the level of interaction but, it breaks the space into rectangular regions, and needs many regions to capture all possible interactions. For this reason, ensembles of trees such as bagging (many samples of the original data generated with replacement) and boosting (sequence of trees with restricted depth, improved performance and weighted stats) are better performers. Here is a representation of the clusters for Ames data: Here is the outcome of a basic decision tree for the Ischemic Stroke data: see full code in the scripts folder: 6_ames_hclust.R Tree ensembles are so effective at identifying predictor-response relationships because they are aggregating many trees from slightly different versions of the original data. An example is Random Forest which is a variation of bagging. Partial dependency compares the joint effect of two (or more) predictors with the individual effect of each predictor in a model. This comparison is named H statistic, which is 0 if no interaction is found and &gt;0 otherwise. More about the H statistic can be found in Friedman and Popescu 2008. see full code in the scripts folder: 7_ames_H_stat.R see the code: Ames trees example with H statistic Out-of-bag (OOB) samples: Random forest uses bootstrap samples to create many models to be used in the ensemble. Since the bootstrap is being used, each tree has an associated assessment set (historically called the out-of-bag (OOB) samples) that we not used to fit the model. To be mentioned is this interesting package: pre package 7.6.3 The Feasible Solution Algorithm A preselction is done before searching for interaction among predictors. When linear and logistic models are used some predictor selection methods are applied: Forward selection: It starts with no predictors and select the best one, then select the second best and so on… Backward selection: It starts with all predictors and make a selection based on least contribution on optimization. Stepwise selection: It adds and removes predictors at a time at each step based on optimization criteria, until model’s results negatively impact model performance. FSA: The feasible solution algorithm (Miller’s approach extended by Hawkins) to find the optimal subset. Miller’s approach with 10 predictors, randomly select 3 of the predictors among all the others. It makes a model selecting one of the three predictors. Then the other two are modeled against all the remaining ones. If any of the new added predictors gives a better result, the first predictor is chosen. Then more swapping is performed among the first three predictors against all the others, untile the best one is found, or until it converges. Hawkins’ extension: q: random starts m: terms in the subset p: predictors The space is \\(q\\text{ x }m\\text{ x }p\\), in general the space is \\(p^m\\). Lambert’s approach add the interaction selection to the FSA algorithm. See the code for FSA Application to Ames data "],["other-potentially-useful-tools.html", "7.7 Other Potentially Useful Tools", " 7.7 Other Potentially Useful Tools Multivariate adaptive regression splines (MARS) is a nonlinear modeling technique for a continuous response MARS has also been extended to classification outcomes, and this method is called flexible discriminant analysis (FDA). Cubist (Kuhn and Johnson 2013) is a rule-based regression model that builds an initial tree and decomposes it into set of rules that are pruned and perhaps eliminated. "],["conclusion.html", "7.8 Conclusion", " 7.8 Conclusion It is challeging to detect all the real interacting effects on a data set with many predictors. A good advice from the expert in the field of data could be key to identify predictors that are force of interaction for that specific data. In addition, the application of pairwise interaction selection is still to be applied for looking at possible changes that might apply to data overtime. "],["meeting-videos-5.html", "7.9 Meeting Videos", " 7.9 Meeting Videos 7.9.1 Cohort 1 Meeting chat log LOG "],["handling-missing-data.html", "Chapter 8 Handling Missing Data", " Chapter 8 Handling Missing Data Learning objectives: Identify types of missing data Understand the nature and severity of missing data Perform visualizations for missing data Methods to treat/cope with missing data (deletion, imputation) "],["types-of-missing-data.html", "8.1 Types of missing data", " 8.1 Types of missing data Three common mechanisms: Structural deficiencies in the data (example: AMES dataset) Random occurrences (MCAR) Specific causes (MAR, MNAR) Source: Little, R, and D Rubin. 2014. Statistical Analysis with Missing Data. John Wiley; Sons. "],["missing-data-mechanisms.html", "8.2 Missing data mechanisms", " 8.2 Missing data mechanisms Missing Completely at Random (MCAR) Locations of missing values in the dataset are purely random, they do not depend on any other data. Example: A weather sensor is measuring temperature and sending the data to a database. There are some missing entries in the database for when the sensor broke down. MCAR test (naniar package) Missing at Random (MAR) Locations of missing values in the dataset depend on some other, observed data. Example: There are some missing temperature values in the database for when the sensor was switched off for maintenance. As the maintenance team never work on the weekends, the locations of missing values depend on the day of the week. Missing Not at Random (MNAR) Locations of missing values in the dataset depend on the missing values themselves. Example: When it’s extremely cold, the weather sensor freezes and stops working. So, it does not record very low temperatures. Thus, the locations of missing values in the temperature variable depend on the values of this variable themselves. "],["statistical-rethinking-bayesian-chapter-20---missing-data-other-opportunities.html", "8.3 Statistical Rethinking (Bayesian) Chapter 20 - Missing Data &amp; Other Opportunities", " 8.3 Statistical Rethinking (Bayesian) Chapter 20 - Missing Data &amp; Other Opportunities Comment: Missing data lecture start at approximately at 32:00 "],["why-detecting-the-missing-data-mechanism-is-important.html", "8.4 Why detecting the missing data mechanism is important?", " 8.4 Why detecting the missing data mechanism is important? Imputation methods make assumptions on the type of missing data mechanism. Most machine learning algorithms do not accept data with missing values (see section 8.2 - Models that are Resistant to Missing Values). Justifies the strategy to deal with missing data. Missing Values Treatment "],["visualizing-missing-information.html", "8.5 Visualizing Missing Information", " 8.5 Visualizing Missing Information Load scat dataset data(scat) scat %&gt;% glimpse() ## Rows: 110 ## Columns: 19 ## $ Species &lt;fct&gt; coyote, coyote, bobcat, coyote, coyote, coyote, bobcat, bobc… ## $ Month &lt;fct&gt; January, January, January, January, January, January, Januar… ## $ Year &lt;int&gt; 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, 2012, … ## $ Site &lt;fct&gt; YOLA, YOLA, YOLA, YOLA, YOLA, YOLA, ANNU, ANNU, ANNU, ANNU, … ## $ Location &lt;fct&gt; edge, edge, middle, middle, edge, edge, off_edge, off_edge, … ## $ Age &lt;int&gt; 5, 3, 3, 5, 5, 5, 1, 3, 5, 5, 3, 1, 3, 3, 1, 5, 5, 5, 5, 3, … ## $ Number &lt;int&gt; 2, 2, 2, 2, 4, 3, 5, 7, 2, 1, 1, 1, 1, 1, 1, 1, 7, 6, 4, 3, … ## $ Length &lt;dbl&gt; 9.5, 14.0, 9.0, 8.5, 8.0, 9.0, 6.0, 5.5, 11.0, 20.5, 8.0, 8.… ## $ Diameter &lt;dbl&gt; 25.7, 25.4, 18.8, 18.1, 20.7, 21.2, 15.7, 21.9, 17.5, 18.0, … ## $ Taper &lt;dbl&gt; 41.9, 37.1, 16.5, 24.7, 20.1, 28.5, 8.2, 19.3, 29.1, 21.4, N… ## $ TI &lt;dbl&gt; 1.63, 1.46, 0.88, 1.36, 0.97, 1.34, 0.52, 0.88, 1.66, 1.19, … ## $ Mass &lt;dbl&gt; 15.89, 17.61, 8.40, 7.40, 25.45, 14.14, 14.82, 26.41, 16.24,… ## $ d13C &lt;dbl&gt; -26.85, -29.62, -28.73, -20.07, -23.24, -29.00, -28.06, -27.… ## $ d15N &lt;dbl&gt; 6.94, 9.87, 8.52, 5.79, 7.01, 8.28, 4.20, 3.89, 7.34, 6.06, … ## $ CN &lt;dbl&gt; 8.50, 11.30, 8.10, 11.50, 10.60, 9.00, 5.40, 5.60, 5.80, 7.7… ## $ ropey &lt;int&gt; 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, … ## $ segmented &lt;int&gt; 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, … ## $ flat &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, … ## $ scrape &lt;int&gt; 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … Skim scat skimr::skim(scat) %&gt;% knitr::kable() skim_type skim_variable n_missing complete_rate factor.ordered factor.n_unique factor.top_counts numeric.mean numeric.sd numeric.p0 numeric.p25 numeric.p50 numeric.p75 numeric.p100 numeric.hist factor Species 0 1.0000000 FALSE 3 bob: 57, coy: 28, gra: 25 NA NA NA NA NA NA NA NA factor Month 0 1.0000000 FALSE 9 Nov: 17, Jan: 16, Apr: 14, Sep: 14 NA NA NA NA NA NA NA NA factor Site 0 1.0000000 FALSE 2 ANN: 92, YOL: 18 NA NA NA NA NA NA NA NA factor Location 0 1.0000000 FALSE 3 mid: 47, edg: 38, off: 25 NA NA NA NA NA NA NA NA numeric Year 0 1.0000000 NA NA NA 2011.9363636 0.7074605 2011.00 2011.0000 2012.000 2012.000 2013.00 ▅▁▇▁▃ numeric Age 0 1.0000000 NA NA NA 3.3454545 1.3709728 1.00 3.0000 3.000 5.000 5.00 ▃▁▇▃▆ numeric Number 0 1.0000000 NA NA NA 2.6181818 1.4270121 1.00 2.0000 2.000 3.000 7.00 ▇▃▂▁▁ numeric Length 0 1.0000000 NA NA NA 9.2981818 3.4372749 2.50 6.5000 9.000 11.500 20.50 ▆▇▇▂▁ numeric Diameter 6 0.9454545 NA NA NA 18.5586538 3.8820126 7.80 16.0750 18.050 21.325 30.00 ▁▅▇▅▁ numeric Taper 17 0.8454545 NA NA NA 27.4333333 15.0551330 2.30 17.3000 25.800 37.400 91.50 ▇▇▃▁▁ numeric TI 17 0.8454545 NA NA NA 1.6015054 1.0061106 0.23 0.9900 1.430 1.890 8.68 ▇▂▁▁▁ numeric Mass 1 0.9909091 NA NA NA 12.4552294 8.8487894 0.94 5.6600 9.750 17.610 53.70 ▇▃▂▁▁ numeric d13C 2 0.9818182 NA NA NA -26.8601852 2.1755519 -29.85 -28.0825 -27.470 -26.445 -19.67 ▇▇▂▂▁ numeric d15N 2 0.9818182 NA NA NA 7.4364815 3.0164537 1.84 5.6200 6.885 8.305 18.00 ▂▇▂▁▁ numeric CN 2 0.9818182 NA NA NA 8.3987963 3.6622504 4.50 6.2000 7.250 8.650 23.60 ▇▂▁▁▁ numeric ropey 0 1.0000000 NA NA NA 0.5636364 0.4982036 0.00 0.0000 1.000 1.000 1.00 ▆▁▁▁▇ numeric segmented 0 1.0000000 NA NA NA 0.5636364 0.4982036 0.00 0.0000 1.000 1.000 1.00 ▆▁▁▁▇ numeric flat 0 1.0000000 NA NA NA 0.0545455 0.2281302 0.00 0.0000 0.000 0.000 1.00 ▇▁▁▁▁ numeric scrape 0 1.0000000 NA NA NA 0.0454545 0.2092522 0.00 0.0000 0.000 0.000 1.00 ▇▁▁▁▁ Plot missing values vis_dat(scat) scat %&gt;% plot_missing() vis_miss(scat) Plot pattern of missingness using an upset plot gg_miss_upset(scat, nsets = 7) MCAR test mcar_test(scat) ## # A tibble: 1 × 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 169. 65 3.64e-11 5 The MCAR hypothesis test result in a p-value &lt; 0.05, indicating that the missing data mechanism is not random. "],["exploring-pairwise-relationships-between-predictors.html", "8.6 Exploring pairwise relationships between predictors", " 8.6 Exploring pairwise relationships between predictors Plot a scatterplot with diameter’ vs ‘mass’ with ‘flat’ indicator (fig. 8.2) scat_flat &lt;- scat %&gt;% mutate(flat = ifelse(flat == 1, &quot;yes&quot;, &quot;no&quot;)) scat_flat %&gt;% ggplot(aes(col = flat)) + geom_point(aes(x = Diameter, y = Mass), alpha = .5) + geom_rug(data = scat_flat[is.na(scat_flat$Mass),], aes(x = Diameter), sides = &quot;b&quot;, lwd = 1)+ geom_rug(data = scat_flat[is.na(scat_flat$Diameter),], aes(y = Mass), sides = &quot;l&quot;, lwd = 1) + theme(legend.position = &quot;top&quot;) ## Warning: Removed 7 rows containing missing values (geom_point). scat_flat_NA &lt;- scat %&gt;% mutate(diameter_NA = are_na(Diameter) %&gt;% as.numeric()) cor(scat_flat_NA$flat, scat_flat_NA$diameter_NA) ## [1] 1 "],["missing-values-for-the-chicago-ridership-data.html", "8.7 Missing Values for the Chicago ridership data", " 8.7 Missing Values for the Chicago ridership data Chicago ridership raw entries only_rides &lt;- raw_entries %&gt;% select(-date) only_rides %&gt;% glimpse() ## Rows: 5,733 ## Columns: 146 ## $ s_40010 &lt;dbl&gt; 0.290, 1.240, 1.412, 1.388, 1.465, 0.613, 0.403, 1.463, 1.505,… ## $ s_40020 &lt;dbl&gt; 0.633, 2.950, 3.107, 3.259, 3.357, 1.569, 0.887, 3.222, 3.281,… ## $ s_40030 &lt;dbl&gt; 0.483, 1.230, 1.394, 1.370, 1.453, 0.839, 0.589, 1.500, 1.547,… ## $ s_40040 &lt;dbl&gt; 0.374, 7.737, 8.051, 8.027, 7.653, 0.844, 0.464, 8.371, 8.351,… ## $ s_40050 &lt;dbl&gt; 0.804, 3.199, 3.476, 3.540, 3.684, 2.467, 1.367, 3.544, 3.612,… ## $ s_40060 &lt;dbl&gt; 1.165, 4.046, 4.153, 4.362, 4.400, 2.231, 1.565, 4.599, 4.725,… ## $ s_40070 &lt;dbl&gt; 0.649, 5.777, 6.482, 6.766, 6.308, 1.798, 1.055, 7.341, 7.537,… ## $ s_40080 &lt;dbl&gt; 1.116, 3.854, 4.147, 4.202, 4.404, 2.545, 1.823, 4.527, 4.514,… ## $ s_40090 &lt;dbl&gt; 0.411, 1.823, 1.905, 2.008, 2.088, 1.024, 0.561, 2.095, 2.185,… ## $ s_40100 &lt;dbl&gt; 1.698, 3.807, 4.047, 4.107, 4.381, 3.021, 2.187, 4.411, 4.399,… ## $ s_40120 &lt;dbl&gt; 0.318, 1.654, 1.777, 1.878, 1.825, 0.650, 0.380, 2.009, 2.088,… ## $ s_40130 &lt;dbl&gt; 0.364, 0.913, 1.071, 0.994, 1.068, 0.599, 0.409, 1.174, 1.066,… ## $ s_40140 &lt;dbl&gt; 0.000, 2.107, 2.363, 2.379, 2.334, 0.000, 0.000, 2.522, 2.572,… ## $ s_40150 &lt;dbl&gt; 0.000, 0.398, 0.397, 0.456, 0.450, 0.004, 0.000, 0.461, 0.483,… ## $ s_40160 &lt;dbl&gt; 0.096, 3.508, 3.809, 3.820, 3.547, 0.310, 0.154, 3.780, 3.902,… ## $ s_40170 &lt;dbl&gt; 0.246, 1.142, 1.235, 1.343, 1.216, 0.558, 0.339, 1.460, 1.541,… ## $ s_40180 &lt;dbl&gt; 0.157, 1.156, 1.282, 1.357, 1.298, 0.358, 0.205, 1.421, 1.429,… ## $ s_40190 &lt;dbl&gt; 1.131, 2.661, 2.843, 3.023, 3.184, 1.977, 1.515, 3.420, 3.499,… ## $ s_40200 &lt;dbl&gt; 0.834, 5.194, 5.472, 5.503, 5.529, 1.885, 0.923, 5.616, 5.629,… ## $ s_40210 &lt;dbl&gt; 0.000, 0.566, 0.613, 0.629, 0.627, 0.001, 0.000, 0.666, 0.643,… ## $ s_40220 &lt;dbl&gt; 0.318, 0.702, 0.812, 0.774, 0.860, 0.654, 0.414, 0.945, 0.947,… ## $ s_40230 &lt;dbl&gt; 0.788, 4.110, 4.436, 4.541, 4.461, 1.520, 1.028, 4.885, 4.947,… ## $ s_40240 &lt;dbl&gt; 2.470, 6.075, 6.694, 7.097, 7.437, 4.715, 3.316, 7.496, 7.586,… ## $ s_40250 &lt;dbl&gt; 0.448, 1.074, 1.084, 1.106, 1.232, 0.868, 0.586, 1.280, 1.276,… ## $ s_40260 &lt;dbl&gt; 2.059, 7.286, 7.781, 7.808, 7.954, 3.662, 2.483, 8.082, 8.063,… ## $ s_40270 &lt;dbl&gt; 0.279, 1.104, 1.139, 1.203, 1.187, 0.688, 0.352, 1.208, 1.246,… ## $ s_40280 &lt;dbl&gt; 0.700, 1.835, 1.763, 2.076, 2.099, 1.208, 0.856, 2.240, 2.335,… ## $ s_40290 &lt;dbl&gt; 0.540, 1.523, 1.592, 1.698, 1.720, 0.862, 0.632, 1.810, 1.825,… ## $ s_40300 &lt;dbl&gt; 0.199, 0.456, 0.514, 0.474, 0.551, 0.279, 0.242, 0.573, 0.596,… ## $ s_40310 &lt;dbl&gt; 0.460, 2.627, 2.846, 3.086, 3.073, 0.994, 0.614, 3.319, 3.344,… ## $ s_40320 &lt;dbl&gt; 0.854, 3.650, 4.029, 4.158, 4.139, 1.933, 1.290, 4.370, 4.476,… ## $ s_40330 &lt;dbl&gt; 2.542, 7.790, 8.301, 8.543, 8.871, 5.234, 3.823, 8.221, 8.358,… ## $ s_40340 &lt;dbl&gt; 1.046, 2.891, 3.114, 3.176, 3.295, 1.979, 1.502, 3.298, 3.385,… ## $ s_40350 &lt;dbl&gt; 0.273, 1.775, 1.945, 2.049, 2.145, 0.729, 0.485, 4.663, 4.676,… ## $ s_40360 &lt;dbl&gt; 0.417, 2.409, 2.635, 2.672, 2.795, 0.964, 0.545, 2.691, 2.704,… ## $ s_40370 &lt;dbl&gt; 1.039, 7.757, 8.257, 8.303, 8.482, 2.143, 1.399, 8.397, 8.452,… ## $ s_40380 &lt;dbl&gt; 1.080, 13.263, 14.416, 15.118, 14.980, 2.267, 1.405, 15.561, 1… ## $ s_40390 &lt;dbl&gt; 0.660, 2.971, 3.241, 3.260, 3.395, 1.199, 0.752, 3.597, 3.672,… ## $ s_40400 &lt;dbl&gt; 0.072, 0.389, 0.459, 0.482, 0.534, 0.336, 0.180, 0.549, 0.553,… ## $ s_40420 &lt;dbl&gt; 0.000, 0.754, 0.791, 0.820, 0.892, 0.003, 0.000, 0.845, 0.836,… ## $ s_40430 &lt;dbl&gt; 0.546, 2.329, 2.409, 2.398, 2.389, 0.804, 0.676, 2.403, 2.402,… ## $ s_40440 &lt;dbl&gt; 0.000, 0.588, 0.677, 0.728, 0.723, 0.001, 0.000, 0.805, 0.821,… ## $ s_40450 &lt;dbl&gt; 3.948, 11.692, 12.824, 13.091, 13.263, 7.284, 5.134, 14.409, 1… ## $ s_40460 &lt;dbl&gt; 0.185, 5.350, 5.766, 5.824, 5.554, 0.775, 0.231, 6.481, 6.477,… ## $ s_40470 &lt;dbl&gt; 0.286, 1.142, 1.388, 1.574, 1.510, 0.765, 0.393, 2.528, 2.532,… ## $ s_40480 &lt;dbl&gt; 0.405, 1.019, 1.145, 1.222, 1.157, 0.709, 0.486, 1.321, 1.366,… ## $ s_40490 &lt;dbl&gt; 0.182, 1.003, 1.073, 1.106, 1.129, 0.533, 0.313, 1.132, 1.134,… ## $ s_40500 &lt;dbl&gt; 1.181, 6.507, 6.783, 6.906, 6.802, 2.582, 1.498, 6.909, 7.123,… ## $ s_40510 &lt;dbl&gt; 0.248, 0.617, 0.657, 0.697, 0.770, 0.443, 0.295, 0.695, 0.696,… ## $ s_40520 &lt;dbl&gt; 0.126, 0.546, 0.641, 0.720, 0.683, 0.416, 0.223, 0.708, 0.736,… ## $ s_40530 &lt;dbl&gt; 0.670, 3.858, 4.124, 4.151, 4.288, 1.825, 0.925, 4.380, 4.441,… ## $ s_40540 &lt;dbl&gt; 1.449, 3.519, 4.211, 4.132, 4.144, 2.780, 1.913, 4.603, 4.519,… ## $ s_40550 &lt;dbl&gt; 0.731, 3.444, 3.519, 3.591, 3.624, 1.582, 1.200, 3.744, 3.853,… ## $ s_40560 &lt;dbl&gt; 1.255, 8.712, 9.608, 9.735, 9.586, 3.411, 1.909, 12.197, 12.00… ## $ s_40570 &lt;dbl&gt; 0.666, 2.508, 2.617, 2.723, 2.661, 1.355, 0.884, 2.887, 2.863,… ## $ s_40580 &lt;dbl&gt; 0.000, 1.404, 1.466, 1.530, 1.554, 0.004, 0.000, 1.546, 1.495,… ## $ s_40590 &lt;dbl&gt; 0.870, 3.624, 3.793, 3.944, 4.020, 1.735, 1.229, 4.098, 4.180,… ## $ s_40600 &lt;dbl&gt; 0.000, 0.218, 0.269, 0.275, 0.253, 0.001, 0.000, 0.332, 0.337,… ## $ s_40610 &lt;dbl&gt; 0.141, 1.036, 1.222, 1.217, 1.201, 0.372, 0.195, 1.235, 1.303,… ## $ s_40630 &lt;dbl&gt; 2.314, 5.657, 6.012, 6.239, 6.405, 4.631, 3.355, 6.425, 6.630,… ## $ s_40640 &lt;dbl&gt; 0.368, 3.294, 3.523, 3.530, 3.767, 1.093, 0.342, 3.604, 3.832,… ## $ s_40650 &lt;dbl&gt; 1.156, 3.093, 3.263, 3.345, 3.491, 2.621, 1.802, 3.572, 3.605,… ## $ s_40660 &lt;dbl&gt; 0.355, 2.537, 2.831, 2.938, 2.846, 1.090, 0.499, 3.517, 3.566,… ## $ s_40670 &lt;dbl&gt; 0.621, 2.504, 2.601, 2.837, 2.740, 1.270, 0.812, 2.848, 2.959,… ## $ s_40680 &lt;dbl&gt; 0.700, 5.750, 6.149, 6.461, 6.311, 1.979, 0.943, 7.055, 7.619,… ## $ s_40690 &lt;dbl&gt; 0.177, 0.561, 0.628, 0.617, 0.679, 0.420, 0.269, 0.622, 0.612,… ## $ s_40700 &lt;dbl&gt; 0.346, 1.091, 1.128, 1.189, 1.223, 0.657, 0.378, 1.312, 1.416,… ## $ s_40710 &lt;dbl&gt; 0.384, 3.485, 3.797, 3.826, 3.806, 1.225, 0.554, 4.172, 4.248,… ## $ s_40720 &lt;dbl&gt; 0.391, 1.216, 1.316, 1.358, 1.387, 0.675, 0.474, 1.433, 1.399,… ## $ s_40730 &lt;dbl&gt; 0.259, 6.788, 7.321, 7.350, 6.983, 0.662, 0.307, 7.560, 7.576,… ## $ s_40740 &lt;dbl&gt; 0.000, 0.418, 0.457, 0.478, 0.505, 0.002, 0.000, 0.533, 0.505,… ## $ s_40750 &lt;dbl&gt; 0.469, 2.299, 2.443, 2.579, 2.542, 1.012, 0.625, 2.655, 2.760,… ## $ s_40760 &lt;dbl&gt; 1.059, 2.717, 2.878, 2.875, 3.028, 1.932, 1.529, 3.057, 3.047,… ## $ s_40770 &lt;dbl&gt; 0.874, 2.212, 2.441, 2.558, 2.599, 1.874, 1.364, 2.663, 3.093,… ## $ s_40780 &lt;dbl&gt; 0.000, 0.337, 0.419, 0.411, 0.419, 0.002, 0.000, 0.471, 0.471,… ## $ s_40790 &lt;dbl&gt; 0.342, 4.971, 5.431, 5.604, 5.541, 0.948, 0.608, 5.672, 6.013,… ## $ s_40800 &lt;dbl&gt; 0.431, 1.844, 1.954, 2.047, 2.092, 0.936, 0.554, 2.146, 2.257,… ## $ s_40810 &lt;dbl&gt; 0.479, 1.372, 1.628, 1.638, 1.698, 0.965, 0.689, 2.013, 1.826,… ## $ s_40820 &lt;dbl&gt; 0.808, 4.433, 4.769, 5.003, 5.336, 1.929, 1.190, 5.048, 5.338,… ## $ s_40830 &lt;dbl&gt; 0.000, 0.813, 0.881, 0.884, 0.925, 0.002, 0.000, 0.981, 0.979,… ## $ s_40840 &lt;dbl&gt; 0.202, 0.745, 0.842, 0.827, 0.843, 0.431, 0.233, 0.887, 0.875,… ## $ s_40850 &lt;dbl&gt; 0.156, 1.878, 2.095, 2.223, 1.961, 0.598, 0.239, 2.614, 2.563,… ## $ s_40870 &lt;dbl&gt; 0.196, 0.965, 0.986, 1.026, 1.056, 0.425, 0.225, 1.084, 1.123,… ## $ s_40880 &lt;dbl&gt; 0.953, 2.291, 2.476, 2.599, 2.721, 1.790, 1.326, 2.962, 3.104,… ## $ s_40890 &lt;dbl&gt; 6.383, 9.301, 8.584, 8.280, 8.109, 6.109, 6.438, 7.854, 7.518,… ## $ s_40900 &lt;dbl&gt; 2.068, 5.437, 5.814, 5.876, 5.955, 3.828, 2.501, 6.150, 6.324,… ## $ s_40910 &lt;dbl&gt; 1.366, 3.047, 3.350, 3.371, 3.456, 2.437, 1.815, 3.594, 3.768,… ## $ s_40920 &lt;dbl&gt; 0.474, 1.001, 1.060, 1.058, 1.209, 0.845, 0.641, 1.195, 1.232,… ## $ s_40930 &lt;dbl&gt; 2.030, 7.727, 7.448, 7.257, 7.265, 2.536, 1.806, 7.653, 7.692,… ## $ s_40940 &lt;dbl&gt; 0.230, 0.674, 0.681, 0.659, 0.724, 0.439, 0.282, 0.688, 0.682,… ## $ s_40960 &lt;dbl&gt; 0.469, 3.563, 3.922, 3.204, 4.036, 1.183, 0.621, 5.496, 5.345,… ## $ s_40970 &lt;dbl&gt; 0.413, 0.836, 0.949, 0.918, 0.933, 0.730, 0.487, 0.989, 0.986,… ## $ s_40980 &lt;dbl&gt; 0.173, 0.667, 0.727, 0.773, 0.768, 0.379, 0.239, 0.793, 0.756,… ## $ s_40990 &lt;dbl&gt; 2.366, 5.732, 5.977, 6.206, 6.436, 4.497, 3.041, 6.889, 6.899,… ## $ s_41000 &lt;dbl&gt; 1.273, 2.036, 2.114, 2.168, 2.421, 2.167, 1.556, 2.244, 2.331,… ## $ s_41010 &lt;dbl&gt; 0.209, 1.107, 1.186, 1.166, 1.227, 0.523, 0.324, 1.220, 1.302,… ## $ s_41020 &lt;dbl&gt; 0.987, 4.199, 4.366, 4.405, 4.649, 2.204, 1.521, 4.669, 4.828,… ## $ s_41030 &lt;dbl&gt; 0.000, 2.011, 2.217, 2.302, 2.246, 0.008, 0.000, 2.481, 2.436,… ## $ s_41040 &lt;dbl&gt; 0.000, 0.335, 0.380, 0.366, 0.399, 0.000, 0.000, 0.452, 0.463,… ## $ s_41050 &lt;dbl&gt; 0.176, 0.941, 0.993, 1.113, 1.122, 0.406, 0.227, 1.052, 1.094,… ## $ s_41060 &lt;dbl&gt; 0.242, 0.988, 1.091, 1.145, 1.190, 0.543, 0.304, 1.319, 1.314,… ## $ s_41070 &lt;dbl&gt; 0.357, 0.850, 0.943, 0.967, 1.008, 0.607, 0.385, 1.065, 1.204,… ## $ s_41080 &lt;dbl&gt; 0.427, 1.116, 1.216, 1.165, 1.349, 0.754, 0.448, 1.259, 1.224,… ## $ s_41090 &lt;dbl&gt; 0.979, 6.848, 6.922, 7.227, 7.271, 2.410, 1.408, 6.877, 7.537,… ## $ s_41120 &lt;dbl&gt; 0.448, 1.195, 1.357, 1.389, 1.478, 0.765, 0.460, 1.661, 1.701,… ## $ s_41130 &lt;dbl&gt; 0.306, 1.774, 2.029, 2.074, 2.129, 0.689, 0.352, 2.479, 2.556,… ## $ s_41140 &lt;dbl&gt; 0.144, 0.580, 0.627, 0.705, 0.695, 0.403, 0.243, 0.718, 0.690,… ## $ s_41150 &lt;dbl&gt; 0.460, 2.335, 2.588, 2.710, 2.682, 1.142, 0.599, 3.013, 3.020,… ## $ s_41160 &lt;dbl&gt; 0.217, 1.681, 1.733, 1.791, 1.634, 0.395, 0.282, 1.809, 1.901,… ## $ s_41170 &lt;dbl&gt; 1.457, 3.748, 3.977, 4.185, 4.533, 3.341, 1.972, 4.590, 4.511,… ## $ s_41180 &lt;dbl&gt; 0.402, 1.173, 1.403, 1.379, 1.461, 0.883, 0.560, 1.449, 1.417,… ## $ s_41190 &lt;dbl&gt; 0.590, 1.338, 1.345, 1.390, 1.510, 1.072, 0.782, 1.454, 1.496,… ## $ s_41200 &lt;dbl&gt; 0.993, 2.405, 2.569, 2.641, 2.638, 1.805, 1.280, 2.763, 2.847,… ## $ s_41210 &lt;dbl&gt; 0.270, 2.194, 2.449, 2.548, 2.466, 0.759, 0.416, 2.606, 2.639,… ## $ s_41220 &lt;dbl&gt; 1.763, 7.054, 7.519, 7.919, 8.074, 4.535, 3.114, 10.856, 10.85… ## $ s_41230 &lt;dbl&gt; 0.787, 1.902, 1.965, 2.130, 2.182, 1.613, 1.252, 2.370, 2.322,… ## $ s_41240 &lt;dbl&gt; 0.388, 1.998, 2.095, 2.117, 2.106, 0.846, 0.516, 2.500, 2.570,… ## $ s_41250 &lt;dbl&gt; 0.180, 0.842, 0.865, 0.886, 0.847, 0.468, 0.249, 0.963, 0.959,… ## $ s_41260 &lt;dbl&gt; 0.399, 1.621, 1.871, 1.951, 1.985, 0.855, 0.553, 1.983, 2.048,… ## $ s_41270 &lt;dbl&gt; 0.211, 0.640, 0.713, 0.695, 0.724, 0.411, 0.258, 0.821, 0.791,… ## $ s_41280 &lt;dbl&gt; 1.302, 5.812, 6.171, 6.385, 6.334, 2.692, 1.856, 6.595, 6.750,… ## $ s_41290 &lt;dbl&gt; 0.869, 3.335, 3.480, 3.519, 3.730, 2.101, 1.200, 4.059, 4.268,… ## $ s_41300 &lt;dbl&gt; 1.403, 3.508, 3.781, 3.963, 3.967, 2.733, 1.884, 4.306, 4.337,… ## $ s_41310 &lt;dbl&gt; 0.323, 1.795, 2.004, 1.925, 2.067, 0.966, 0.543, 2.028, 2.042,… ## $ s_41320 &lt;dbl&gt; 2.872, 7.703, 8.253, 8.897, 9.639, 6.733, 4.514, 8.922, 9.026,… ## $ s_41330 &lt;dbl&gt; 0.383, 1.637, 1.786, 1.859, 1.819, 0.668, 0.485, 1.836, 1.915,… ## $ s_41340 &lt;dbl&gt; 0.142, 1.866, 2.023, 1.991, 1.888, 0.337, 0.184, 2.254, 2.369,… ## $ s_41350 &lt;dbl&gt; 0.170, 1.272, 1.333, 1.429, 1.375, 0.467, 0.296, 1.419, 1.405,… ## $ s_41360 &lt;dbl&gt; 0.213, 0.619, 0.660, 0.694, 0.805, 0.385, 0.252, 0.756, 0.781,… ## $ s_41380 &lt;dbl&gt; 1.407, 3.983, 4.063, 4.113, 4.189, 2.609, 2.003, 4.444, 4.414,… ## $ s_41400 &lt;dbl&gt; 1.363, 4.775, 5.243, 5.270, 5.184, 3.068, 2.112, 5.809, 5.920,… ## $ s_41410 &lt;dbl&gt; 0.252, 1.706, 1.918, 1.962, 1.997, 0.633, 0.429, 2.071, 2.163,… ## $ s_41420 &lt;dbl&gt; 1.227, 3.937, 4.329, 4.607, 4.666, 2.710, 1.935, 4.785, 4.906,… ## $ s_41430 &lt;dbl&gt; 1.659, 4.577, 4.782, 5.111, 5.266, 3.010, 2.044, 5.464, 5.408,… ## $ s_41440 &lt;dbl&gt; 0.225, 1.512, 1.699, 1.717, 1.736, 0.613, 0.307, 1.868, 1.889,… ## $ s_41450 &lt;dbl&gt; 4.395, 11.058, 11.680, 11.883, 12.771, 8.718, 5.822, 11.698, 1… ## $ s_41460 &lt;dbl&gt; 0.327, 2.040, 2.124, 2.246, 2.362, 0.962, 0.541, 2.402, 2.417,… ## $ s_41480 &lt;dbl&gt; 0.715, 3.194, 3.272, 3.398, 3.346, 1.656, 1.006, 3.609, 3.528,… ## $ s_41490 &lt;dbl&gt; 0.502, 2.390, 2.495, 2.531, 2.202, 1.214, 0.822, 2.903, 2.932,… ## $ s_41500 &lt;dbl&gt; 0.338, 1.710, 1.888, 1.905, 2.049, 0.877, 0.531, 2.001, 2.008,… ## $ s_41510 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ s_41580 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ s_41660 &lt;dbl&gt; 2.942, 12.087, 12.622, 12.936, 13.043, 5.444, 3.579, 13.170, 1… ## $ s_41670 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ s_41680 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ s_41690 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… Skim only_rides skimr::skim(only_rides) (#tab:onle_rides_skim)Data summary Name only_rides Number of rows 5733 Number of columns 146 _______________________ Column type frequency: numeric 146 ________________________ Group variables None Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist s_40010 0 1.00 1.52 0.57 0.19 0.96 1.66 1.99 2.73 ▂▅▅▇▂ s_40020 0 1.00 3.08 0.98 0.07 2.34 3.38 3.81 5.98 ▂▅▇▇▁ s_40030 0 1.00 1.45 0.46 0.40 1.11 1.47 1.81 2.59 ▂▅▇▆▁ s_40040 0 1.00 5.59 2.92 0.00 1.85 7.02 7.79 9.82 ▅▁▁▇▅ s_40050 0 1.00 3.33 0.81 0.00 2.90 3.63 3.87 4.88 ▁▂▂▇▃ s_40060 0 1.00 4.09 1.26 0.71 2.91 4.47 4.94 6.42 ▁▃▂▇▃ s_40070 0 1.00 6.03 2.61 0.00 3.33 7.03 7.89 18.41 ▅▆▇▁▁ s_40080 0 1.00 4.38 1.16 0.62 3.62 4.63 5.21 10.59 ▂▆▇▁▁ s_40090 0 1.00 1.74 0.80 0.00 1.12 1.92 2.33 3.41 ▃▅▇▇▂ s_40100 0 1.00 3.80 0.94 0.00 3.21 3.99 4.44 5.80 ▁▁▃▇▂ s_40120 0 1.00 2.21 0.87 0.20 1.41 2.44 2.88 4.86 ▃▂▇▃▁ s_40130 0 1.00 0.96 0.30 0.00 0.72 1.05 1.15 3.01 ▂▇▂▁▁ s_40140 0 1.00 1.80 1.00 0.00 1.00 2.21 2.52 4.08 ▅▂▇▆▁ s_40150 0 1.00 0.76 0.41 0.00 0.44 0.75 1.14 1.75 ▅▇▆▇▁ s_40160 0 1.00 2.24 1.27 0.00 0.52 2.95 3.17 5.01 ▆▁▅▇▁ s_40170 0 1.00 1.79 0.84 0.00 1.19 1.68 2.46 8.35 ▇▇▁▁▁ s_40180 0 1.00 1.32 0.56 0.08 0.73 1.51 1.77 2.51 ▃▂▃▇▁ s_40190 30 0.99 3.93 1.85 0.00 2.81 3.69 4.76 13.83 ▃▇▂▁▁ s_40200 0 1.00 5.73 2.39 0.00 3.93 6.01 7.33 13.51 ▂▃▇▁▁ s_40210 0 1.00 0.91 0.48 0.00 0.57 0.88 1.32 1.95 ▅▇▆▇▂ s_40220 0 1.00 1.19 0.42 0.00 0.90 1.10 1.55 2.58 ▁▇▇▅▁ s_40230 0 1.00 4.05 1.44 0.00 2.85 4.59 5.03 9.47 ▂▂▇▁▁ s_40240 30 0.99 6.55 1.84 0.00 5.26 7.30 7.85 11.82 ▁▂▃▇▁ s_40250 0 1.00 1.62 0.53 0.45 1.28 1.61 2.01 3.03 ▃▅▇▅▁ s_40260 0 1.00 7.63 2.60 0.00 5.69 8.24 9.61 14.15 ▁▃▆▇▁ s_40270 0 1.00 1.03 0.28 0.00 0.85 1.16 1.23 1.92 ▁▃▃▇▁ s_40280 0 1.00 1.99 0.56 0.01 1.49 2.21 2.40 3.89 ▁▆▇▇▁ s_40290 0 1.00 1.44 0.51 0.00 1.00 1.54 1.75 3.96 ▂▇▇▁▁ s_40300 0 1.00 0.69 0.28 0.00 0.47 0.68 0.91 2.38 ▃▇▂▁▁ s_40310 0 1.00 2.87 1.12 0.28 1.77 3.27 3.60 5.74 ▃▃▇▅▁ s_40320 0 1.00 4.33 1.62 0.00 3.00 4.56 5.46 9.36 ▂▃▇▃▁ s_40330 0 1.00 9.37 2.49 0.00 8.21 9.32 10.76 24.73 ▁▇▅▁▁ s_40340 0 1.00 2.98 0.70 0.00 2.55 3.25 3.47 4.29 ▁▁▃▇▆ s_40350 0 1.00 4.04 2.34 0.26 1.87 3.59 6.22 9.64 ▇▇▃▇▁ s_40360 0 1.00 2.33 1.02 0.00 1.56 2.67 3.07 4.56 ▂▃▅▇▁ s_40370 0 1.00 7.47 3.26 0.00 4.58 8.32 9.71 16.11 ▃▂▇▅▁ s_40380 0 1.00 13.61 6.57 0.60 6.17 15.89 18.93 26.06 ▅▂▃▇▁ s_40390 0 1.00 3.12 1.13 0.33 1.90 3.69 3.94 7.35 ▂▂▇▁▁ s_40400 0 1.00 0.60 0.22 0.00 0.46 0.63 0.75 1.69 ▃▇▇▁▁ s_40420 0 1.00 0.92 0.39 0.00 0.71 0.96 1.21 2.32 ▂▆▇▂▁ s_40430 0 1.00 2.44 1.04 0.00 1.43 2.68 3.15 4.56 ▃▅▆▇▃ s_40440 0 1.00 0.93 0.50 0.00 0.58 0.91 1.32 9.70 ▇▁▁▁▁ s_40450 30 0.99 10.87 3.65 0.00 7.67 12.18 13.77 18.90 ▁▃▂▇▁ s_40460 0 1.00 4.68 2.54 0.00 1.78 5.60 6.45 12.10 ▅▁▇▂▁ s_40470 0 1.00 1.83 0.78 0.14 1.10 1.99 2.51 5.81 ▆▇▇▁▁ s_40480 0 1.00 1.21 0.34 0.35 0.95 1.31 1.47 2.28 ▂▃▇▃▁ s_40490 0 1.00 1.56 0.72 0.00 1.12 1.50 2.06 8.83 ▇▅▁▁▁ s_40500 2780 0.52 4.85 3.81 0.00 0.00 5.95 7.98 12.44 ▇▃▃▆▂ s_40510 0 1.00 1.44 2.01 0.19 0.82 1.20 1.44 17.30 ▇▁▁▁▁ s_40520 0 1.00 0.66 0.23 0.00 0.49 0.70 0.83 1.48 ▂▃▇▂▁ s_40530 0 1.00 3.96 1.67 0.00 2.62 4.46 5.24 8.97 ▂▃▇▃▁ s_40540 0 1.00 4.89 1.50 1.10 3.70 5.14 5.93 11.57 ▃▇▇▁▁ s_40550 0 1.00 3.41 1.13 0.00 2.26 3.84 4.25 5.48 ▁▃▂▇▃ s_40560 0 1.00 8.97 3.84 0.00 5.22 10.09 11.90 23.94 ▃▃▇▁▁ s_40570 0 1.00 3.25 1.28 0.00 2.45 3.22 4.16 6.29 ▂▃▇▅▂ s_40580 0 1.00 1.52 0.67 0.00 1.10 1.67 2.03 3.88 ▂▅▇▁▁ s_40590 0 1.00 4.55 1.68 0.00 3.82 4.56 5.61 16.88 ▂▇▁▁▁ s_40600 0 1.00 0.33 0.16 0.00 0.22 0.32 0.44 0.70 ▃▇▇▆▂ s_40610 0 1.00 1.07 0.45 0.01 0.56 1.31 1.39 2.35 ▃▂▇▃▁ s_40630 0 1.00 6.51 1.46 0.00 5.88 6.85 7.45 12.06 ▁▂▇▅▁ s_40640 0 1.00 4.08 2.28 0.00 2.20 4.37 5.88 12.52 ▆▇▇▁▁ s_40650 0 1.00 4.33 1.23 0.00 3.67 4.17 5.20 7.92 ▁▂▇▃▁ s_40660 0 1.00 3.14 1.23 0.01 2.14 3.53 4.04 5.25 ▂▃▂▇▃ s_40670 0 1.00 3.58 1.48 0.02 2.43 3.72 4.56 16.36 ▆▇▁▁▁ s_40680 0 1.00 6.23 2.65 0.00 3.86 6.98 7.89 21.62 ▃▇▂▁▁ s_40690 0 1.00 0.71 0.17 0.00 0.62 0.74 0.82 1.87 ▁▇▇▁▁ s_40700 0 1.00 1.17 0.35 0.31 0.89 1.28 1.42 4.15 ▃▇▁▁▁ s_40710 0 1.00 4.32 2.05 0.03 2.64 4.24 5.96 11.31 ▅▇▆▃▁ s_40720 0 1.00 1.14 0.37 0.22 0.81 1.25 1.38 2.59 ▂▃▇▁▁ s_40730 0 1.00 5.26 2.94 0.00 1.31 6.78 7.36 11.09 ▅▁▂▇▁ s_40740 0 1.00 0.76 0.36 0.00 0.54 0.75 1.08 2.04 ▂▇▇▂▁ s_40750 0 1.00 2.32 0.88 0.00 1.39 2.72 2.95 5.06 ▂▃▇▃▁ s_40760 0 1.00 3.28 0.84 0.00 2.83 3.38 3.79 6.16 ▁▂▇▅▁ s_40770 0 1.00 2.77 0.73 0.00 2.37 2.86 3.27 5.32 ▁▃▇▅▁ s_40780 0 1.00 0.74 0.43 0.00 0.42 0.73 1.11 1.64 ▆▇▆▇▃ s_40790 0 1.00 4.85 2.37 0.00 2.20 5.69 6.33 12.30 ▅▁▇▂▁ s_40800 0 1.00 2.80 1.04 0.00 2.19 2.85 3.66 8.92 ▃▇▃▁▁ s_40810 0 1.00 2.28 1.11 0.32 1.17 2.31 3.11 5.09 ▇▅▇▅▂ s_40820 0 1.00 4.51 1.66 0.13 3.39 4.82 5.53 15.68 ▃▇▁▁▁ s_40830 0 1.00 1.22 0.56 0.00 0.93 1.16 1.67 3.59 ▂▇▆▁▁ s_40840 0 1.00 0.66 0.23 0.00 0.44 0.77 0.82 1.16 ▁▃▁▇▁ s_40850 0 1.00 2.87 1.40 0.00 1.88 2.73 4.02 12.90 ▇▇▁▁▁ s_40870 0 1.00 1.04 0.45 0.00 0.67 1.11 1.42 1.83 ▂▅▆▇▆ s_40880 0 1.00 2.51 0.69 0.00 1.98 2.73 2.99 5.39 ▁▃▇▁▁ s_40890 0 1.00 8.93 1.88 0.00 7.70 8.80 10.01 18.55 ▁▂▇▁▁ s_40900 0 1.00 5.41 1.29 1.38 4.44 5.91 6.28 8.58 ▁▂▂▇▁ s_40910 30 0.99 2.93 0.82 0.00 2.37 3.14 3.53 9.55 ▁▇▁▁▁ s_40920 0 1.00 1.40 0.39 0.44 1.18 1.35 1.73 2.40 ▂▅▇▅▁ s_40930 0 1.00 7.32 2.40 1.19 5.03 8.20 9.05 14.97 ▂▂▇▂▁ s_40940 0 1.00 0.70 0.33 0.00 0.48 0.69 0.83 2.90 ▅▇▁▁▁ s_40960 0 1.00 4.08 1.70 0.29 2.23 4.89 5.40 9.31 ▃▂▇▁▁ s_40970 0 1.00 1.08 0.30 0.25 0.86 1.07 1.31 1.83 ▁▅▇▆▂ s_40980 0 1.00 0.83 0.29 0.03 0.60 0.87 1.05 1.58 ▁▅▇▇▁ s_40990 30 0.99 5.12 1.45 0.00 4.19 5.60 6.14 8.74 ▁▂▃▇▁ s_41000 30 0.99 3.48 1.00 0.00 2.95 3.51 4.08 11.70 ▁▇▁▁▁ s_41010 0 1.00 1.26 0.53 0.00 0.79 1.38 1.68 2.20 ▂▃▃▇▃ s_41020 0 1.00 4.87 1.74 0.76 3.60 4.90 6.10 13.15 ▃▇▅▁▁ s_41030 0 1.00 2.32 1.27 0.00 0.88 2.87 3.34 4.51 ▅▁▂▇▂ s_41040 0 1.00 0.64 0.37 0.00 0.37 0.62 0.95 2.44 ▇▇▅▁▁ s_41050 0 1.00 0.90 0.35 0.00 0.75 0.93 1.08 2.35 ▃▇▇▂▁ s_41060 0 1.00 1.25 0.42 0.15 0.86 1.39 1.57 2.28 ▂▃▅▇▁ s_41070 0 1.00 1.17 0.38 0.32 0.86 1.23 1.44 2.28 ▃▃▇▃▁ s_41080 0 1.00 1.13 0.37 0.00 0.88 1.20 1.35 2.80 ▁▅▇▁▁ s_41090 0 1.00 7.17 3.15 0.00 4.68 7.24 9.74 17.70 ▃▆▇▂▁ s_41120 0 1.00 1.88 0.87 0.14 1.31 1.92 2.27 8.53 ▆▇▁▁▁ s_41130 0 1.00 2.10 0.84 0.16 1.25 2.42 2.69 6.76 ▅▇▂▁▁ s_41140 0 1.00 0.57 0.18 0.08 0.43 0.61 0.69 1.34 ▂▅▇▁▁ s_41150 0 1.00 2.52 0.91 0.26 1.61 2.91 3.19 4.93 ▃▃▇▇▁ s_41160 0 1.00 2.63 1.51 0.10 1.38 2.29 4.24 5.44 ▆▇▂▇▅ s_41170 30 0.99 3.45 0.98 0.00 2.90 3.77 4.13 5.99 ▁▂▃▇▁ s_41180 0 1.00 1.56 0.63 0.00 1.23 1.60 1.90 5.46 ▂▇▁▁▁ s_41190 0 1.00 1.33 0.31 0.00 1.17 1.40 1.53 2.15 ▁▁▃▇▁ s_41200 0 1.00 2.46 0.60 0.00 2.12 2.57 2.79 4.70 ▁▂▇▃▁ s_41210 0 1.00 2.04 1.11 0.00 1.03 2.43 2.81 10.48 ▅▇▁▁▁ s_41220 0 1.00 10.23 3.39 0.70 7.99 10.42 12.60 18.06 ▁▅▇▆▂ s_41230 30 0.99 2.63 0.78 0.00 2.10 2.87 3.20 7.39 ▂▇▇▁▁ s_41240 0 1.00 2.17 0.83 0.00 1.35 2.48 2.77 3.85 ▁▃▂▇▁ s_41250 0 1.00 0.75 0.37 0.00 0.44 0.84 0.91 5.50 ▇▁▁▁▁ s_41260 0 1.00 1.67 0.55 0.02 1.15 1.93 2.08 3.24 ▁▅▅▇▁ s_41270 0 1.00 0.83 0.30 0.00 0.58 0.88 1.04 1.94 ▂▃▇▁▁ s_41280 0 1.00 5.39 1.83 0.26 3.44 6.24 6.73 11.33 ▂▃▇▂▁ s_41290 0 1.00 3.25 1.11 0.00 2.44 3.66 4.08 5.62 ▁▂▂▇▁ s_41300 0 1.00 4.58 1.22 0.88 3.82 4.73 5.46 8.15 ▁▃▇▅▁ s_41310 0 1.00 1.94 0.88 0.00 1.26 2.12 2.55 4.53 ▂▃▇▃▁ s_41320 0 1.00 10.41 2.50 1.24 9.15 10.39 12.23 36.32 ▂▇▁▁▁ s_41330 0 1.00 1.71 0.67 0.00 1.06 1.91 2.14 3.09 ▁▃▂▇▂ s_41340 0 1.00 2.13 1.00 0.00 1.16 2.37 2.93 5.08 ▃▂▇▃▁ s_41350 0 1.00 1.29 0.44 0.02 0.90 1.45 1.62 2.67 ▂▅▇▅▁ s_41360 0 1.00 0.86 0.29 0.19 0.61 0.89 1.11 1.50 ▃▃▆▇▁ s_41380 0 1.00 4.06 0.99 0.00 3.30 4.40 4.77 6.59 ▁▂▂▇▁ s_41400 0 1.00 8.29 2.69 1.24 6.29 8.11 10.51 21.73 ▂▇▆▁▁ s_41410 0 1.00 2.74 1.20 0.00 1.86 2.69 3.70 5.09 ▂▃▇▅▅ s_41420 0 1.00 7.27 3.59 0.13 5.17 6.11 7.83 21.33 ▂▇▂▁▁ s_41430 30 0.99 4.21 1.28 0.00 3.34 4.71 5.15 9.67 ▁▃▇▁▁ s_41440 0 1.00 1.71 0.86 0.00 0.97 1.86 2.36 4.38 ▃▃▇▂▁ s_41450 0 1.00 12.91 3.01 0.00 11.68 13.44 14.83 25.28 ▁▂▇▂▁ s_41460 0 1.00 2.06 0.98 0.00 1.24 2.28 2.88 6.14 ▅▇▇▁▁ s_41480 0 1.00 3.19 0.96 0.39 2.61 3.35 3.94 7.25 ▂▃▇▁▁ s_41490 0 1.00 3.09 1.24 0.00 2.14 3.03 3.99 7.89 ▂▇▆▂▁ s_41500 0 1.00 1.90 0.85 0.00 1.23 2.07 2.58 4.04 ▂▃▇▆▁ s_41510 4138 0.28 1.89 0.73 0.00 1.29 1.98 2.44 4.28 ▂▆▇▃▁ s_41580 5702 0.01 0.00 0.00 0.00 0.00 0.00 0.00 0.03 ▇▁▁▁▁ s_41660 0 1.00 13.20 5.50 0.00 8.78 13.61 17.28 28.46 ▂▅▇▅▁ s_41670 151 0.97 0.70 0.25 0.00 0.53 0.71 0.91 1.84 ▂▇▇▁▁ s_41680 4108 0.28 0.67 0.36 0.00 0.38 0.82 0.96 1.26 ▃▅▂▇▃ s_41690 5113 0.11 1.12 0.47 0.00 0.86 1.19 1.39 3.55 ▂▇▂▁▁ Select stations with missing values only only_rides %&gt;% select_if(~any(is.na(.))) %&gt;% plot_missing() MCAR test mcar_test(only_rides) ## Warning in norm::prelim.norm(data): NAs introduced by coercion to integer range ## # A tibble: 1 × 4 ## statistic df p.value missing.patterns ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 18032. 842 0 7 Plot pattern of missingness using an upset plot gg_miss_upset(only_rides, nsets = 10) Stations 40500 (Washington), 41680 (Oakton-Skokie), 41510 (Morgan), 41690 (Cermak-McCormick Place) and 41580 (Homan) missing values are highly related. "],["missing-data-patterns-for-stations-originally-in-the-chicago-ridership-data.html", "8.8 Missing data patterns for stations originally in the Chicago ridership data", " 8.8 Missing data patterns for stations originally in the Chicago ridership data Fig. 8.5 miss_entries &lt;- raw_entries %&gt;% dplyr::select(-date) %&gt;% is.na() miss_num &lt;- apply(miss_entries, 2, sum) has_missing &lt;- vapply(raw_entries[, -1], function(x) sum(is.na(x)) &gt; 1, logical(1)) miss_station &lt;- names(has_missing)[has_missing] # do clustering on just the station data (not time) and get a reordering # of the stations for plotting miss_data &lt;- raw_entries[, miss_station] %&gt;% is.na() clst &lt;- hclust(dist(t(miss_data))) clst_stations &lt;- tibble( station_id = colnames(miss_data), order = clst$order ) station_names &lt;- stations %&gt;% dplyr::select(name, station_id) %&gt;% right_join(clst_stations, by = &quot;station_id&quot;) station_lvl &lt;- station_names[[&quot;name&quot;]][station_names$order] miss_vert &lt;- raw_entries %&gt;% gather(station_id, raw_entries, -date) %&gt;% filter(station_id %in% miss_station) %&gt;% mutate(status = ifelse(is.na(raw_entries), &quot;missing&quot;, &quot;complete&quot;)) %&gt;% full_join(station_names, by = &quot;station_id&quot;) %&gt;% mutate( name = factor(name, levels = station_lvl), status = factor(status, levels = c(&quot;missing&quot;, &quot;complete&quot;)) ) miss_vert %&gt;% ggplot(aes(x = date, y = name, fill = status)) + geom_tile() + ylab(&quot;&quot;) + xlab(&quot;&quot;) + scale_fill_grey() + theme(legend.position = &#39;top&#39;) There are nine stations whose data are almost complete except for a single month gap. These stations are all on the Red Line and occur during the time of the Red Line Reconstruction Project that affected stations north of Cermak-Chinatown to the 95th Street station. "],["models-that-are-resistant-to-missing-values.html", "8.9 Models that are Resistant to Missing Values", " 8.9 Models that are Resistant to Missing Values Many popular predictive models such as support vector machines, the glmnet, and neural networks, cannot tolerate any amount of missing values. However, certain implementations of tree-based models have clever procedures to accommodate incomplete data (ex. CART, Naive Bayes). "],["deletion-of-data.html", "8.10 Deletion of Data", " 8.10 Deletion of Data When it is desirable to use models that are intolerant to missing data, then the missing values must be extricated from the data. However, one must carefully consider a number of aspects of the data prior to taking this approach. "],["encoding-missingness.html", "8.11 Encoding Missingness", " 8.11 Encoding Missingness When a predictor is discrete in nature, missingness can be directly encoded into the predictor as if it were a naturally occurring category. This makes sense for structurally missing values such as the example of alleys in the Ames housing data. Here, it is sensible to change the missing values to a category of “no alley.” In other cases, the missing values could simply be encoded as “missing” or “unknown.” "],["imputation-methods.html", "8.12 Imputation methods", " 8.12 Imputation methods Another approach to handling missing values is to impute or estimate them. Missing value imputation has a long history in statistics and has been thoroughly researched. Good places to start are Little and Rubin (2014), Van Buuren (2012) and Allison (2001). In essence, imputation uses information and relationships among the non-missing predictors to provide an estimate to fill in the missing value. Imputation methods: KNN (K-Nearest Neighbors) Tree-based methods (bagging, missRanger) Linear methods "],["summary-1.html", "8.13 Summary", " 8.13 Summary Missing values are common occurrences in data. Unfortunately, most predictive modeling techniques cannot handle any missing values. Therefore, this problem must be addressed prior to modeling. Missing data may occur due to random chance or due to a systematic cause. Understanding the nature of the missing values can help to guide the decision process about how best to remove or impute the data. "],["meeting-videos-6.html", "8.14 Meeting Videos", " 8.14 Meeting Videos 8.14.1 Cohort 1 Meeting chat log LOG "],["working-with-profile-data.html", "Chapter 9 Working with Profile Data", " Chapter 9 Working with Profile Data What even is profile data??? It’s sort of like Inception Profile data refers to datasets that have complex hierarchcical relationships.An example would be if you’re trying to predict whether a student would pass a course. You might have demographic data on that student, but you’d probably also have data on the courses they took, absences grades etc.. There might also be a time component related to how quickly they’re finishing their degree. There are a lot of layers to this, and feature engineering can help us create better models, insofar as we understand the data’s structure. Learning objectives: Be able to preprocess profile data to make adequate predictions "],["illustrative-data-pharmaceutical-manufacturing-monitoring.html", "9.1 Illustrative Data: Pharmaceutical Manufacturing Monitoring", " 9.1 Illustrative Data: Pharmaceutical Manufacturing Monitoring 9.1.1 Introduction Pharmaceutical companies use spectroscopy measurements to assess critical process parameters during the manufacturing of a biological drug (Berry et al. 2015). Models built on this process can be used with real-time data to recommend changes that can increase product yield. In the example that follows, Raman spectroscopy was used to generate the data77 (Hammes 2005). To manufacture the drug being used for this example, a specific type of protein is required and that protein can be created by a particular type of cell. A batch of cells are seeded into a bioreactor which is a device that is designed to help grow and maintain the cells. In production, a large bioreactor would be about 2000 liters and is used to make large quantities of proteins in about two weeks. Company is trying to produce a biological drug that produces special proteins. Cell are placed in a bioreactor where they produce the proteins (the yield) Factors like food and temperature can be recorded, as well as cell specific cellular data like glucose production and ammonia (which can negatively impact the yield) Measuring glucose and ammonia is time consuming, so spectroscopy measurements are taken from 15 small reactors and 3 large reactors. A schematic for the experimental design for pharmaceutical manufacturing. The goal here is to see if we can take the results from the small reactors to predict the results of the big reactors. 9.1.2 IMPORTANT DEFINITIONS ! Spectroscopy is the general field of study that measures and interprets the electromagnetic spectra that result from the interaction between electromagnetic radiation and matter as a function of the wavelength or frequency of the radiation. Spectra/Spectrum, a band of colors, as seen in a rainbow, produced by separation of the components of light by their different degrees of refraction according to wavelength. Radiant Intensity: In radiometry, radiant intensity is the radiant flux emitted, reflected, transmitted or received, per unit solid angle, and spectral intensity is the radiant intensity per unit frequency or wavelength 9.1.3 Preliminary Results Looking at the spectra over multiple days we see that the intensity exhibits similar patterns in the small and large reactors. The difference being the big reactors lead to slightly higher intensities (a) Spectra for a small- and large-scale reactor on days 1, 7, and 14. (b) The spectra for a single reactor over 14 days. "],["what-are-the-experimental-unit-and-the-unit-of-prediction.html", "9.2 What are the Experimental Unit and the Unit of Prediction?", " 9.2 What are the Experimental Unit and the Unit of Prediction? Nearly 2600 wavelengths are measured each day for two weeks for each of 15 small-scale bioreactors. This type of data forms a hierarchical, or nested, structure in which wavelengths are measured within each day and within each bioreactor. The key characteristic of this is that the data within a nesting is more related than data between nestings. For example, the spectra within a day are more related to each other than between different days, AND the wavelengths within the days are more correlated with each other than wavelengths between reactors. WOW this is confusing Interrelated correlations can be visualized through a plot of autocorrelations. In the plot below, we can see that the autocorrelation between wavelengths is different on different days. (a) Autocorrelations for selected lags of wavelengths for small-scale bioreactor 1 on the first day. (b) Autocorrelations for lags of days for average wavelength intensity for small-scale bioreactor 1. Figure 9.4 (b) shows the autocorrelations for the first 13 lagged days. Here correlations for the first lag is greater than 0.95, with correlations tailing off fairly quickly. So what is the unit of prediction? Spectra? Number of days to reach a certain wavelength? Because everything inside a bioreactor is independent of other bioreactors, the unit of analysis is day within a bioreactor. Everything WITHIN a bioreactor is going to have autocorrelation. The unit of analysis will help us decide how to use cross-validation to evaluate our results honestly. Leave one out or kfold cross validation should be used. We can’t do cross validation on days or wavelengths because they’re not independent units. "],["reducing-background.html", "9.3 Reducing Background", " 9.3 Reducing Background Excess variation due to spurious sources can have a detrimental impact on the models (the book mentions principal component regression and partial least square). Removing all the background noise is almost impossible, but we can approximate it. For spectroscopy data, intensity deviations from zero are called baseline drift and are generally due to noise in the measurement system, interference, or fluorescence (Rinnan, Van Den Berg, and Engelsen 2009) and are not due to the actual chemical substance of the sample. To get rid of this noise, the author uses a polynomial fit on the lowest intensities. Then they take the negative residuals and subtract those data points. A polynomial baseline correction for the small-scale bioreactor 1, day 1 intensities. "],["reducing-other-noise.html", "9.4 Reducing Other Noise", " 9.4 Reducing Other Noise There can also be systemic variation between bioreactor measurement instruments rather than difference in the amount of molecules being sampled. You saw in the graph comparing small and large bioreactors that although the peaks were the same, the intensity appeared higher for large ones. Given that these peaks are most important, we can rescale the data across reactors to ensure results are comparable. The author uses standard normal variate (SNV) from spectroscopy literature to rescale the data based on the mean and standard deviation. Since these measures can be influenced by outliers, trimming is used to take out extreme values. Figure 9.6 compares the profiles of the spectrum with the lowest variation and highest variation for (a) the baseline corrected data across all days and small-scale bioreactors. This figure demonstrates that the amplitudes of the profiles can vary greatly. Figure 9.6: (a) The baseline-corrected intensities for the spectra that are the most- and least variable. (b) The spectra after standardizing each to have a mean of 0 and standard deviation of 1. Another source of noise here was the intensity measurements for EACH wavelength within a spectrum. The author uses splines and moving averages to smooth out the intensities. The key here was to select the correct number of points to include in the moving averages. Figure 9.7: The moving average of lengths 5, 15, and 50 applied to the first day of the first small-scale bioreactor for wavelengths 950 through 1200. "],["exploiting-correlation.html", "9.5 Exploiting Correlation", " 9.5 Exploiting Correlation The previous two chapter were about reducing the noise , which allows us to make better predictions. However, this does not help us deal with the correlation between features -in this case the correlation between wavelengths within a spectra One approach to dealing with correlation is principal component analysis (PCA), which gets rid of the correlation between features, but isn’t guaranteed to improve the predictive power because the output isn’t taken into account. Figure 9.8: PCA dimension reduction applied across all small-scale data. (a) A scree plot of the cumulative variability explained across components. (b) Scatterplots of Glucose and the first three principal components. Figure 9.8: PCA dimension reduction applied across all small-scale data. (a) A scree plot of the cumulative variability explained across components. (b) Scatterplots of Glucose and the first three principal components. Using PCA, the 80% of the variation can be explained by using just 11 components, seen in the scree plot above. That being said, each component does not contain contain much predictive power to the response. For this reason, partial least squares should have been used. (Not sure why they didn’t?) A second approach to deal with the correlation is to take the first-order derivative within each profile. To compute first-order differentiation, the response at the \\((p−1)^{st}\\) value in the profile is subtracted from the response at the \\(p^{th}\\) value in the profile. This difference represents the rate of change of the response between consecutive measurements in the profile. Larger changes correspond to a larger movement and could potentially be related to the signal in the response. This means that the autocorrelation between profiles should be greatly reduced. Figure 9.9: Autocorrelations before and after taking derivatives of the spectra. We can see in Figure 9.9 that using the first order derivatives dramatically takes out autocorrelation after the 3rd day lag. 9.5.1 Altogether~ Taken altogether, subsequent preprocessing steps nearly eliminate within-spectra drift and most of the trends that are unrelated to the peaks have been minimized. Figure 9.10: Spectra for the first day of the first small-scale bioreactor where the preprocessing steps have been sequentially applied. "],["impacts-of-data-processing-on-modeling.html", "9.6 Impacts of Data Processing on Modeling", " 9.6 Impacts of Data Processing on Modeling All of these preprocessing steps can be considered tuning parameters, where different combinations of preprocessing and models are tested. 9.6.1 Cross Validation In this case, our dataset is tiny with 15 small bioreactors that contain 14 daily measurements. Because of the size, we can try out leave-one-out and kfold cross validation. Resample Heldout Bioreactor 1 5, 9, and 13 2 4, 6, and 11 3 3, 7, and 15 4 1, 8, and 10 5 2, 12, and 14 Both of these CV methods are at risk of being influenced by one unusual hold out or fold. For that reason, it’s best to increase the number of repeats with kfold CV. The book recommends 5 repeats, depending on the computational load required. 9.6.2 Model Selection In terms of model selection, options are limited. PCA or PLS are often used, but these should only be selected if the predictors and response is linear or planar. For non-linear relationships, neural networks, support vector machines, or tree-based methods can be used. NN’s and SVM’s cannot handle profile data directly and need to be preprocessed. Tree based methods are a good option since they can handle correlated features, but variable importance can be deceiving because of their inclusion. In this section, PLS, SVM’s, and NN’s will be used, along with the preprocessing steps, to make comparison. Figure 9.11: Cross-validation performance using different preprocessing steps for profile data across several models. Results from cross-validation reveals several things: Subsequent preprocessing steps generally decreases RMSE Preprocessing had the biggest impact on NN’s and regular SVM The biggest decline were actually in the standard deviation of the performance (the error bars) PLS after derivatives was clearly the best method overall Cubist SVM also did well after smoothing Looking closer at PLS, we can see that preprocessing reduced both RMSE AND the number of components required. Using the derivatives on the final step, PLS only required 4 components, while the others needed around 10 to contain the same amount of variation. Figure 9.12: The tuning parameter profiles for partial least squares across profile preprocessing steps. Comparing PLS and the Cubist SVM we can see the positive impact preprocessing has on the observed vs predicted plots. Without preprocessing, the models are quite biased. After taking the derivatives the predictive power significantly improves. Figure 9.13: A comparison of the observed and predicted glucose values for the large-scale bioreactor data. It’s important to note that the above modeling approach used the bioreactors as the unit of analysis because the days within them would be correlated. If cross-validation would have been done on the days instead of the bioreactors, the RMSE would have been small, but we would have an overlly optimistic interpretation of our results. Across all models and all profile preprocessing steps, the hold-out RMSE values are artificially lower when day is used as the experimental unit as compared to when bioreactor is used as the unit. Ignoring or being unaware that bioreactor was the experimental unit would likely lead one to be overly optimistic about the predictive performance of a model. Figure 9.14: Cross-validation performance comparison using bioreactors, the experimental units, or naive resampling of rows. "],["summary-2.html", "9.7 Summary", " 9.7 Summary Profile data has a specific structure, can be over time, have highly correlated features, and contain a hierarchical structure. Understanding the experimental unit is essential to make decisions around preprocessing and evaluation. Basic preprocessing steps for profiled data can include reducing baseline effect, reducing noise across the profile, and harnessing the information contained in the correlation among predictors. No one set of preprocessing steps or model will work best in every situation. Finding the right combination, along with using expert knowledge, can produce a very effective model. Fin "],["meeting-videos-7.html", "9.8 Meeting Videos", " 9.8 Meeting Videos 9.8.1 Cohort 1 Meeting chat log LOG "],["feature-selection-overview.html", "Chapter 10 Feature Selection Overview", " Chapter 10 Feature Selection Overview Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-1.html", "10.1 SLIDE 1", " 10.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-8.html", "10.2 Meeting Videos", " 10.2 Meeting Videos 10.2.1 Cohort 1 Meeting chat log LOG "],["greedy-search-methods.html", "Chapter 11 Greedy Search Methods", " Chapter 11 Greedy Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-2.html", "11.1 SLIDE 1", " 11.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-9.html", "11.2 Meeting Videos", " 11.2 Meeting Videos 11.2.1 Cohort 1 Meeting chat log LOG "],["global-search-methods.html", "Chapter 12 Global Search Methods", " Chapter 12 Global Search Methods Learning objectives: THESE ARE NICE TO HAVE BUT NOT ABSOLUTELY NECESSARY "],["slide-1-3.html", "12.1 SLIDE 1", " 12.1 SLIDE 1 ADD SLIDES AS SECTIONS (##). TRY TO KEEP THEM RELATIVELY SLIDE-LIKE; THESE ARE NOTES, NOT THE BOOK ITSELF. "],["meeting-videos-10.html", "12.2 Meeting Videos", " 12.2 Meeting Videos 12.2.1 Cohort 1 Meeting chat log LOG "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
