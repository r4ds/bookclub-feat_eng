# Engineering Numeric Predictors

```{r message=FALSE, warning=FALSE, include=FALSE}
# ------------------------------------------------------------------------------
# Feature Engineering and Selection: A Practical Approach for Predictive Models
# by Max Kuhn and Kjell Johnson
#
# ------------------------------------------------------------------------------
# 
# Code for Section 6.1 at
# https://bookdown.org/max/FES/numeric-one-to-one.html
#
# ------------------------------------------------------------------------------
# 
# Code requires these packages: 

library(caret)
library(tidymodels)

theme_set(theme_bw() + theme(legend.position = "top"))

# ------------------------------------------------------------------------------

data("segmentationData")

# ------------------------------------------------------------------------------

segmentationData$Cell <- NULL
segmentationData <- segmentationData[, c("EqSphereAreaCh1", "PerimCh1", "Class", "Case")]
names(segmentationData)[1:2] <- paste0("Predictor", LETTERS[1:2])

example_train <- subset(segmentationData, Case == "Train")
example_test  <- subset(segmentationData, Case == "Test")

example_train$Case <- NULL
example_test$Case  <- NULL

simple_trans_rec <- recipe(Class ~ ., data = example_train) %>%
  step_BoxCox(PredictorA, PredictorB) %>%
  prep(training = example_train)

simple_trans_test <- bake(simple_trans_rec, example_test)
pred_b_lambda <-
  tidy(simple_trans_rec, number = 1) %>% 
  filter(terms == "PredictorB") %>% 
  select(value)

bc_before <- ggplot(example_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "blue", fill = "blue", alpha = .6) + 
  xlab("Predictor B") + 
  ggtitle("(a)")
bc_after <- ggplot(simple_trans_test, aes(x = PredictorB)) + 
  geom_histogram(bins = 35, col = "red", fill = "red", alpha = .6) + 
  xlab("Predictor B (inverse)") + 
  ggtitle("(b)")


```

**Learning objectives:**

-   Learn about common issues and techniques when handling continuous predictors<br><br>
-   Often dealing with continuous predictors can be corrected by the model you select
    -   Skewed data? Use tree-based methods

        -   K-nearest neighbor and support vector machines should be avoided

    -   Highly correlated variables? Use Partial Least Squares

        -   Multiple linear regression and neural networks should be avoided\
            <br><br>
-   Feature Engineering techniques to:
    -   Address problematic characteristics of individual predictors

    -   Expand individual predictors to better represent complex relationships

    -   Consolidate redundant information

## Problematic Characteristics of predictors 

One of the first things we can do when transforming our data is to rescale it to improve model performance. There are several different methods that can be used in different situations. Box-Cox and Yeo Johnson transformations can be used to correct for highly skewed predictors. Standardizing strategies such as scaling, centering, and smoothing can also be used to ensure your predictors have similar qualities.

### Dealing with Skewed Data  

**Do a Box-Cox Transformation!**

Originally developed to transform the outcome variable, box-cox uses maximum likelihood estimation to estimate a transformation parameter λ. The transformation allows the data to follow a normal distribution. The plot below shows the before and after effects. \

```{r box_cox, echo=FALSE, fig.dim = c(8, 8)}

library(patchwork)

pw = bc_before / bc_after
pw + plot_annotation(
  caption = 'The distribution of a skewed predictor before (a) and after (b) applying the Box-Cox transformation.'
)
```

#### What does a Box-Cox transformation do? 

Because the estimated parameter is in the exponent, it is called a POWER transformation. Different values of the parameter mimick no transformation (1), log transformation (0), square root (0.5), or inverse (-1). Because it is so flexible it can handle many different types of distributions

-   Several caveats:

    -   Can only be used when predictors are greater or equal to 0

    -   Yeo-Johnson transformation can be used when there are negative values.

    -   Best used for models that use polynomial transformations like linear regression, neural networks, or support vector machines. \

#### Logit Transformation 

Proportions, or data that falls between 0 and 1, are a special case that warrants their own transformation! Sometimes when a proportion is the outcome variable, model predictions can include results that are outside these bounds. Using a **logit transformation**, we can correct for that by changing the values between negative and positive infinity. If π is the variable, the logit transformations is:

$$
logit(π) = log(\frac{π}{1-π})
$$ After the predictions are calculated, you can use the inverse logit to return the data to its original form.

### Standardizing 

**Why standardize?** When your predictors are all in different units, with varying ranges, some may have an out-sized influence on the outcome when that should not be the case. This is especially true for models that use a distance computation like kNN, PCA, SVM, lasso and ridge regression, and variable importance calculation. Logistic regression and tree based methods not so much....

The main techniques illustrated in the book are centering, scaling, and smoothing sequential (time) data.

#### Centering and Scaling

**Centering** is pretty straight forward. For each variable, every value is subtracted by its average. After which, all variables will have a mean of zero.

<br><br>

**Scaling** is the process of dividing a variable by the corresponding training set\'s standard deviation. This ensures that that variables have a standard deviation of one.

In addition to improving model performance, centering and scaling can also help better interpret our models for a couple reasons

1.  It is easier to interpret the y-intercept when the predictors have been centered and scaled

2.  Centering allows for an easier interpretation of coefficients with different magnitudes\

#### Time data: Smoothing

When your model uses sequential, or time related, there can be a lot of noise in your model. By computing a running average,or a running median, we can *smooth* out our predictor and reduce noise. These are computed by taking the average of a point and one point before and after. The size of the moving window is extremely important. Small windows might not involve much smoothing at all, and large windows may miss out on important trends.

Computing the running median is especially important when there are major outliers in your sequential data.

-   Standardizing

    -   Centering

        -   Forcing all values to mean zero\

    -   Scaling

        -   Standard Deviation

        -   Range

    -   Time/sequential data

        -   Smoothing

            -   Use a running-mean or running median if there are outliers

            -   More advanced = splines

            -   \*\* insert graph\*\*

## Many Transformations

Expanding to one predictor to multiple!

-   Basis Expansion

    -   What is a basis?

        -   A basis is a set of vectors V such that every vector V can be written uniquely as a finite combination of vectors of the basis.

            -   **include image of basis**

        -   Coefficients of the basis expansion can be estimated using basic linear regression

-   Polynomial Splines

    -   This basis expansion breaks the space up into regions whose boundaries are called knots

    -   Knots are chosen using percentiles, let's say 33% and 66% for three regions.

    -   **Include splines graph**

    -   Can use grid search to find the right number of knots, but some spline functions include Generalized cross-validation to estimate the complexity.

    -   Additionally, you can use the outcome variable to model spline complexity

        -   Generalized Additive Models (GAMS) model separate basis functions for each variable

        -   Loess Model (Locally estimated scatter plot smoothing

    -   Hinge Function transformation!

        -   **insert graphic**

## Many MANY Transformation Methods

-   correcting for outliers and reducing dimensionality

-   Linear Projection Methods = how do we direct dimension reduction process to find the optimal predictor space?

    -   PCA, kernel PCA, independent component analysis (ICA), non-negative matrix factorization (NNMF), and partial least squares (PLS)

    -   Unsupervised methods are good at decreasing computation time but not generally model performance

-   PCA

    -   finds a linear combination of the original predictors that summarizes the maximum amount of variation. Since they are orthogonal, the predictor space is partitioned in a way that does not overlap. (uncorrelated)

    -   **insert component plot**

    -   **insert component heatmap**

    -   plot the first few score against each other to see classes, outliers, or interesting patterns.

-   Kernel PCA

    -   PCA works best when predictors are linearly correlated, but what if the relationship is not actually linear but quadratic? That's were kernel PCA comes in.

    -   There are MANY different kernels based on the shape of the predictor space. (Polynomial, Gaussian etc)

    -   Can lead to much better fit\

-   Independent Component Analysis

    -   PCA components are uncorrelated with each other, but they are often not independent.

    -   ICA is similar to PCA except that it components are also as statistically independent as possible

    -   Unlike PCA, there is no ordering in ICA.

    -   pre-processing is essential\

-   Non-negative factorization

    -   when features are greater than or equal to zero. Most popular for text data!\

-   Partial Least Squares

    -   supervised version of pca that guides dimension reduction optimally related to the response.

    -   finds latent variables that have optimal covariance with the response

    -   It takes fewer components than PCA

    -   Must be more cognizant of overfitting

    -   PLS and kPCA have potential to outperform PCA

-   Autoencoders

    -   Computationally complex multivariate method for finding representations of the predictor space, primarily used in deep learning.

    -   Generally they don't have any actual interpretation but they have some strengths

    -   Example: used in the pharmaceutical industry to estimate how good a drug might be based on its chemical structure

        -   useful when there is a large amount of unlabled data

        -   To simulate a drug discovery project just starting up, a random set of 50 data points were used as a training set and another random set of 25 were allocated to a test set. The remaining 4327 data points were treated as unlabeled data that do not have melting point data.

        -   **show autoencoder graphi**c

    -   Spatial Sign

        -   Primarily used in image analysis, transforms the predictors based on their center to the distribution and projects them onto a nD sphere

        -   Example of taking animal 'scat' data to predict the species.

        -   **Insert graphic**

        -   Good for decreasing impact of outliers

    -   Distance and depth features

        -   Useful for classification

        -   Recompute the predictors based distance to class centroid, sort of like knearest neighbor

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>

<summary>Meeting chat log</summary>

    LOG

</details>
