# Engineering Numeric Predictors

**Learning objectives:**

-   Learn about common issues and techniques when handling continuous predictors
-   Often dealing with continuous predictors can be corrected by the model you select
    -   Skewed data? Use tree-based methods

        -   K-nearest neighbor and support vector machines should be avoided

    -   Highly correlated variables? Use Partial Least Squares

        -   Multiple linear regression and neural networks should be avoided
-   Feature Engineering techniques to:
    -   Address problematic characteristics of individual predictors

    -   Expand individual predictors to better represent complex relationships

    -   Consolidate redundant information

## Transformations

-   Skewed Data!
-   Use box-cox when you're data
    -   **Insert Box cox plot here**

    -   Orginally developed to transform the outcome variable, box-cox uses maximum likelihood estimation to estimate a transformation parameter Î»

    -   Because the estimated parameter is in the exponent, it is called a POWER transformation. Different values of the parameter mimick no transformation (1), log transformation (0), square root (0.5), or inverse (-1). Because it is so flexible it can handle many different types of distributions

    -   Can only handle positive numbers
-   Yeo and Johnson can be used when there are negative values
-   Any model that uses polynomial transformations like linear regression, neural networks, or support vector machines **could benefit**\
-   For data between 0 and 1 we can use a different transformation LOGIT!
-   Standardizing
    -   Centering

        -   Forcing all values to mean zero \

    -   Scaling

        -   Standard Deviation

        -   Range

    -   Time/sequential data

        -   Smoothing

            -   Use a running-mean or running median if there are outliers

            -   More advanced = splines

            -   \*\* insert graph\*\*

## Many Transformations

Expanding to one predictor to multiple!

-   Basis Expansion

    -   What is a basis?

        -   A basis is a set of vectors V such that every vector V can be written uniquely as a finite combination of vectors of the basis.

            -   **include image of basis**

        -   Coefficients of the basis expansion can be estimated using basic linear regression

-   Polynomial Splines

    -   This basis expansion breaks the space up into regions whose boundaries are called knots

    -   Knots are chosen using percentiles, let's say 33% and 66% for three regions.

    -   **Include splines graph**

    -   Can use grid search to find the right number of knots, but some spline functions include Generalized cross-validation to estimate the complexity.

    -   Additionally, you can use the outcome variable to model spline complexity

        -   Generalized Additive Models (GAMS) model separate basis functions for each variable

        -   Loess Model (Locally estimated scatter plot smoothing

    -   Hinge Function transformation!

        -   **insert graphic**

## Many MANY Transformation Methods

-   correcting for outliers and reducing dimensionality

-   Linear Projection Methods = how do we direct dimension reduction process to find the optimal predictor space?

    -   PCA, kernel PCA, independent component analysis (ICA), non-negative matrix factorization (NNMF), and partial least squares (PLS)

    -   Unsupervised methods are good at decreasing computation time but not generally model performance

-   PCA

    -   finds a linear combination of the original predictors that summarizes the maximum amount of variation. Since they are orthogonal, the predictor space is partitioned in a way that does not overlap. (uncorrelated)

    -   **insert component plot**

    -   **insert component heatmap**

    -   plot the first few score against each other to see classes, outliers, or interesting patterns.

-   Kernel PCA

    -   PCA works best when predictors are linearly correlated, but what if the relationship is not actually linear but quadratic? That's were kernel PCA comes in.

    -   There are MANY different kernels based on the shape of the predictor space. (Polynomial, Gaussian etc)

    -   Can lead to much better fit \

-   Independent Component Analysis

    -   PCA components are uncorrelated with each other, but they are often not independent.

    -   ICA is similar to PCA except that it components are also as statistically independent as possible

    -   Unlike PCA, there is no ordering in ICA.

    -   pre-processing is essential \

-   Non-negative factorization

    -   when features are greater than or equal to zero. Most popular for text data!\

-   Partial Least Squares

    -   supervised version of pca that guides dimension reduction optimally related to the response.

    -   finds latent variables that have optimal covariance with the response

    -   It takes fewer components than PCA

    -   Must be more cognizant of overfitting

    -   PLS and kPCA have potential to outperform PCA

-   Autoencoders

    -   Computationally complex multivariate method for finding representations of the predictor space, primarily used in deep learning.

    -   Generally they don't have any actual interpretation but they have some strengths

    -   Example: used in the pharmaceutical industry to estimate how good a drug might be based on its chemical structure

        -   useful when there is a large amount of unlabled data

        -   To simulate a drug discovery project just starting up, a random set of 50 data points were used as a training set and another random set of 25 were allocated to a test set. The remaining 4327 data points were treated as unlabeled data that do not have melting point data.

        -   **show autoencoder graphi**c

    -   Spatial Sign

        -   Primarily used in image analysis, transforms the predictors based on their center to the distribution and projects them onto a nD sphere

        -   Example of taking animal 'scat' data to predict the species.

        -   **Insert graphic**

        -   Good for decreasing impact of outliers

    -   Distance and depth features

        -   Useful for classification

        -   Recompute the predictors based distance to class centroid, sort of like knearest neighbor

## Meeting Videos

### Cohort 1

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>

<summary>Meeting chat log</summary>

    LOG

</details>
